<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Wonchul Kim</title>
 <link href="http://wonchul-kim.github.io/atom.xml" rel="self"/>
 <link href="http://wonchul-kim.github.io/"/>
 <updated>2021-08-08T15:50:59+09:00</updated>
 <id>http://wonchul-kim.github.io</id>
 <author>
   <name>Wonchul Kim</name>
   <email></email>
 </author>

 
 <entry>
   <title>Metrics for evaluating algorithms</title>
   <link href="http://wonchul-kim.github.io/lectures/2021/08/08/metrics/"/>
   <updated>2021-08-08T00:00:00+09:00</updated>
   <id>http://wonchul-kim.github.io/lectures/2021/08/08/metrics</id>
   <content type="html">&lt;h1 id=&quot;metrics-to-evaluate-algorithm-performance&quot;&gt;Metrics to evaluate algorithm performance&lt;/h1&gt;

&lt;p&gt;구현한 알고리즘의 성능을 수치로 나타내기 위한 기법으로서 여러 종류가 존재하기 때문에 잘 선택해야한다!&lt;/p&gt;

&lt;h2 id=&quot;confusion-matrix&quot;&gt;Confusion matrix&lt;/h2&gt;

&lt;p&gt;알고리즘의 결과(예측값, prediction)는 다음과 같이 실제의 값(ground truth)과 비교하면 다음과 같이 confusion matrix로 정리가 가능하다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;^^&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;^^&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;PREDICTION&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;^^&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;^^&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;positive&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;negative&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;GROUND TRUTH&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;positive&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;^^&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;negative&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TN&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt; &amp;amp; &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt; : 예측값이 실제값과 같은지에 대한 판단&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;positive &amp;amp; negative : 예측값에 대한 판단&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그리고 이를 바탕으로 다양한 알고리즘의 성능을 정량화가 가능하고, 알고리즘의 목적(task)에 따라서 다음과 같이 다양한 수치로 정의하여 알고리즘의 성능을 정의할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;for-object-detectoinsegmentation&quot;&gt;For object detectoin/segmentation&lt;/h2&gt;

&lt;h4 id=&quot;precision-정확도&quot;&gt;Precision (정확도)&lt;/h4&gt;

&lt;p&gt;&lt;u&gt;전체 검출 결과 중&lt;/u&gt;에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;를 예측한 비율&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Precision = \frac{TP}{TP + FP} = \frac{TP}{all \; detected}&lt;/script&gt;

&lt;h4 id=&quot;recall-재현율-or-sensitivity민감도-hit-rate적중률&quot;&gt;Recall (재현율, or sensitivity(민감도), hit rate(적중률))&lt;/h4&gt;

&lt;p&gt;&lt;u&gt;정답 중&lt;/u&gt;에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;를 예측한 비율&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Recall = \frac{TP}{TP + FN} = \frac{TP}{all \; ground \; truth}&lt;/script&gt;

&lt;h4 id=&quot;precision-recall-curve-pr-curve&quot;&gt;Precision-Recall curve (PR curve)&lt;/h4&gt;

&lt;p&gt;알고리즘의 성능을 대변하는 precision과 recall의 두 지표는 서로 &lt;u&gt;반비례&lt;/u&gt; 관계를 가지기 때문에 이 둘을 모두 고려하는 precision-recall curve를 이용해야 한다.&lt;/p&gt;

&lt;p&gt;이에 앞서, prediction에서 positive와 negative는 이를 구분할 수 있는 기준 및 수치인 &lt;code class=&quot;highlighter-rouge&quot;&gt;threshold&lt;/code&gt;에 따라서 값이 달라지기 때문에 precision과 recall도 달라진다. 예를 들어, 어떤 물체를 탐지하는 데에 있어서 가능성(confidence)을 80%라고 할 때, &lt;code class=&quot;highlighter-rouge&quot;&gt;threshold&lt;/code&gt;가 70이면 positive이지만, 90일 경우에는 negative가 된다.&lt;/p&gt;

&lt;p&gt;이러한 &lt;code class=&quot;highlighter-rouge&quot;&gt;threshold&lt;/code&gt;를 0부터 1까지 정해진 단위간격(e.g. 0.1)으로 precision과 recall이 달라지는 두 관계를 나타낸 것이 precision-recall curve이고, $y$축은 precision이고, $x$축은 recall이다. 따라서, PR curve는 recall의 변화에 따른 precision이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Average Precision (AP)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;precision-Recall curve를 통하여 알고리즘의 성능을 정량적으로 나타내기 위한 개념으로서 그래프 선 아래 쪽의 면적(넓이)으로 계산한다. 이 때, AP가 높으면 높을수록 그 알고리즘의 성능이 전체적으로 우수하다는 의미로 통한다.&lt;/p&gt;

    &lt;p&gt;&lt;u&gt;특히, computer vision에서 object detection 또는 segmentation 분야에서는 주로 AP로 성능을 평가 및 비교하고, 이는 각 클래스 마다  얼마나 잘 탐지하는지를 말해준다.&lt;/u&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Mean Average Precision (mAP)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;AP로 성능을 평가할 때, 물체의 클래스가 여러 개인 경우 각 클래스에 대한 AP의 평균으로 나눈 값으로 계산하고, 이 수치로 알고리즘의 전체 성능을 수치화하여 평가할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;for-binary-classification&quot;&gt;For binary classification&lt;/h2&gt;

&lt;h4 id=&quot;true-positive-rate-tpr-적중률&quot;&gt;True Positive Rate (TPR, 적중률)&lt;/h4&gt;
&lt;p&gt;실제 &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;인 것을 알고리즘이 &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;라고 예측한 확률로서, 정답 중에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;를 예측하는 확률로 &lt;u&gt;recall과 동일하다.&lt;/u&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;TPR = \frac{TP}{TP + FN}&lt;/script&gt;

&lt;h4 id=&quot;false-positive-rate-fpr-오경보확률-or-false-alarm-rate&quot;&gt;False Positive Rate (FPR, 오경보확률 or &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt; Alarm Rate)&lt;/h4&gt;
&lt;p&gt;전체 &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt;인 것 중에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;로 잘 못 예측한 확률을 나타낸다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;FPR = \frac{FP}{FP + TN} = 1 - specificity&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Specificity&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;실제 &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt;인 것을 알고리즘이 &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt;라고 예측할 확률이다.&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;&amp;lt;img src='./imgs/fpr_`threshold`.png' loc=center&amp;gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;threshold&lt;/code&gt;를 높이면, positive rate가 커지기 떄문에 FPR(sensitivity)는 증가하지만, specificity는 감소한다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;threshold&lt;/code&gt;를 낮추면, positive rate가 작아지기 떄문에 FPR(sensitivity)는 감소하지만, specificity는 증가한다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;receiver-operating-characteristic-roc&quot;&gt;Receiver Operating Characteristic (ROC)&lt;/h4&gt;

&lt;p&gt;$x$축은 FPR, $y$축에 TPR로 한 그래포로 &lt;u&gt;classification 알고리즘의 성능을 정리하기 위한 도구로 사용된다.&lt;/u&gt;&lt;/p&gt;

&lt;p&gt;이 때, &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;로 분류하는 확률의 &lt;code class=&quot;highlighter-rouge&quot;&gt;threshold&lt;/code&gt;가 낮을수록 TPR은 높아지지만 동시에 FPR 역시 높아진다. 반대로 &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;로 분류하는 확률의 &lt;code class=&quot;highlighter-rouge&quot;&gt;threshold&lt;/code&gt;가 높을수록 FPR은 낮아지지만 동시에 TPR역시 낮아진다. 따라서, TPR이 높을수록, 그리고 FPR이 낮을수록 좋은 알고리즘이기 때문에 이를 염두에 두고 threhold를 찾을 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Area Under ROC (AUROC)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;AUC는 ROC curve 아래쪽의 면적(넓이)으로 계산이 되고, 클 수록 FPR과 관계없이 TPR이 높다고 볼 수 있으므로 좋은 알고리즘이라고 할 수 있다. 반대로, 최악의 모델은 AUROC가 0.5로 이는 기울기 1의 직선에 해당하는 것으로 가각 &lt;code class=&quot;highlighter-rouge&quot;&gt;True&lt;/code&gt;와 &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt;일 확률이 0.5이므로 어느 한쪽으로 결정을 할 수 없다는 것과 마찬가지이다. 그리고 AUROC가 0.5보다 낮으면 아이러니하게도 클래스를 반대로 예측한다는 것을 의미한다.&lt;/p&gt;

    &lt;p&gt;TPR은 전체 양성 샘플 중에 양성으로 예측된 샘플의 비율이기 때문에 TPR은 높을 수록 좋고, FPR은 전체 음성 샘플 중에 양성으로 잘못 예측된 것의 비율이기 때문에 FPR은 낮을 수록 좋다. 그리고 &lt;code class=&quot;highlighter-rouge&quot;&gt;threshold&lt;/code&gt;가 커지면 양성으로 예측되는 경우가 적어지기 때문에 TPR이든 FPR이든 작아질 수 밖에 없다. 따라서, &lt;u&gt;FPR이 작아질 때 TPR이 천천히 작아진다면, 좋은 성능을 가진 알고리즘이고,&lt;/u&gt; 이는 곧 곡선 아래의 넓이가 1에 가까워지는 것을 의미한다. 그렇기 때문에 AUC로 이진분류기의 성능을 평가할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;대개 AUC가 0.8이상이면 아주 훌륭한 성능을 가진 이진분류기라고 평가합니다. 0.7과 0.8 사이라면 좋은 이진분류기입니다. AUC가 0.5~0.7라면 있으면 도움은 되는 이진분류기입니다. AUC가 만약 0.5이하라면 쓸모없는 이진분류기입니다. 없는 것만 못합니다.&lt;/p&gt;

&lt;h4 id=&quot;f_beta&quot;&gt;$F_{\beta}$&lt;/h4&gt;
&lt;p&gt;F-베타 또한 이진 분류모델의 성능을 나타내는 지표값입니다. FBeta는 Recall과 Precision을 하나로 합쳐준 값이며 다음과 같이 계산됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F_{beta} = (1 + \beta^2) \times \frac{precision \times recall}{(\beta^2 \times precision) + recall}&lt;/script&gt;

&lt;p&gt;여기서 \beta는 Recall을 Precision보다 얼마나 중요하게 다룰지를 나타냅니다. \beta는 데이터의 특성과 분류 목적에 따라 정하면 됩니다. Recall과 Precision이 동일하게 중요한 경우 1을, Recall이 중요한 경우 1보다 큰 값을, Precision이 중요한 경우 0과 1 사이값을 선택하면 되겠습니다.&lt;/p&gt;

&lt;p&gt;예를 들어 전염병 검사와 같이 Recall이 중요한 경우 (&lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt; Negative를 줄여야 하는 경우), 높은 \beta값을 선택해야 합니다.
하지만 동영상 추천과 같이 Precision이 중요한 경우 (&lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt; Positive를 줄여야 하는 경우), 낮은 \beta값을 선택해야 합니다. 왜냐하면 모델이 추천해준 동영상 목록 중 사용자가 관심 있는 동영상이 많고 관심 없는 동영상이 적어야 하기 때문입니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/the-ultimate-guide-to-binary-classification-metrics-c25c3627dd0a#9891&quot;&gt;The ultimate guide to binary classification metrics&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://bskyvision.com/1082&quot;&gt;[python] 쉽고 간단하게 마스크 착용 유무 판별기 만들기.txt&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://angeloyeo.github.io/2020/08/05/ROC.html&quot;&gt;ROC curve&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;to-do&quot;&gt;TO DO&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;https://newsight.tistory.com/53&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Bag of Tricks</title>
   <link href="http://wonchul-kim.github.io/lectures/2021/07/02/tricks/"/>
   <updated>2021-07-02T00:00:00+09:00</updated>
   <id>http://wonchul-kim.github.io/lectures/2021/07/02/tricks</id>
   <content type="html">&lt;p&gt;인공지능과 관련된 많은 논문들이 정확도를 0.01%라도 개선하기 위해서 엄청난 computation time과 cost를 통해서 노력을 하고 있는 것에 반해, &lt;a href=&quot;https://dawn.cs.stanford.edu/benchmark/&quot;&gt;An End-to-End Deep Learning Benchmark and Competition - DAWNBench&lt;/a&gt;에서는 &lt;strong&gt;training time, training cost, inference latency, inference cost&lt;/strong&gt;에 중점을 두고 있다.&lt;/p&gt;

&lt;p&gt;해당 사이트에 가보면, ImageNet, CIFAR10, SQuAD의 데이터셋에 대해 진행되었고, 이 중에서 &lt;a href=&quot;https://myrtle.ai/learn/how-to-train-your-resnet-8-bag-of-tricks/&quot;&gt;CIFAR10에 대해서 단일 GPU로 94.1%의 정확도를 약 26초만에 얻은 전략을 정리해 놓은 글&lt;/a&gt;을 살펴보고자 하고, &lt;a href=&quot;https://colab.research.google.com/github/davidcpage/cifar10-fast/blob/master/bag_of_tricks.ipynb&quot;&gt;colab&lt;/a&gt;도 제공하고 있다.&lt;/p&gt;

&lt;p&gt;Baseline으로 학습한 시간은 75초이고, 이를 26초로 줄여나가는 전략들을 소개하고 있다. 이에 대해서 기본적은 목표가 94%이상이라는 정확도와 함께 computation time과 cost를 줄이는 것이기 때문에 94.1% 정확도가 넘어가는 경우, 그 차이만큼 epoch 수를 줄여나가서 94.1%의 정확도로 계속 고정하여 computation time과 cost를 줄이고 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Preprocessing on the GPU (~70초)
이미지의 normalizing, transposing, 패딩과 같은 전처리 뿐만 아니라 Augmentation 까지도 GPU를 활용해서 처리. (이 때, 데이터강화는 기존 CPU의 샘플 하나단위가 아닌 배치단위로 처리)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Moving max-pool layers (~64초)
conv -&amp;gt; batch norm -&amp;gt; act -&amp;gt; max_pool의 순서를 conv -&amp;gt; max_pool -&amp;gt; batch_norm -&amp;gt; act로 변경. (레이어의 이미지 크기를 줄인 후, 계산작업을 수행해서 시간 단축)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Label smoothing (~59초)
뉴럴넷을 generalization 시켜주고, 학습시간을 줄여주는것으로 알려진 Label Smoothing 기법을 추가.(94.2%의 정확도)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CELU activations (~52초)
Continuously Differentiable Exponential Linear Unit의 약자로, ELU와 비슷하지만 약간 더 가벼움. (94.3%의 정확도)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ghost batch norm (~46초)
학습 배치를 여러개의 부분으로 쪼개서, 각 부분마다 독립적으로 batch norm을 적용하는 기법 (94.2%의 정확도)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Frozen batch norm scales (~43초)
학습시 레이어별 weight와 bias 값을 추적한 결과, 크게 변동사항이 없다는것을 발견. 고정된 Batch Norm 스케일을 적용함 (94.2%의 정확도)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Input patch whitening (~36초)
global PCA whitening 기법 대신, 패치 기반의 PCA Whitening 기법을 적용 (94.3%의 정확도)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Exponential moving averages (~34초)
학습률의 값을 조정해 나가기 위하여 5개의 배치마다 moving average 값을 업데이트 (94.3%의 정확도)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Test-time augmentation (~26초)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;references&quot;&gt;references&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;원글: https://myrtle.ai/learn/how-to-train-your-resnet-8-bag-of-tricks/&lt;/li&gt;
  &lt;li&gt;Colab: https://colab.research.google.com/github/davidcpage/cifar10-fast/blob/master/bag_of_tricks.ipynb&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Pytorch - shape 변환</title>
   <link href="http://wonchul-kim.github.io/lectures/2021/07/02/pytorch/"/>
   <updated>2021-07-02T00:00:00+09:00</updated>
   <id>http://wonchul-kim.github.io/lectures/2021/07/02/pytorch</id>
   <content type="html">&lt;h2 id=&quot;view-reshape-transpose-permute&quot;&gt;view(), reshape(), transpose(), permute()&lt;/h2&gt;

&lt;h3 id=&quot;1view-vs-reshape&quot;&gt;1.view() vs. reshape()&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;공통점: 두 함수 모두 주어진 tensor에 대한 size/shape을 바꾸기 위해서 사용&lt;/li&gt;
  &lt;li&gt;차이점
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;view()&lt;/code&gt;: 주어진 tensor에 대한 pointer를 공유하기 떄문에 &lt;code class=&quot;highlighter-rouge&quot;&gt;view()&lt;/code&gt;를 통해 반환한 tensor의 값을 바꾸면 기존의 주어진 tensor도 바뀐다. 그리고 반환된 tensor는 &lt;strong&gt;contiguous&lt;/strong&gt;하다.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;reshape()&lt;/code&gt;: 주어진 tensor에 대한 pointer를 공유할 수도 있고 안할 수도 있다. 그렇기 때문에 정확히 하기 위해서는 &lt;code class=&quot;highlighter-rouge&quot;&gt;clone()&lt;/code&gt;을 통해서 copy를 수행하고 나서 &lt;code class=&quot;highlighter-rouge&quot;&gt;view()&lt;/code&gt;를 해야한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_ptr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_ptr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensor([[[0.1073, 0.5023, 0.3928],
         [0.8484, 0.9984, 0.2771]],
        [[0.2419, 0.7654, 0.4761],
         [0.1559, 0.6650, 0.6054]]])
1354206646080

tensor([[0.1073, 0.5023, 0.3928, 0.8484, 0.9984, 0.2771],
        [0.2419, 0.7654, 0.4761, 0.1559, 0.6650, 0.6054]])
1354206646080
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;왜 이렇게 만들었는지는 모르겠지만, 결과적으로는 정확히 하기 위해서는 &lt;code class=&quot;highlighter-rouge&quot;&gt;reshape()&lt;/code&gt;보다는 &lt;code class=&quot;highlighter-rouge&quot;&gt;view()&lt;/code&gt;를 사용하는 것을 추천하며, 이 때 pointer를 공유하는 것이 아닌 주어진 tensor를 copy하고 싶다면 다음과 같이 &lt;code class=&quot;highlighter-rouge&quot;&gt;clone()&lt;/code&gt;을 하고 나서 &lt;code class=&quot;highlighter-rouge&quot;&gt;view()&lt;/code&gt;를 하도록 한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_ptr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_ptr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1354206644416
1354206646080
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2transpose&quot;&gt;2.transpose()&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;transpose()&lt;/code&gt;도 &lt;code class=&quot;highlighter-rouge&quot;&gt;view()&lt;/code&gt;와 마찬가지로 기존의 tensor에 대한 pointer를 공유하면서 size/shape을 바꾸어서 새로운 tensor를 반환한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Tensor.transpose(dim0, dim1) → Tensor
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;하지만, &lt;code class=&quot;highlighter-rouge&quot;&gt;view()&lt;/code&gt;와는 달리 arg로서 정의된 &lt;code class=&quot;highlighter-rouge&quot;&gt;dim0&lt;/code&gt;과 &lt;code class=&quot;highlighter-rouge&quot;&gt;dim1&lt;/code&gt;만 서로 뒤바끼고 반환하는 새로운 tensor는 &lt;strong&gt;non-contiguous&lt;/strong&gt;하며, &lt;strong&gt;contiguous&lt;/strong&gt;와 &lt;strong&gt;non-contiguous&lt;/strong&gt;의 tensor에 대해서 모두 동작한다. &lt;code class=&quot;highlighter-rouge&quot;&gt;view()&lt;/code&gt;는 오직 &lt;strong&gt;contiguous&lt;/strong&gt;의 tensor에 대해서만 하기 때문에 &lt;code class=&quot;highlighter-rouge&quot;&gt;transpose()&lt;/code&gt;로 반환된 tensor는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Tensor.transpose(dim0, dim1).contiguous().view(-)&lt;/code&gt;를 해주어야한다.&lt;/p&gt;

&lt;h3 id=&quot;3permute&quot;&gt;3.permute()&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;permute()&lt;/code&gt;는 주어진 tensor에 해당하는 shape/size의 해당하는 차원의 index를 인자로 받아서 재배열한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;permute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;  
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_contiguous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch.Size([3, 32, 16])
False
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;그리고 &lt;code class=&quot;highlighter-rouge&quot;&gt;permute()&lt;/code&gt;도 마찬가지로 반환하는 tensor는 &lt;strong&gt;non-contiguous&lt;/strong&gt;하다.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;transpose()&lt;/code&gt;와 &lt;code class=&quot;highlighter-rouge&quot;&gt;permute()&lt;/code&gt;는 &lt;strong&gt;non-contiguous&lt;/strong&gt; tensor를 반환한다. 이로부터 &lt;code class=&quot;highlighter-rouge&quot;&gt;view()&lt;/code&gt;를 사용하려면 &lt;strong&gt;contiguous&lt;/strong&gt;로 고쳐주어야 한다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;view()&lt;/code&gt;는 차원을 분리해야할 때 사용하자. (예, [B*2, C, D] -&amp;gt; [B, 2, C, D])&lt;/li&gt;
  &lt;li&gt;1차원의 생성/제거는 &lt;code class=&quot;highlighter-rouge&quot;&gt;unsqueeze()&lt;/code&gt;와 &lt;code class=&quot;highlighter-rouge&quot;&gt;squeeze()&lt;/code&gt;를 활용하자.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;transpose()&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;permute()&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;view()&lt;/code&gt;는 모두 기존의 tensor의 pointer를 공유한다.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Data-Centric AI Competition</title>
   <link href="http://wonchul-kim.github.io/competitions/2021/07/02/data-centric/"/>
   <updated>2021-07-02T00:00:00+09:00</updated>
   <id>http://wonchul-kim.github.io/competitions/2021/07/02/data-centric</id>
   <content type="html">&lt;h2 id=&quot;data-centric-ai-competition&quot;&gt;&lt;a href=&quot;https://https-deeplearning-ai.github.io/data-centric-comp/?fbclid=IwAR2TKDf6tk7B0-a9DRQ6IFivBqoMsTuBW1pFl32VxXS8shnwmgV_SFzA4gU&quot;&gt;Data-Centric AI Competition&lt;/a&gt;&lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;references&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;competition: https://https-deeplearning-ai.github.io/data-centric-comp/?fbclid=IwAR2TKDf6tk7B0-a9DRQ6IFivBqoMsTuBW1pFl32VxXS8shnwmgV_SFzA4gU&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Pytorch - Tensor.contiguous()</title>
   <link href="http://wonchul-kim.github.io/lectures/2021/07/02/contiguous()/"/>
   <updated>2021-07-02T00:00:00+09:00</updated>
   <id>http://wonchul-kim.github.io/lectures/2021/07/02/contiguous()</id>
   <content type="html">&lt;h2 id=&quot;tensorcontiguous&quot;&gt;Tensor.contiguous()&lt;/h2&gt;

&lt;p&gt;https://titania7777.tistory.com/3
https://hanseokhyeon.tistory.com/entry/PyTorch-contiguous-%ED%95%A8%EC%88%98
https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays/26999092#26999092
https://gaussian37.github.io/dl-pytorch-snippets/&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Inductive Bias</title>
   <link href="http://wonchul-kim.github.io/lectures/2021/07/02/bias/"/>
   <updated>2021-07-02T00:00:00+09:00</updated>
   <id>http://wonchul-kim.github.io/lectures/2021/07/02/bias</id>
   <content type="html">&lt;p&gt;‘모델이 학습과정에서 본 적이 없는 분포의 데이터를 입력 받았을 때, 해당 데이터에 대한 판단을 내리기 위해 가지고 있는, 학습과정에서 습득된 편향’을 Inductive bias라고 정의하는 것 같더라구요. 
그러니까 분류모델을 예로 들자면 Inductive Bias가 없는 모델은 학습과정에서 본 적 없는 모든 입력에 대해 Unknown, 혹은 (Unknown class가 따로 없다면)균등확률분포를 뱉게 된다는 거죠.&lt;/p&gt;

&lt;p&gt;다행히 저런 비모수적 모델말고 모수적인 모델들은 대부분 학습과정에서 어떤 형태로든 Inductive bias를 갖게 되는 것 같습니다.&lt;/p&gt;

&lt;p&gt;다들 별 거부감 없이 사용하는 “Transformer는 CNN보다 Inductive bias가 적다.”라는 표현에는 좀 어폐가 있는 걸로 느껴집니다.
마치 Inductive bias 자체가 모델의 성능을 저해하는 요소인 것 처럼 오해할 수도 있을 것 같거든요.
Transformer는 CNN보다 ‘구조적으로 강제되는’ Inductive bias가 적다고 표현해야 맞는거 아닌가요?
저는 Transformer가 CNN보다 (데이터만 많으면) 잘 되는 이유가,
Transformer가 Locality나 Translation Equivalence와 같이 CNN구조라면 강제적으로 가질 수 밖에 없는 Inductive bias를 더 큰 자유도를 가진 상태에서 적절하게 조절해서 가질 수 있고, 대신 기존에 구조적으로 가질 수 없었던 Inductive bias(공간적으로 아주 멀리 떨어진 픽셀들간의 상관관계 등)는 더 많이 가질 수 있게 되기 때문이라고 이해하고 있거든요.
따라서 저러한 표현을 할 때 적절하게 Inductive bias를 수식해주지 않으면, 어떤 Inductive bias는 줄어든 반면에, 다른 Inductive bias는 늘어났기 때문에 심각한 어폐가 생긴다고 생각합니다.
물론 제가 읽은 문헌(단단한 머신러닝)이 잘못됐고 제가 예시로 든 용례로 쓰시는 분들이 알고 계신 정의가 맞는 걸수도 있지만, 적어도 여러 용례가 충돌하기 때문에, 용어의 오용을 방지하기 위해 어느 정의가 맞는 것인지 확실하게 집고 넘어갈 필요가 있다고 느껴집니다. 
Inductive Bias는 도대체 뭐라고 정의해야 엄밀한 걸까요?&lt;/p&gt;

&lt;p&gt;ps. 많은 의견을 나눠주셔서 감사합니다.
다만 제가 아직도 영 마음에 안 드는건  ‘Transformer는 CNN보다 Inductive Bias가 적다.’ 표현인데요… 
CNN의 Locality가 Inductive Bias면 Transformer나 Vanilla MLP의 Globality(Full Connectivity)도 Inductive Bias이기 때문에, 
결과적으로 CNN과 Transformer는 서로 어떤  Inductive Bias는 상대보다 적게 가지고 있되, 
다른 Inductive Bias는 상대보다 더 많이 가지고 있을 거고, 
따라서 Transformer가 CNN보다 inductive bias의 총량(정량적으로 표현할 방법이 없는걸로 알고 있지만 다들 이야기 하는 많다, 적다하는 맥락에서)이 줄어들었냐?하는 명제에 대해서 확언할 근거가 부족하다고 생각합니다. 
데이터량에 따른 성능 변화를 관찰함으로서 실험적으로 가정해 볼 순 있지만 사실 Inductive Bias의 총량 이외에 다른 더 중요한 원인이 있을 수도 있으니까요.
아니면 정말 극단적인 예로 Transformer가 CNN보다 사실 Inductive Bias는 더 많은 데 그 편향들이 죄다 모집단의 분포를 그대로 빼다 박듯이 근사해버릴 수 있는 극도로 유용한 Inductive Bias들이고, 학습이 느려지는 원인은 따로 존재할 수도 있고요&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Artificial Neural Network</title>
   <link href="http://wonchul-kim.github.io/lectures/2021/07/02/DL/"/>
   <updated>2021-07-02T00:00:00+09:00</updated>
   <id>http://wonchul-kim.github.io/lectures/2021/07/02/DL</id>
   <content type="html">&lt;p&gt;인공지능은 “사람처럼”을 목표로 하여 사람의 뇌를 모방하고자 하는 Artificial Neural Network를 기반으로 이루어진다. 이는 지금은 대중적으로 많이 알려져 있는 딥러닝이라는 소프트웨어적인 관점에서도 많은 발전이 이루어지고 있지만, 하드웨어적인 측면에서도 마찬가지로 고도화가 되고 있다.&lt;/p&gt;

&lt;h2 id=&quot;소프트웨어&quot;&gt;소프트웨어&lt;/h2&gt;

&lt;h3 id=&quot;2세대-신경망&quot;&gt;2세대 신경망&lt;/h3&gt;

&lt;p&gt;2세대 신경망은 현재 많은 발전을 이루고 있는&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./imgs/ann2.jfif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ref: https://blog.lgcns.com/1750&lt;/p&gt;

&lt;p&gt;위의 그림에서도 알 수 있듯이, 실제 뇌와 현재의 인공신경망에는 많은 차이가 존재한다.&lt;/p&gt;

&lt;h3 id=&quot;3세대-신경망&quot;&gt;3세대 신경망&lt;/h3&gt;

&lt;p&gt;Spiking Neural Networks(SNN)는 앞선 2세대 신경망에서의 뇌의 synapse를 모방하고자 하는 학습과정을 좀 더 정교하게 반영하고자 한다. 인간의 뉴런과 시냅스 간의 정보 전달은 전기적 신호에 기반하고, 이는 시간의 흐름에 따라 강도가 조절되며 시냅스를 통해 다음 뉴런으로 전달된다. 2세대에서는 이를 단순히 가중치의 합으로 구현하였지만, SNN에서는 신호전달 과정을 모델링하려고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./imgs/ann3.jfif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ref: https://blog.lgcns.com/1750&lt;/p&gt;

&lt;p&gt;위의 그림과 같이 시간에 따라 변화하는 전기적 신호의 강도를 계산해 뉴런 간 정보전달을 구현하려고 한다.&lt;/p&gt;

&lt;h2 id=&quot;하드웨어&quot;&gt;하드웨어&lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;references&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;https://blog.lgcns.com/1750&lt;/li&gt;
  &lt;li&gt;https://jinseob2kim.github.io/deep_learning.html&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>Lectures and tutorials</title>
   <link href="http://wonchul-kim.github.io/lectures/2021/05/01/lectures/"/>
   <updated>2021-05-01T00:00:00+09:00</updated>
   <id>http://wonchul-kim.github.io/lectures/2021/05/01/lectures</id>
   <content type="html">&lt;h2 id=&quot;deep-learning&quot;&gt;Deep Learning&lt;/h2&gt;
&lt;h3 id=&quot;lectures&quot;&gt;Lectures&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=rsgY0pNfF94&amp;amp;list=PLuv1FSpHurUc2nlabZjCLLe8EQa9fOoa9&quot;&gt;Full Stack Deep Learning - Spring 2021 - UC Berkeley&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLuv1FSpHurUemjLiP4L1x9k6Z9D8rNbYW&quot;&gt;CS224W: Machine Learning with Graphs - Stanford / Winter 2021&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tutorials&quot;&gt;Tutorials&lt;/h3&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;computer-vision&quot;&gt;Computer Vision&lt;/h2&gt;
&lt;h3 id=&quot;lectures-1&quot;&gt;Lectures&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/lectures/computer-vision/?fbclid=IwAR1xkCFT-0mCV4qSVgGI9avp1MXluB2pKIyHdUbMhP1AMVhibmvdwKamPW4&quot;&gt;3D Computer Vision - UNI-TUEBINGEN&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLuv1FSpHurUflLnJF6hgi0FkeNG1zSFCZ&quot;&gt;3D Computer Vision - National University of Singapore - 2021&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dvl.in.tum.de/teaching/&quot;&gt;TUM  Dynamic Vision and Learning Group&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dvl.in.tum.de/teaching/adl4cv-ss20/?fbclid=IwAR3NMGvFHR8FXMFbKld4roJXBSXmkfV_PehDqKe2zI8gUPbb8AxtoWkopus&quot;&gt;Advanced Deep Learning for Computer vision (ADL4CV) (IN2364) in Summer semester 2020&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dvl.in.tum.de/teaching/cv3dst-ss20/&quot;&gt;Computer Vision III: Detection, Segmentation and Tracking (CV3DST) (IN2375)  in Summer semester 2020&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://svl.stanford.edu/&quot;&gt;Fei-Fie Li&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://sites.google.com/view/berkeley-cs294-158-sp20/home&quot;&gt;CS294-158-SP20 Deep Unsupervised Learning Spring 2020&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://manipulation.csail.mit.edu/Fall2020/?fbclid=IwAR3G6Dabb99YMEfeS8ZZQ9Be6Z-8D0PtENX1G6Ot_SY5dssGVfsCOh6tw4s&quot;&gt;Robotic Manipulation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;https://cs230.stanford.edu/past-projects/#winter-2019&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tutorials-1&quot;&gt;Tutorials&lt;/h3&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;natural-language-process&quot;&gt;Natural Language Process&lt;/h2&gt;
&lt;h3 id=&quot;lectures-2&quot;&gt;Lectures&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://people.cs.umass.edu/~miyyer/cs685/schedule.html?fbclid=IwAR3EJMDAqhFnGMuPJNimBNHnvi0Cl9geCwrgo8ZEkY2wXEOL8nf6HDQ_eFk&quot;&gt;CS 685, Fall 2020, UMass Amherst, Advanced NLP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tutorials-2&quot;&gt;Tutorials&lt;/h3&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;etc&quot;&gt;Etc.&lt;/h2&gt;

</content>
 </entry>
 
 <entry>
   <title>Train YOLO v4 using darknet with custom datasets</title>
   <link href="http://wonchul-kim.github.io/computer%20vision/2021/04/28/yolo/"/>
   <updated>2021-04-28T00:00:00+09:00</updated>
   <id>http://wonchul-kim.github.io/computer%20vision/2021/04/28/yolo</id>
   <content type="html">&lt;h3 id=&quot;install-darknet&quot;&gt;Install darknet&lt;/h3&gt;

&lt;h4 id=&quot;download&quot;&gt;download&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/AlexeyAB/darknet.git
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;install&quot;&gt;install&lt;/h4&gt;

&lt;p&gt;In Makefile,&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;GPU=1 
CUDNN=1 
CUDNN_HALF=1 
OPENCV=1 
AVX=0 
OPENMP=0 
LIBSO=1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;CUDNN_HALF: 좀 더 학습시간을 향샹&lt;/li&gt;
  &lt;li&gt;LIBSO: 이전 redmon의 darknet은 make하면 빠져나왔던 so 파일이 옵션으로 추가됨&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;make가 끝나고 나면 darknet파일들이 생성되고, so파일도 잘 생성된 것을 확인할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;demo&quot;&gt;Demo&lt;/h3&gt;
&lt;h4 id=&quot;download-weights&quot;&gt;Download weights&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;inference&quot;&gt;Inference&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./darknet detect cfg/yolov4.cfg yolov4.weights data/dog.jpg
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;train&quot;&gt;Train&lt;/h3&gt;

&lt;h4 id=&quot;convert-datasets-for-yolo&quot;&gt;Convert datasets for Yolo&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/ssaru/convert2Yolo.git
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;edit-configuration-cfgyolov4cfg&quot;&gt;edit configuration (cfg/yolov4.cfg)&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;subdivisions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt; 
&lt;span class=&quot;c&quot;&gt;# Training &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#width=512 &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#height=512 &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;416&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;416&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;momentum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.949&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;decay&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0005&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;angle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;saturation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;exposure&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;hue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.00261&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;burn_in&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;max_batches&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30000&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# maximum iteration&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;24000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;27000&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;scales&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;height, width: 32의 배수&lt;/li&gt;
  &lt;li&gt;subdivisions = 16
    &lt;blockquote&gt;
      &lt;p&gt;Out of memory 오류가 난다면 subdivision을 16, 32, 64 등으로 증가시킨다.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;max_batches = 데이터의 클래스 갯수 * 2000&lt;/li&gt;
  &lt;li&gt;steps = max_batches의 80%의 수치와 90%의 수치&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convolutional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;### (num_classes + 5)*3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt; 

&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yolo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;anchors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;36&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;36&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;76&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;55&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;72&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;146&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;142&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;110&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;192&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;243&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;459&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;401&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;classes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;jitter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;ignore_thresh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;truth_thresh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;scale_x_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.2&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;iou_thresh&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.213&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;cls_normalizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;iou_normalizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.07&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;iou_loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ciou&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;nms_kind&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;greedynms&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;beta_nms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[yolo]&lt;/code&gt;는 총 3개가 존재하므로, 3군데를 모두 수정한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;anchor size의 경우 kmeans를 이용하여 본인의 데이터셋에 맞게끔 수정해줄 수 있다. 하지만 훈련을 충분히 시킬 수 있다면, 그냥 default로 설정되어 있는 coco데이터셋의 anchor size를 이용해도 된다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;make-names&quot;&gt;make names&lt;/h4&gt;
&lt;p&gt;yolo는 class의 index로 훈련을 하지만 index에 따른 class의 이름을 같이 표시해줄 수 있게 만들어 졌다. 그러므로 names파일을 넣어줘야 class의 이름을 확인할 수 있다. gedit명령어를 통해 custom.names파일을 생성한다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;airplane
ship
storage_tank
baseball_diamond
tennis_court
basketball_court
ground_track_field
harbor
bridge
vehicle
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;make-data&quot;&gt;make data&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;classes= 10 
train = custom/train.txt 
valid = custom/validation.txt 
names = custom/custom.names 
backup = backup/
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;download-wieghts&quot;&gt;download wieghts&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;train-custom-data&quot;&gt;train custom data&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./darknet detector train cfg/mifs.data cfg/mifs.cfg backup/yolov4.conv.137 -dont_show -map -show_imgs
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;map&quot;&gt;MAP&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./darknet detector map cfg/mifs.data cfg/mifs.cfg backup/yolov4.conv.137 -thresh 0.5 -iou_thresh 0.5 -points 11
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;recall&quot;&gt;recall&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./darknet detector recall cfg/mifs.data cfg/mifs.cfg backup/yolov4.conv.137 -thresh 0.5 -iou_thresh 0.5 -points 11
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;valid&quot;&gt;valid&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./darknet detector valid cfg/mifs.data cfg/mifs.cfg backup/yolov4.conv.137 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;single-image-test&quot;&gt;single image test&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./darknet detector test .data .cfg .weights -thresh THRESH OPTION
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;-ext_output: Output coordinates&lt;/li&gt;
  &lt;li&gt;-i 1: Use GPU 1&lt;/li&gt;
  &lt;li&gt;-thresh 0.25 -dont_show -save_labels &amp;lt; list.txt: List of Image에 대한 결과 저장&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;video&quot;&gt;Video&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./darknet detector demo .data .cfg .weights .videofile OPTION
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;-c 0: WebCam 0&lt;/li&gt;
  &lt;li&gt;http://192.168.0.80:8080/video?dummy=param.mjpg: Net-videocam&lt;/li&gt;
  &lt;li&gt;-out_filename OUT.videofile: Save result&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] https://github.com/AlexeyAB/darknet&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>확률</title>
   <link href="http://wonchul-kim.github.io/machine%20learning/2020/12/04/probability/"/>
   <updated>2020-12-04T00:00:00+09:00</updated>
   <id>http://wonchul-kim.github.io/machine%20learning/2020/12/04/probability</id>
   <content type="html">&lt;h1 id=&quot;확률-및-확률분포-120분&quot;&gt;확률 및 확률분포 (120분)&lt;/h1&gt;

&lt;h2 id=&quot;1-확률-probability&quot;&gt;1. 확률 (Probability)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;확률&lt;/strong&gt;은 정규적인 교육과정을 받은 사람이라면 누구나 알고 있으며 그렇지 못하더라도 어떠한 일이 일어날 가능성이라는 추상적이지만 이해하는 데에 있어서 부족함이 없는 정의를 이미 알고 있을 것입니다. 하지만, 본 교육은 추상적인 개념이 아닌, 전문적인 교육과정을 제공하기 위한 것이기 때문에 다음과 같이 일반적이면서 공식적인 정의를 살펴보도록 하겠습니다. &lt;strong&gt;&lt;em&gt;확률은 어떠한 사건이 어떠한 일이 일어날 수 가능성을 나타내는 수치입니다.&lt;/em&gt;&lt;/strong&gt; 예를 들어서, 정육면체의 모형의 주사위를 한 번 던지는 행위를 할 것입다.(이러한 행위를 사건이라고 할 수 있습니다.) 그리고 이 행위의 결과로서 1이 나올 수 있는 가능성, 즉 확률은 $\frac{1}{6}$입니다. 그리고 이에 대한 해석은 이 주사위를 6번 던지면 한번은 1이 나올 수 있다는 것을 의미합니다. 따라서, 다음과 같은 수식으로 확률을 계산하여 나타낼 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;확률 (p) = \frac{어떤 사건이 실제로 일어난 횟수}{어떤 사건이 일어날 횟수+ 어떤 사건이 일어나지 않을 횟수}&lt;/script&gt;

&lt;p&gt;즉, 어떤 사건이 실제로 일어날 것인지 혹은 일어났는지에 대한 비율이라고 할 수 있으며, 확률은 보통 $p$(probability)로 나타내며 특정 사건이 일어날 확률을 $p$(사건)으로 표기합니다. &lt;br /&gt;
(이때, 확률에 100을 곱하면 퍼센티지(%)의 단위로 바뀝니다.)&lt;/p&gt;

&lt;p&gt;확률은 수학이라는 커다란 나무와 같은 학문에 있어서 하나의 가지로서 뻗어있는 수학의 영역입니다. 그렇기 때문에 확률을 하나의 영역으로서 깊에 배우고 싶으신 분들은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Probability&quot;&gt;위키&lt;/a&gt;를 참조하실 수 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 확률은 다음과 같은 특징을 갖습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$0 \leq p \leq 1$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\sum{P}_i^{i=N} = 1$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다시 말해서, 확률은 전체 사건에 대해 나누어지기 때문에 항상 0보다는 크거나 같고 1보다는 작거나 같습니다. 그리고 각 확률의 총합은 항상 1입니다.&lt;/p&gt;

&lt;p&gt;예를 들어서, 정육면체의 주사위를 굴리려고 합니다. 이 때의 각각의 면이 나올 확률은 어떻게 될까요? python을 활용해서 알아보겠습니다.&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;예제-1&quot;&gt;예제 1)&lt;/h4&gt;

&lt;p&gt;정육면체이기 때문에 6개의 면이 존재하므로 총 경우의 수는 6입니다. 그리고 한번 주사위를 던지면 6개의 면 중 하나의 면만 나올 수 있으므로 각 면이 한번 나올 횟수는 1이 되어 $\frac{1}{6}$로 모두 동일합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;nb_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 1을 나타내는 면이 나올 수 있는 횟수&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nb_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 2을 나타내는 면이 나올 수 있는 횟수&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nb_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 3을 나타내는 면이 나올 수 있는 횟수&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nb_4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 4을 나타내는 면이 나올 수 있는 횟수&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nb_5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 5을 나타내는 면이 나올 수 있는 횟수&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nb_6&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 6을 나타내는 면이 나올 수 있는 횟수&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 1이 나올 확률&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 값을 출력&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0.16666666666666666
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;동일한 방법으로 나머지 사건에 대해서 알아보면 다음과 같습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;p_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_6&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 값을 출력&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0.16666666666666666 0.16666666666666666 0.16666666666666666 0.16666666666666666 0.16666666666666666
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;따라서, 모든 사건에 대해서 확률이 0.166으로 동일함을 알 수 있습니다.
(python을 활용하여 문제를 표기할 경우 변수명은 사건명과 비슷하게 하여 쉽게 파악할수 있도록 하는 것이 좋습니다.)&lt;/p&gt;

&lt;h4 id=&quot;예제-2&quot;&gt;예제 2)&lt;/h4&gt;

&lt;p&gt;수년간의 데이터를 통해서 강원도 지역이 내일 홍수가 날 확률을 p(강원도 지역은 내일 홍수가 남)로 표기합니다. 그렇다면, 강원도 지역에서 내일 홍수가 나지 않을 확률은 무엇일까요? &lt;br /&gt;
앞서 확률의 특징에 대해서 언급하였듯이, 모든 확률의 합은 1입니다. 따라서,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;1 - (강원도 지역은 내일 홍수가 남) = p(강원도 지역은 내일 홍수 안남)&lt;/script&gt;

&lt;p&gt;으로 나타낼 수 있습니다. &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위의 예를 python을 활용하여 알아보겠습니다. (강원도 지역이 내일 홍수가 날 확률을 0.3으로 하겠습니다.)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#먼저 확률을 지정해 줍니다. &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_flood&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 강원도 지역이 내일 홍수가 날 확률&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_not_flood&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_flood&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 강원도 지역이 내일 홍수가 나지 않을 확률&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_not_flood&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 값을 출력&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0.7
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;위와 같이 강원도 지역이 내일 홍수가 나지 않을 확률을 구할 수 있으며, 앞서 언급하였던 확률의 총합은 1이라는 특징도 확인할 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;p_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_flood&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_not_flood&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 확률의 총합&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 값을 출력&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;2-joint-marginal-and-conditional-probability&quot;&gt;2. Joint, Marginal, and Conditional Probability&lt;/h2&gt;

&lt;p&gt;먼저, 강의를 시작하기에 앞서 강의의 제목에서 영어로 표기하였습니다. 물론, 우리말로 해석하여 결합(joint), 주변(marginal), 그리고 조건부(conditional) 확률로서 표기가 가능하지만, 앞으로 여러 분들이 계속적으로 공부를 하고 이러한 확률의 이론을 더 깊은 곳에 사용하고자 하려면 결국에는 논문이나 전문서적을 읽어야하기 때문에 앞으로는 영어로서 표기하도록 하겠습니다. 특히, 이러한 영어 표기 자체를 한국어로 해석하는 것이기 때문에 공식적인 표기가 존재하지는 않아 한국어로는 여러 표기가 존재하기도 합니다. 그럼 강의를 시작하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;앞서 확률의 강의에서 배운 내용은 하나의 변수에 대해 일어날 수 있는 가능성을 나타낸 것입니다. 하지만, 우리가 접하는 대부분의 상황 또는 사건은 하나의 변수만 작용하여 해당 사건을 해석할 수 없으며, 여러가지의 변수를 조합하여야 합니다. 그리고 확률은 이러한 여러가지의 변수에 대해서도 조합을 할 수 있도록 해주는 명확한 공식인 &lt;strong&gt;Joint&lt;/strong&gt;, &lt;strong&gt;Marginal&lt;/strong&gt;, 그리고 &lt;strong&gt;Conditional Probability&lt;/strong&gt;이 있습니다. 이번 강의에서는 이러한 공식을 활용하여 여러가지의 변수에 대한 변수들을 조합하여 사건을 해석하는 방법을 배워보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;강의에 앞서 이번에 다룰 내용에 대해서 간략하게 설명하면 다음과 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Joint probability는 두 개의 이상의 사건(event)이 동시에 일어날 확률을 의미합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Marginal probability는 다른 변수와 관계없이 사건(event)이 일어날 확률을 의미합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Conditional probability는 하나 이상의 사건이 이미 일어난 상황에서 어떠 한 다른 사건이 일어날 확률을 의미합니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-1-joint-probability&quot;&gt;2-1. Joint probability&lt;/h3&gt;

&lt;p&gt;먼저, &lt;strong&gt;joint probability&lt;/strong&gt;는 두 개 이상의 사건이 동시에 일어날 확률을 의미합니다. 따라서 두 개 이상의 확률변수를 가집니다. 예를 들어, 두 확률변수(또는 사건) $A$와 $B$에 대한 결합확률은 두 사건의 교집합의 확률을 계산하는 것과 같으며, 다음과 같이 나타낼 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A \cap B) ~~ 또는 ~~ P(A, B)&lt;/script&gt;

&lt;p&gt;이 때, 다 사건이 결합확률로서 계산되기 위해서는 다음과 같은 조건이 필요합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;두 사건 $A$와 $B$는 동시에 일어나야 합니다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;예를 들어서, 두 개의 주사위($A, B$)를 동시에 던지는 경우에는 $A, B$가 동시에 일어납니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;두 사건 $A와 B$는 반드시 서로 독립(independent)이어야 한다. 즉, $A와 B$는 독립사건이어야 합니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서, 위의 두 조건을 만족할 경우 다음과 같이 계산할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A, B) = P(A \cap B) = P(A)*P(B)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;만약 두 사건 $A와 B$가 서로 종속(dependent)일 경우에는 어떻게 될까요? &lt;br /&gt;&lt;br /&gt;
예를 들어, 감기가 걸릴 확률을 사건 $A$라 하고 콧물이나 기침이 날 확률에 대한 사건을 $B$라 한다면, 콧물이나 기침은 감기가 걸렸을 경우에 대해서는 영향을 받기 때문에 사건 $A$는 사건 $B$에 영향을 주게됩니다. 따라서 두 사건 $A$와 $B$는 서로 독립 사건이 아니게 되어 종속이라고 합니다.
이러한 경우에는 위와 같은 joint probability를 사용할 수 없는데, 그 이유는 두 사건 $A와 B$가 독립적으로 동시에 일어나지 않기 때문입니다. 그러므로 위와 같은 사건 $A와 B$에 대한 결합확률은 결국 $P(A \cap B) = 0$ 이 됩니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그리고 $n$개의 확률변수에 대해서 서로 독립일 경우에는 다음과 같이 간단하게 계산이 가능합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X_1 = x_1, X_2 = x_2, ... , X_n = x_n) = p(X_1 = x_1) * p(X_2 = x_2) ~*~ ... ~*~ P(X_n = x_n)&lt;/script&gt;

&lt;h3 id=&quot;2-2-marginal-probability&quot;&gt;2-2. Marginal probability&lt;/h3&gt;

&lt;p&gt;앞선 joint probabilty와 대비되는 개념으로서 &lt;strong&gt;marginal probability&lt;/strong&gt;는 다른 사건에 대해서 관계없이 개별 사건의 확률을 의미합니다. 예를 들어서, 두 가지의 확률변수인 $X와 Y$가 주어지고 하나의 확률변수 ($X$)에 대해서만 확률을 구하는 것입니다. 그리고 이는 다른 변수($Y$)에 대한 모든 확률의 합을 구하는 것과 동일하다고 할 수 있습니다. 그렇기 때문에 특별한 표기가 존재하지 않으며, 단순히 $P(A) 또는 P(B)$이며 다음과 같이 계산합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X = A) = \sum^{y \in Y} P(X = A, Y = y)&lt;/script&gt;

&lt;p&gt;다시 말해서, 단순히 구하고자 하는 marginal probaility의 다른 모든 변수에 대한 모든 확률을 합하는 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;2-3-conditional-probability&quot;&gt;2-3. Conditional probability&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;conditional probability&lt;/strong&gt;는 두 개의 사건에 대한 확류에 대해서 하나의 사건이 일어나는 조건하에 또 다른 사건이 발생할 확률을 의미합니다. 예를 들어서, 두 사건 $A, B$에 대해여 사건 $A$가 일어났다는 조건하에 사건 $B$가 일어날 확률 또는 사건 $B$가 일어났다는 조건하에 사건 $A$가 일어날 확률로 나타낼 수 있으며, 전자는 다음과 같이 표기합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B) ~~ 또는 ~~ P(B|A)&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;여기서 기호 ‘&lt;/td&gt;
      &lt;td&gt;‘는 만약(if)과 같은 의미로 조건을 뜻합니다. 그래서 전자는 $P(A&lt;/td&gt;
      &lt;td&gt;B)$이고, 후자는 $P(B&lt;/td&gt;
      &lt;td&gt;A)$입니다. 이 conditional probability를 계산은 다음과 같이 정의합니다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B) = \frac{P(A, B)}{P(B)} ~~ 또는 ~~ P(B|A) = \frac{P(A, B)}{P(A)}&lt;/script&gt;

&lt;p&gt;전자의 경우에 대해서 위와 같이 conditional probabilty는 다음과 같은 근거를 따릅니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;사건 $B$ 가 사실이므로 모든 가능한 표본은 사건 $B$ 에 포함되어야 합니. 즉, 새로운 실질적 표본공간은  $\Omega_{new} \in B$ 가 됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;사건 $A$ 의 원소는 모두 사건 $B$ 의 원소도 되므로 사실상 사건 $(A \cap B)$ 의 원소가 됩니다. 즉, 새로운 A_{new} \in $(A \cap B)$ 가 됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;따라서 사건 $A$의 확률 즉, 신뢰도는 원래의 신뢰도(결합확률)를 새로운 표본공간의 신뢰도(확률)로 정규화(normalize)한 값이라고 할 수 있습니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B) = \frac{P(A_{new})}{P(\Omega_{new})} = \frac{P(A, B)}{P(B)}&lt;/script&gt;

&lt;p&gt;그렇다면, 앞선 joint, marginal probability를 활용하여 두 사건이 독립일 경우에 대해서 conditional probability는 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B) = \frac{P(A, B)}{P(B)} = \frac{P(A)*P(B)}{P(B)} = P(A)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(B|A) = \frac{P(A, B)}{P(A)} = \frac{P(A)*P(B)}{P(A)} = P(B)&lt;/script&gt;

&lt;p&gt;즉, 두 사건 $A와 B$는 서로 영향을 미치지 않기 때문에 conditional probability를 구할 때, 서로 일어날 사건이 조건으로 되더라도 고려할 필요가 없는 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;3-확률분포-probability-distribution&quot;&gt;3. 확률분포 (Probability distribution)&lt;/h2&gt;

&lt;p&gt;앞서 배운 내용은 확률에서 우리는 하나의 사건에 대해서 어떠한 특수한 상황이 발생할 가능성으로서의 확률을 배웠습니다. 예를 들어, 주사위를 던지는 데에 있어서 1일 나올 확률은 $\frac{1}{6}$이라고 하였습니다. 하지만, 주사위를 던지는 행위에서 발생할 수 있는 사건 또는 결과(event)는 1, 2, 3, 4, 5, 또는 6입니다. 그러므로 주사위를 던지는 행위에 대해서 이러한 모든 사건에 대해서 나타내는 방법이 필요하며, 이를 &lt;strong&gt;확률분포&lt;/strong&gt;를 이용하여 해결할 수 있습니다. 즉, 앞서 배운 확률은 확률분포를 설명하기 위한 기초적인 작업이라고도 할 수 있습니다. 그리고 앞으로의 여러 확률에 관한 문제를 접하는 과정에서 고차원의 공식에에 대해 기본이 되는 계산 법칙이라고도 할 수 있습니다. 더욱이 앞으로의 목표인 딥러닝 또는 머신러닝에서 있어서는 다음과 같이 다양하게 사용되는 주된 확률분포에 대해서 아는 것이 매우 중요합니다. 왜냐하면, 우리가 풀고자 하는 문제에 대한 상태를 확률로서 표현을 해야하며, 이는 기존에 정의되어 있는 확률분포를 활용하여 계산을 하기 때문입니다. 그렇기 때문에 여러 확률분포에 대해서 인지하고 있다면, 더욱 더 문제에 대한 정의를 잘 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;따라서, &lt;strong&gt;&lt;em&gt;확률분포(probability distribution)는 어떠한 사건에 대해서 일어날 수 있는 모든 가능성을 의미하는 확률변수(random variable)나 확률변수의 집합(set)에 대응하는 확률들을 하나의 분포로서 정의하는 것입니다.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;마찬가지로, 확률분포에 대해서 더욱이 깊은 공부를 원하시는 분들은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Probability_distribution&quot;&gt;위키&lt;/a&gt;부터 참조하셔서 공식적인 정의를 숙지하시길 추천드립니다.&lt;/p&gt;

&lt;h3 id=&quot;3-1-확률변수-random-variable&quot;&gt;3-1. 확률변수 (random variable)&lt;/h3&gt;

&lt;p&gt;확률분포에 대해서 배우기에 앞서, 확률분포를 정의 또는 표현하기 위한 &lt;strong&gt;확률변수&lt;/strong&gt;에 대해서 먼저 살펴보도록 하겠습니다. 확률변수는 하나의 사건에 대해서 일어날 수 있는 모든 가능성 또는 결과(event)를 표현하기 위한 도구라고 할 수 있습니다. 앞선 예에서 우리는 주사위를 던지는 것을 살펴보았고, 이 중에서 일어날 수 있는 사건은 1, 2, 3, 4, 5 또는 6입니다. 따라서, 주사위를 던지는 행위에 대한 표본공간 $S$는 {1, 2, 3, 4, 5, 6}이 되는 것이고, 확률변수를 $X$로 표기할 경우 다음과 같이 확률변수를 정의합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X: S \rightarrow \{1, 2, 3, 4, 5, 6\}&lt;/script&gt;

&lt;p&gt;즉, 어떠한 상황에 대해서 일어나는 사건 또는 결과를 표현하고자 할 때, 그 사건이 일어날 수 있는 확률에 대한 변수라고 생각하시면 됩니다.&lt;/p&gt;

&lt;h3 id=&quot;3-2-이산확률변수와-확률질량함수&quot;&gt;3-2. 이산확률변수와 확률질량함수&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;이산확률변수(discrete random variable)&lt;/strong&gt;은 말 그대로 앞서 배운 확률변수가 이산이라는 것을 의미합니다. 즉, 확률변수가 표현하는 공간 또는 대상이 연속적이지가 않다는 것입니다. 앞선 예에서 살펴본 주사위를 던지는 상황에 대한 확률변수는 1, 2, .., 6으로 6개로서 1.1 또는 2.3 등을 가질 수 없는 이산확률변수라고 할 수 있습니다. 따라서, &lt;strong&gt;&lt;em&gt;이산확률변수는 전체 표본공간(어떠한 사건에 대해서 일어날 수 있는 모든 가능성 도는 결과들의 집합)이 유한집한이거나 가산집합(요소들을 셀 수 있는 집합)인 확률변수를 의미합니다.&lt;/em&gt;&lt;/strong&gt; 그리고 이러한 이산확률변수 ($X$)의 확률분포를 &lt;strong&gt;확률질량함수(probability mass function)&lt;/strong&gt;라고 합니다. 따라서, &lt;strong&gt;&lt;em&gt;확률질량함수는 이산확률변수가 어떠한 특정 사건이 일어날 확률로서 표현되도록 확률 구해주는 함수라고 할 수 있습니다.&lt;/em&gt;&lt;/strong&gt; 즉, 어떠한 상황에서 일어날 수 있는 사건들이 이산확률변수이면 이를 $X$라고 표현하고 이에 대해 일어날 수 있는 하나의 사건은 보통 이에 대한 소문자로서 $x$로 표현합니다. 그리고 $x$가 발생할 수 있는 확률에 대응하는 함수를 $p_x$로서 확률질량함수라고 하는 것입니다. 예를 들어서, 주사위를 한 번 던졌을 경우에 나타날 수 있는 사건 $x$는 다음과 같이 표현됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_x(x) = \frac{1}{6} ~~~~ ,where x \in \{1, 2, 3, 4, 5, 6\}&lt;/script&gt;

&lt;p&gt;그리고 $x$가 위의 표본공간에 포함되지 않는 이산확률변수에 대해서는 $p_x(x) = 0$이라고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번에는 좀 더 일반적으로 통용되는 정의로서 설명을 해보겠습니다. 표본공간 $S$에 정의된 이산확률변수 $X$에 대해서 사건 $x$가 일어날 확률을 확률질량함수로서 정의하고 다음과 같이 표현합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_x(x) = P(X = x) = P({s \in S|X(s) = x})&lt;/script&gt;

&lt;h3 id=&quot;3-3-확률분포확률질량함수의-성질&quot;&gt;3-3. 확률분포(확률질량함수)의 성질&lt;/h3&gt;

&lt;p&gt;1) $0 \leq p_x(x = x_k) \leq 1$&lt;/p&gt;

&lt;p&gt;2) $\sum_{k=0}^{\infty}p_x(x_k) = 1$&lt;/p&gt;

&lt;p&gt;3) $P(X = x_k or X = x_m) = P(X = x_k) + P(X = x_m) (단, k \neq m)$&lt;/p&gt;

&lt;p&gt;4) $P(a \leq X \leq b) = \sum_{x=b}^aP(X=x)$&lt;/p&gt;

&lt;h3 id=&quot;3-4-확률분포의-평균기대값-및-분산과-표준편차&quot;&gt;3-4. 확률분포의 평균(기대값) 및 분산과 표준편차&lt;/h3&gt;

&lt;p&gt;이산확률변수 $X$와 이에 대한 확률인 확률질량함수를 곱하여 다 더한 값은 어떠한 사건의 평균 또는 기대값이라고 하며, 다음과 같이 구할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_1p_1 + x_2p_2 + x_3p_3 + ··· + x_np_n&lt;/script&gt;

&lt;p&gt;이는 $X$의 평균 또는 기댓값이라 하고, $m$ 또는 $E(X)$로 나타냅니다.&lt;/p&gt;

&lt;p&gt;그리고 확률변수 $X$의 평균이 위와 같이 $m$으로 구해지면, 확률변수의 편차의 제곱의 평균인 $E{(X - m)^2}$을 $X$의 분산이라 하고, $V(X)$ 또는 $σ^2(X)$로 구할 수 있습니다. 또 분산의 양의 제곱근인 표준편차는 $σ(X)$로 나타냅니다.&lt;/p&gt;

&lt;h3 id=&quot;3-5-다양한-확률분포&quot;&gt;3-5. 다양한 확률분포&lt;/h3&gt;

&lt;p&gt;앞서서 우리는 확률과 확률분포에 대해서 배웠습니다. 이러한 정의를 배우는 이유는 우리의 생활에 존재하는 변화 또는 상황(사건)들을 수치로서 표현을 하고 설명을 하고 싶기 때문입니다. 그리고 수치로서 설명이 가능해지면, 우리는 다음의 상황, 즉 미래를 예측할 수 있게 됩니다. 하지만, 실생활에서 일어나는 사건들을 의도적으로 발생할 수도 있지만, 우연과 같은 우리가 예측할 수 없는 또는 수치로서 측정할 수 없는 변수들이 존재하기 때문에 완벽하게 표현할 수 있는 확률분포는 존재하지 않는다고 보는 것이 당연합니다. 그럼에도 불구하고 지금까지 많은 과학자들의 노력에 의해 보조적인 수치로서 탄생한 확률분포들이 많이 존재하며, 어떠한 분포들은 특정한 사건에 대해서 신기할 정도로 완벽하게 들어맞고, 그렇지 않는 것도 존재합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;확률분포의 종류는 매우 다양하기 때문에 본 과정에서는 하나씩 다룰 수 없기 때문에 관심있으신 분들은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Probability_distribution#Common_probability_distributions_and_their_applications&quot;&gt;위키&lt;/a&gt;를 참조하시기 바랍니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그 중에서도 &lt;strong&gt;정규분포(Gaussian distribution)&lt;/strong&gt;는 대표적인 확률분포로서 가장 널리 통용되고 있습니다. 그리고 신기하게도 거의 모든 영역에서 사용함에도 적절하게 들어맞으며, 우리가 궁극적으로 배우고자 하는 머신러닝과 딥러닝에서도 매우 많이 사용되고 있습니다. 특히, 어떠한 변수가 무작위로 가질 수 있는 실제값에 관한 분포를 기술하는데에 있어서 매우 유용하다고 합니다. 이러한 ***정규분포는 앞서 배운 이산확률분포와는 다른 연속확률분포로서 다음과 같이 구할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma}exp(−\frac{(x − \mu)^2}{2\sigma^2)}&lt;/script&gt;

&lt;p&gt;여기서 $\mu$는 이 밀도함수의 평균값 또는 기대값이고 $\sigma$는 표준편차(standard deviation), $\sigma^2$는 분산(variance)입니다. 다음에서 python을 통해 그려볼 것이지만, 이 확률밀도함수는 종 모양의 곡선이고, 평균값을 기준으로 좌우가 대칭이면서 좌우 극단으로 나아갈수록 급격하게 수치가 낮아지는 특징을 지닙니다. 그리고 변곡점도 두 개가 존재하며, 모두 평균에서 표준편차만큼 떨어져 있습니다.&lt;/p&gt;

&lt;p&gt;정규분포의 경우에는 머신러닝에 있어서 매우 많이 사용되는 확률분포이며, 정규 교육과정을 이수하신 분들이라면 누구에게나 익숙한 확률분포라고도 할 수 있습니다. 가우시간 확률 분포와 쌍두마차로서 거의 모든 실생활에서 사용될 수 있는 정도이며, python으로 그래프를 그려보도록 하겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;stats&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# 여러 확률분포를 쉽게 나타낼 수 있도록 해주는 library&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;           &lt;span class=&quot;c&quot;&gt;# numpy library&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# plot을 위한 library&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;먼저, 그래프를 그리기 위한 라이브러리와 정규분포에 대한 계산식을 제공하는 라이브러리를 &lt;code class=&quot;highlighter-rouge&quot;&gt;import&lt;/code&gt;합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;101&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;          &lt;span class=&quot;c&quot;&gt;# 표현하고자 하는 x축 범위 지정&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 직접 위의 식을 이용하여 표현&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;         &lt;span class=&quot;c&quot;&gt;# library를 활용하여 확률분포 설정&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;          &lt;span class=&quot;c&quot;&gt;# 플롯 사이즈 지정&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;            
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;                    
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;                      &lt;span class=&quot;c&quot;&gt;# x축 레이블 지정&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;y&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;                      &lt;span class=&quot;c&quot;&gt;# y축 레이블 지정&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;                           &lt;span class=&quot;c&quot;&gt;# 플롯에 격자 보이기&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Normal Distribution with scipy.stats&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# 타이틀 표시&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;                          &lt;span class=&quot;c&quot;&gt;# 레이블 표시&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;                           &lt;span class=&quot;c&quot;&gt;# 플롯 보이기&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;output_20_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;maximum-likelihood-120분&quot;&gt;Maximum Likelihood (120분)&lt;/h1&gt;

&lt;h2 id=&quot;1-likelihood&quot;&gt;1. likelihood&lt;/h2&gt;

&lt;p&gt;먼저 &lt;strong&gt;likelihood&lt;/strong&gt;라는 것에 대한 개념이 약간 생소할 수 있지만, 이는 probability와 매우 유사한 개념이라고 할 수 있습니다. 둘 다 어떤 확률에 대한 것을 의미하지만, probability는 확률분포를 알고 있는 대상의 시행 전 확률을 의미하는 개념입니다. 그리고 likelihood는 확률분포를 모르는(정확히는 parameter) 대상의 관측된 데이터를 바탕으로 확률을 역추적할 때 쓰이는 개념이라고 생각하면 됩니다.&lt;/p&gt;

&lt;p&gt;예를 들어, 동전을 던지는 시행에 있어서 앞면과 뒷면이 나올 확률은 각각 $\frac{1}{2}$입니다. 이러한 동전을 n번을 던진다고 할때, 기댓값은 $n*\frac{1}{2}$으로 계산할 수 있습니다. 그리고 이러한 계산법을 probability theory(확률론)의 접근방법이라고 할 수 있습니다. 하지만, 우리가 실제로 우리가 동전 던지기를 500번 수행하고 그 결과를 다음과 같이 기록하였습니다. 총 500번 중에서 앞면이 260번 나오고, 뒷면이 240번이 나왔습니다. 이러한 시행결과를 낳게 한 확률분포는 $p(앞면) = \frac{260}{500}$으로 계산될 것입니다. 그리고 이러한 $p$를 likelihood로 볼 수 있으며, 관측된 표본에 의해서 추정되기 떄문에 총합이 1이 아닐 수도 있습니다.&lt;/p&gt;

&lt;p&gt;앞서 말씀드렸듯이, likelihood는 어떠한 대상의 관측된 데이터를 바탕으로 확률분포를 만들 때, 이 확률분포에 사용되는 parameter들을 변화시킴으로서 역추적하는 것을 의미합니다. 그러므로 maximum likelihood는 이러한 확률분포를 최대로하는 parameter들을 찾는 것이라고 할 수 있습니다. 그리고 이러한 과정은 deep learning에서는 deep neural network의 parameter들을 찾는 과정과 매우 유사하다고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://imgur.com/7o1HLRT&quot;&gt;&lt;img src=&quot;https://imgur.com/7o1HLRT.png&quot; width=&quot;600px&quot; title=&quot;source: imgur.com&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;즉, 위의 그림과 같이 주어진 데이터를 보고 이를 나타낼 수 있는 모델을 만드는 것이며, 모델은 자신의 parameter(보통은 $\theta$라고 지칭합니다.)들을 최적화하는 과정입니다.&lt;/p&gt;

&lt;h2 id=&quot;2-probability-density-estimation&quot;&gt;2. Probability density estimation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;density estimation&lt;/strong&gt;은 어떠한 대상에서 관측되는 정보(samples or data)들에 대한 확률분포를 추정하는 문제입니다. 그리고 이러한 density estimation을 하기 위한 수많은 기법들이 존재하지만, machine learning에서 대표적이면서 일반적인 방법으로 maximum likelihood estimatino을 가장 많이 사용하고 있습니다. Maximum likelihood estimation은 확률분포와 이 분포에 대한 parameter들이 주어졌을 때, 먼저 관측되는 정보들의 조건부 확률을 나타내는 likelihood function을 정의하고, 이에 대한 parameter들을 변화시켜 탐색(search) 또는 최적화(optimizatoin)과정을 수행함으로서 machine learning 분야에서 관측되는 정보를 나타내는 모델을 예측할 수 있는 기법으로 많이 사용되고 있습니다.&lt;/p&gt;

&lt;p&gt;모델을 구축하는 데에 있어서 가장 일반적인 분제는 주어진 정보에 대해서 어떻게 joint probability distribution을 추정하는지 입니다. 예를 들어서 어떠한 domain ($x_1, x_2, …, x_n$)으로부터 관측되는 정보($X$)에 대해서 어떤 종류의 확률분포를 선택하고, 이 확률분포에 대한 parameter들을 어떻게 고정할지에 대한 문제를 의미합니다. (관측되는 정보들은 domain으로부터 각각 독립적이고, 동일한 확류분포에 의해서 추출됨을 가정으로 합니다. 이러한 개념을 independent and identically distributed(i.i.d.)라고 합니다.) 따라서, 다음과 같은 질문을 해볼 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;어떻게 확률분포를 선택할 것인가?&lt;/li&gt;
  &lt;li&gt;이 확률분포에 대한 parameter들은 어떻게 고정할 것인가?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이러한 문제들을 해결할 수 있는 기법은 매우 많지만, 다음과 같이 대표적으로 두 가지가 가장 일반적입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Maximum a Posteriori (MAP) - Bayesian method&lt;/li&gt;
  &lt;li&gt;Maixmum Likelihood Estimation (MLE) - frequentist method&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그리고 위의 문제들은 어떠한 대상으로부터 정보를 관착흘 때, 그 정보들의 양이 작고 noise가 존재할 경우에 더욱 더 어려워질 수도 있습니다. 예를 들어서 실제로 물리적인 대상으로부터 정보를 얻을 때, 일반적으로 나타나는 현상이라고 할 수 있으며, 우리가 추정한 probability density function과 그에 대한 paramter들의 결과도 마찬가지로 어느정도의 오차가 존재할 수 있다는 것을 의미합니다.&lt;/p&gt;

&lt;h2 id=&quot;3-maximum-likelihood-estimation&quot;&gt;3. Maximum Likelihood Estimation&lt;/h2&gt;

&lt;p&gt;Mximum likelhood Estimation (MLE)은 관측된 정보($X$)에 대한 joint probability에 대해서 최고의 결과가 나오도록 하는 parameter들을 찾는 탐색 또는 최적화 문제에 대한 해결방안이라고 할 수 있습니다. 이 과정을 설명하기 위해서 앞서 말한 parameter들은 probability density function을 선택하고, 이 분포에 대한 parameter들을 통칭하는 것으로 $\theta$로 명명하도록 하겠습니다. 이 $\theta$는 부드럽게 변화하는 숫자로서 vector로 표현이 되는 것이 보통이며, 각각의 다양한 확률분포와 그 분포에 대한 parameter들을 대표하는 것이라고 생각하면 됩니다. 그리고 maximum이라는 명칭이 붙어 있듯이, 우리는 $\theta$가 주어졌을 때 joint probabilty distribution으로부터 관측되는 정보들에 대한 확률이 최대가 되도록하는 것이 목표이며, 다음과 같이 문제를 정의할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X|\theta)&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;이 조건부 확률은 ‘&lt;/td&gt;
      &lt;td&gt;’ 대신해 ‘;’으로도 표현이 되며, 이는 $\theta$가 random variable이 아니라 모르는 parameter이기 때문입니다. 즉, 다음과 같이 표현할 수 있습니다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X;\theta)&lt;/script&gt;

&lt;p&gt;또는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_1, x_2, ..., x_n; \theta)&lt;/script&gt;

&lt;p&gt;그리고 이 조건부 확률로 나타나는 결과는 $\theta$에 대한 모델로부터 관측되는 정보들의 likelihood로 볼 수 있으며, 다음과 같이 $L()$로 명명합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(X;\theta)&lt;/script&gt;

&lt;p&gt;따라서, Maximum likelihood estimation의 목적은 위의 likelihood function을 최대로 하는 $\theta$를 찾는 것입니다. 즉,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;max L(X;\theta)&lt;/script&gt;

&lt;p&gt;라고 나타냅니다. 그리고 마찬가지로 관측되는 정보인 $X$는 각각의 관측되는 정보인 $x_1, x_2, …, x_n$을 의미하고, 각각은 joint probability distribution을 따르기 때문에 다음과 같이 조건부확률의 곱셈식으로 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;L(X; \theta) = L(x_1, x_2, ... , x_n;\theta)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= \prod_{i=1}^{n} P(x_i;\theta)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;하지만, 확률이 매우 작은 경우에는 곱셈이 이루어질 경우에 매우 불안해질 수 있으므로, 대부분의 경우 $log$ 함수를 확률에 사용하여 다음과 같이 나타냅니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^{n} logP(x_i;\theta)&lt;/script&gt;

&lt;p&gt;이 때, $log$는 e를 base로 하며, log-likelihood function라고 부릅니다. 그리고 대부분의 최적화 문제에서는 cost function을 최소화하는 것을 다루기 때문에 마찬가지로 maximum likelihood estimation에서도 주어진 likelihood function에 -1을 곱한 값을 최대화하는 것으로 합니다. 따라서 다음과 같이 nagative log-likelihood (NLL) function이 만들어집니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;min -\sum_{i=1}^{n} logP(x_i;\theta)&lt;/script&gt;

&lt;h2 id=&quot;4-mle-in-machine-learning&quot;&gt;4. MLE in machine learning&lt;/h2&gt;

&lt;p&gt;앞서 설명한 density estimation은 applied machine learning과 관련이 있습니다. 그리고 machine learning model을 예측하는 것을 probability density estimation의 문제로 볼 수 있습니다. 이를 구체적으로 이야기 하자면, 모델과 모델에 대한 parameter들을 선택하는 문제를 hypothesis ($h$)를 예측하는 것이고, 이 hypothesis는 관측되는 정보를 가장 잘 설명할 수 있는 모델이라고 하며 다음과 같이 정의합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X;h)&lt;/script&gt;

&lt;p&gt;그러므로 이를 다시 maximum likelihood estimation 문제로서 해결하고자 하면 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;max  L(X;h) = max \sum_{i=1}^{n}logP(x_i;h)&lt;/script&gt;

&lt;p&gt;그리고 maximum likelihood estimation은 supervised machine learning에도 유용하게 사용됩니다. 예를 들어, 주어진 input에 대해서 output을 어떻게 mapping하느냐에 따라서 regression 또는 classificaion 문제를 해결할 수 있는 기법으로 사용되며, 다음과 같이 문제를 정의할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;max L(y|X;h) = max \sum_{i=1}^{n}logP(y_i|x_i;h)&lt;/script&gt;

&lt;h3 id=&quot;4-1-mle를-이용한-선형회귀&quot;&gt;4-1. MLE를 이용한 선형회귀&lt;/h3&gt;

&lt;p&gt;앞으로 머신러닝을 배우실 여러 분에게 있어서 &lt;strong&gt;선형회기(learning regression)&lt;/strong&gt;는 가장 기본적인 이론이라고 할 수 있으며, 본 과정에서는 간략하게 설명만 하고 넘어가도로 하겠습니다. &lt;strong&gt;*선형회기란 어떠한 이산 데이터가 존재할 때 이 데이터를 표현하는 분포도를 선형으로 하여금 표현하는 것을 의미합니다.&lt;/strong&gt; 그리고 데이터에 존재하지 않는 수치에 대해서도 예측을 할 수 있도록 하는 것이 목적이며, 선횡회기가 잘 될수록 예측 또한 오차가 적어집니다. 즉, 어떠한 모델이라는 함수 $f$가 존재할 때, 이에 대한 예측값인 $\hat{y}$는 다음과 같이 표현을 합니다.
&lt;script type=&quot;math/tex&quot;&gt;\hat{y} = f(x)&lt;/script&gt;
($\hat{}$이라는 표기인 ‘hat’은 보통 예측값에 많이 사용됩니다.)&lt;/p&gt;

&lt;p&gt;그리고 이 $f$는 용어에서 알 수 있듯이, 선형으로 이루어져 다음과 같이 표현될 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\hat{y} = \beta_0 \times x_0 + \beta_1\times x_1 + \beta_2\times x_2 ... + \beta_n\times x_n&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= \sum^{n}_{i=0} \beta_i\times x_i&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pylab&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# plot을 위한 부분&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'target'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;output_28_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같이 빨간선이 우리가 찾고자하는 모델이고, 파란색 점들이 데이터입니다.&lt;/p&gt;

&lt;p&gt;즉, 선형회기의 목적은 위의 $\beta$라는 parameter들을 구하는 것입니다. 그리고 MLE를 적용하면, 우리가 관심있는 모델인 $f$에 대한 parametmer들인 $\beta$의 posterior 분포를 구하는 것이라고 할 수 있습니다. 우리는 이러한 모델을 만드는 과정을 probability density esitimation의 문제로서 다시 정의할 수 있습니다. 즉히, 모델과 그 모델에서 사용되는 파라미터들을 선택하는 일련의 과정을 모델링을 하는 것으로 이 모델을 $h$라는 가정을 하고, 주어진 데이터를 가장 잘 표현하는 $h$를 찾는 것이라고 할 수 있스니다. 그러므로 다음과 같이 우리는 likelihood함수를 최대화하는 $h$를 찾아야합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;max \sum_{i=1}^n logP(x_i; h)&lt;/script&gt;

&lt;p&gt;그리고 Supervised learning이 다음과 같이 conditional probability 문제로서 주어진 input에 대한 output의 확률을 예측하는 것으로 다음과 같이 표현될 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(y|X)&lt;/script&gt;

&lt;p&gt;그렇기 때문에 우리는 위의 문제를 supervised learning의 영역에서 표현을 하면 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;max \sum_{1}^n logP(y_i | x_i;h)&lt;/script&gt;

&lt;p&gt;이제 우리는 $h$라는 모델 또는 함수를 선형회귀 모델로 바꿀 것입니다. 이에 대해서 우리는 하나의 가정을 사용할 것인데, 이는 매우 합리적이라고 할 수 있습니다. 즉, 주어진 데이터의 측정치는 모두 독립적이고, 동일한 확률분포에 의해서 나타나는 확률변수라는 점입니다. 이는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables&quot;&gt;identical independent distribution&lt;/a&gt;라는 정의로서 자세한 설명은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables&quot;&gt;위키&lt;/a&gt;를 참조하시기 바랍니다. 그리고 우리가 예측하고자하는 값인 $(y)$는 평균이 0인 정규분포의 noise를 갖는다고 할 것입니다. 이러한 가정과 함께 우리는 $X$가 주어질 때 $y$를 예측하는 문제를 동일하게 $X$가 주어질 때 정규분포로부터 $y$를 얻기위한 정규분포의 평균을 예측하는 문제로 바꿀 수 있습니다. 이에 대한 수식적인 표현은 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) = \frac{1}{\sqrt{2\times \pi \times \sigma^2}} \times exp(-\frac{1}{2 \times \sigma^2}\times(y - \mu)^2)&lt;/script&gt;

&lt;p&gt;여기서 $\mu$가 우리가 예측하고자하는 정규분포의 평균이고, $\sigma$는 표준편차입니다. 그리고 우리는 이 함수를 likelihood function으로 사용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;앞선 문제의 경우에서 우리는 $h$를 표현하는 데 사용되는 $a$, $b$라는 파라미터를 구하는 것이 목적입니다. ($a$는 직선의 기울기에, $b$는 $y$절편입니다.) 그러므로 위의 식은 아래와 같이 다시 나타낼 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y|x; h) = \frac{1}{\sqrt{2 \times \sigma^2 \times \pi}}e^{-\frac{(y - (a\times x + b))^2}{2\sigma^2}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;log p(y|x, \theta) = -\frac{(y - (ax + b))^2}{2\sigma^2} + C&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;그리고 $log p(y&lt;/td&gt;
      &lt;td&gt;x, \theta)$를 최소화하는 $a$와 $b$를 찾는 것이 우리의 목표입니다. 이 때, 다른 항들은 $a$와 $b$의 변화에 영향을 받지 않으므로 결과적으로 $-(y - (ax + b))^2$에 대한 최소화를 하는 것과 동일하다고 볼 수 있습니다. 그리고 이에 대해서 우리는 최소자승법(least squares)이라고 부르는 방법을 많이 사용합니다. 이렇게 함수의 최소값을 찾는 것은 gradient descent라는 방식으로 하여금 조금씩 기울기를 따라 내려가면서 최소값을 찾습니다. 기울기를 따라가는 것이기 때문에 미분이 사용되며, 특정 변수에 대한 미분이므로 편비분을 하게 됩니다. 따라서, 위의 문제에 대해서는 다음과 같이 편미분이 이루어집니다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$$ \frac{\partial (logP(y&lt;/td&gt;
      &lt;td&gt;x;h))}{\partial a} = -2(y - (ax + b))x $$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$ \frac{\partial (logP(y&lt;/td&gt;
      &lt;td&gt;x;h))}{\partial b} = -2(y - (ax + b)) $$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;이렇게 임의의 점(또는 초기의 시작점이 주어질 수도 있습니다.)에서 시작하여 gradient의 반대 방향(-가 앞에 존재하기 때문)으로 조금씩 이동하는 방식을 &lt;strong&gt;stochastic gradient descent (SGD)&lt;/strong&gt;라 합니다.&lt;/p&gt;

&lt;p&gt;이를 활용하여 다음과 같이 python으로 구해보도록 하겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# gradient descent 계산&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stepsize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0001&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;theta_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stepsize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_grad&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stepsize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_grad&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;theta_list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;********* Results ***********&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'a_hat: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b_hat: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;********* ******* ***********&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        

&lt;span class=&quot;c&quot;&gt;# plot을 위한 부분&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# plt.plot(x, a*x + b, label='target')&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'regression'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;********* Results ***********
a_hat:  1.0110228329088864
b_hat:  4.498430138396311
********* ******* ***********
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;output_32_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과에서 알 수 있듯이, 우리는 $a$는 1이고, $b$는 5인 target으로부터 데이터를 생성하여 선형모델을 구하는 것을 진행하였습니다. 그리고 그 target이 되는 parameter인 $a, b$에 대해서 각각의 예측값인 $\hat{a}, \hat{b}$는 매우 유사하게 예측이 되었음을 확인할 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Likelihood</title>
   <link href="http://wonchul-kim.github.io/machine%20learning/2020/12/04/likelihood/"/>
   <updated>2020-12-04T00:00:00+09:00</updated>
   <id>http://wonchul-kim.github.io/machine%20learning/2020/12/04/likelihood</id>
   <content type="html">&lt;h1 id=&quot;maximum-likelihood-120분&quot;&gt;Maximum Likelihood (120분)&lt;/h1&gt;

&lt;h2 id=&quot;1-likelihood&quot;&gt;1. likelihood&lt;/h2&gt;

&lt;p&gt;먼저 &lt;strong&gt;likelihood&lt;/strong&gt;라는 것에 대한 개념이 약간 생소할 수 있지만, 이는 probability와 매우 유사한 개념이라고 할 수 있습니다. 둘 다 어떤 확률에 대한 것을 의미하지만, probability는 확률분포를 알고 있는 대상의 시행 전 확률을 의미하는 개념입니다. 그리고 likelihood는 확률분포를 모르는(정확히는 parameter) 대상의 관측된 데이터를 바탕으로 확률을 역추적할 때 쓰이는 개념이라고 생각하면 됩니다.&lt;/p&gt;

&lt;p&gt;예를 들어, 동전을 던지는 시행에 있어서 앞면과 뒷면이 나올 확률은 각각 $\frac{1}{2}$입니다. 이러한 동전을 n번을 던진다고 할때, 기댓값은 $n*\frac{1}{2}$으로 계산할 수 있습니다. 그리고 이러한 계산법을 probability theory(확률론)의 접근방법이라고 할 수 있습니다. 하지만, 우리가 실제로 우리가 동전 던지기를 500번 수행하고 그 결과를 다음과 같이 기록하였습니다. 총 500번 중에서 앞면이 260번 나오고, 뒷면이 240번이 나왔습니다. 이러한 시행결과를 낳게 한 확률분포는 $p(앞면) = \frac{260}{500}$으로 계산될 것입니다. 그리고 이러한 $p$를 likelihood로 볼 수 있으며, 관측된 표본에 의해서 추정되기 떄문에 총합이 1이 아닐 수도 있습니다.&lt;/p&gt;

&lt;p&gt;앞서 말씀드렸듯이, likelihood는 어떠한 대상의 관측된 데이터를 바탕으로 확률분포를 만들 때, 이 확률분포에 사용되는 parameter들을 변화시킴으로서 역추적하는 것을 의미합니다. 그러므로 maximum likelihood는 이러한 확률분포를 최대로하는 parameter들을 찾는 것이라고 할 수 있습니다. 그리고 이러한 과정은 deep learning에서는 deep neural network의 parameter들을 찾는 과정과 매우 유사하다고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://imgur.com/7o1HLRT&quot;&gt;&lt;img src=&quot;https://imgur.com/7o1HLRT.png&quot; width=&quot;600px&quot; title=&quot;source: imgur.com&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;즉, 위의 그림과 같이 주어진 데이터를 보고 이를 나타낼 수 있는 모델을 만드는 것이며, 모델은 자신의 parameter(보통은 $\theta$라고 지칭합니다.)들을 최적화하는 과정입니다.&lt;/p&gt;

&lt;h2 id=&quot;2-probability-density-estimation&quot;&gt;2. Probability density estimation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;density estimation&lt;/strong&gt;은 어떠한 대상에서 관측되는 정보(samples or data)들에 대한 확률분포를 추정하는 문제입니다. 그리고 이러한 density estimation을 하기 위한 수많은 기법들이 존재하지만, machine learning에서 대표적이면서 일반적인 방법으로 maximum likelihood estimatino을 가장 많이 사용하고 있습니다. Maximum likelihood estimation은 확률분포와 이 분포에 대한 parameter들이 주어졌을 때, 먼저 관측되는 정보들의 조건부 확률을 나타내는 likelihood function을 정의하고, 이에 대한 parameter들을 변화시켜 탐색(search) 또는 최적화(optimizatoin)과정을 수행함으로서 machine learning 분야에서 관측되는 정보를 나타내는 모델을 예측할 수 있는 기법으로 많이 사용되고 있습니다.&lt;/p&gt;

&lt;p&gt;모델을 구축하는 데에 있어서 가장 일반적인 분제는 주어진 정보에 대해서 어떻게 joint probability distribution을 추정하는지 입니다. 예를 들어서 어떠한 domain ($x_1, x_2, …, x_n$)으로부터 관측되는 정보($X$)에 대해서 어떤 종류의 확률분포를 선택하고, 이 확률분포에 대한 parameter들을 어떻게 고정할지에 대한 문제를 의미합니다. (관측되는 정보들은 domain으로부터 각각 독립적이고, 동일한 확류분포에 의해서 추출됨을 가정으로 합니다. 이러한 개념을 independent and identically distributed(i.i.d.)라고 합니다.) 따라서, 다음과 같은 질문을 해볼 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;어떻게 확률분포를 선택할 것인가?&lt;/li&gt;
  &lt;li&gt;이 확률분포에 대한 parameter들은 어떻게 고정할 것인가?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이러한 문제들을 해결할 수 있는 기법은 매우 많지만, 다음과 같이 대표적으로 두 가지가 가장 일반적입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Maximum a Posteriori (MAP) - Bayesian method&lt;/li&gt;
  &lt;li&gt;Maixmum Likelihood Estimation (MLE) - frequentist method&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그리고 위의 문제들은 어떠한 대상으로부터 정보를 관착흘 때, 그 정보들의 양이 작고 noise가 존재할 경우에 더욱 더 어려워질 수도 있습니다. 예를 들어서 실제로 물리적인 대상으로부터 정보를 얻을 때, 일반적으로 나타나는 현상이라고 할 수 있으며, 우리가 추정한 probability density function과 그에 대한 paramter들의 결과도 마찬가지로 어느정도의 오차가 존재할 수 있다는 것을 의미합니다.&lt;/p&gt;

&lt;h2 id=&quot;3-maximum-likelihood-estimation&quot;&gt;3. Maximum Likelihood Estimation&lt;/h2&gt;

&lt;p&gt;Mximum likelhood Estimation (MLE)은 관측된 정보($X$)에 대한 joint probability에 대해서 최고의 결과가 나오도록 하는 parameter들을 찾는 탐색 또는 최적화 문제에 대한 해결방안이라고 할 수 있습니다. 이 과정을 설명하기 위해서 앞서 말한 parameter들은 probability density function을 선택하고, 이 분포에 대한 parameter들을 통칭하는 것으로 $\theta$로 명명하도록 하겠습니다. 이 $\theta$는 부드럽게 변화하는 숫자로서 vector로 표현이 되는 것이 보통이며, 각각의 다양한 확률분포와 그 분포에 대한 parameter들을 대표하는 것이라고 생각하면 됩니다. 그리고 maximum이라는 명칭이 붙어 있듯이, 우리는 $\theta$가 주어졌을 때 joint probabilty distribution으로부터 관측되는 정보들에 대한 확률이 최대가 되도록하는 것이 목표이며, 다음과 같이 문제를 정의할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X|\theta)&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;이 조건부 확률은 ‘&lt;/td&gt;
      &lt;td&gt;’ 대신해 ‘;’으로도 표현이 되며, 이는 $\theta$가 random variable이 아니라 모르는 parameter이기 때문입니다. 즉, 다음과 같이 표현할 수 있습니다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X;\theta)&lt;/script&gt;

&lt;p&gt;또는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_1, x_2, ..., x_n; \theta)&lt;/script&gt;

&lt;p&gt;그리고 이 조건부 확률로 나타나는 결과는 $\theta$에 대한 모델로부터 관측되는 정보들의 likelihood로 볼 수 있으며, 다음과 같이 $L()$로 명명합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(X;\theta)&lt;/script&gt;

&lt;p&gt;따라서, Maximum likelihood estimation의 목적은 위의 likelihood function을 최대로 하는 $\theta$를 찾는 것입니다. 즉,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;max L(X;\theta)&lt;/script&gt;

&lt;p&gt;라고 나타냅니다. 그리고 마찬가지로 관측되는 정보인 $X$는 각각의 관측되는 정보인 $x_1, x_2, …, x_n$을 의미하고, 각각은 joint probability distribution을 따르기 때문에 다음과 같이 조건부확률의 곱셈식으로 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;L(X; \theta) = L(x_1, x_2, ... , x_n;\theta)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= \prod_{i=1}^{n} P(x_i;\theta)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;하지만, 확률이 매우 작은 경우에는 곱셈이 이루어질 경우에 매우 불안해질 수 있으므로, 대부분의 경우 $log$ 함수를 확률에 사용하여 다음과 같이 나타냅니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^{n} logP(x_i;\theta)&lt;/script&gt;

&lt;p&gt;이 때, $log$는 e를 base로 하며, log-likelihood function라고 부릅니다. 그리고 대부분의 최적화 문제에서는 cost function을 최소화하는 것을 다루기 때문에 마찬가지로 maximum likelihood estimation에서도 주어진 likelihood function에 -1을 곱한 값을 최대화하는 것으로 합니다. 따라서 다음과 같이 nagative log-likelihood (NLL) function이 만들어집니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;min -\sum_{i=1}^{n} logP(x_i;\theta)&lt;/script&gt;

&lt;h2 id=&quot;4-mle-in-machine-learning&quot;&gt;4. MLE in machine learning&lt;/h2&gt;

&lt;p&gt;앞서 설명한 density estimation은 applied machine learning과 관련이 있습니다. 그리고 machine learning model을 예측하는 것을 probability density estimation의 문제로 볼 수 있습니다. 이를 구체적으로 이야기 하자면, 모델과 모델에 대한 parameter들을 선택하는 문제를 hypothesis ($h$)를 예측하는 것이고, 이 hypothesis는 관측되는 정보를 가장 잘 설명할 수 있는 모델이라고 하며 다음과 같이 정의합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X;h)&lt;/script&gt;

&lt;p&gt;그러므로 이를 다시 maximum likelihood estimation 문제로서 해결하고자 하면 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;max  L(X;h) = max \sum_{i=1}^{n}logP(x_i;h)&lt;/script&gt;

&lt;p&gt;그리고 maximum likelihood estimation은 supervised machine learning에도 유용하게 사용됩니다. 예를 들어, 주어진 input에 대해서 output을 어떻게 mapping하느냐에 따라서 regression 또는 classificaion 문제를 해결할 수 있는 기법으로 사용되며, 다음과 같이 문제를 정의할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;max L(y|X;h) = max \sum_{i=1}^{n}logP(y_i|x_i;h)&lt;/script&gt;

&lt;h3 id=&quot;4-1-mle를-이용한-선형회귀&quot;&gt;4-1. MLE를 이용한 선형회귀&lt;/h3&gt;

&lt;p&gt;앞으로 머신러닝을 배우실 여러 분에게 있어서 &lt;strong&gt;선형회기(learning regression)&lt;/strong&gt;는 가장 기본적인 이론이라고 할 수 있으며, 본 과정에서는 간략하게 설명만 하고 넘어가도로 하겠습니다. &lt;strong&gt;*선형회기란 어떠한 이산 데이터가 존재할 때 이 데이터를 표현하는 분포도를 선형으로 하여금 표현하는 것을 의미합니다.&lt;/strong&gt; 그리고 데이터에 존재하지 않는 수치에 대해서도 예측을 할 수 있도록 하는 것이 목적이며, 선횡회기가 잘 될수록 예측 또한 오차가 적어집니다. 즉, 어떠한 모델이라는 함수 $f$가 존재할 때, 이에 대한 예측값인 $\hat{y}$는 다음과 같이 표현을 합니다.
&lt;script type=&quot;math/tex&quot;&gt;\hat{y} = f(x)&lt;/script&gt;
($\hat{}$이라는 표기인 ‘hat’은 보통 예측값에 많이 사용됩니다.)&lt;/p&gt;

&lt;p&gt;그리고 이 $f$는 용어에서 알 수 있듯이, 선형으로 이루어져 다음과 같이 표현될 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\hat{y} = \beta_0 \times x_0 + \beta_1\times x_1 + \beta_2\times x_2 ... + \beta_n\times x_n&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= \sum^{n}_{i=0} \beta_i\times x_i&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pylab&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# plot을 위한 부분&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'target'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;output_28_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같이 빨간선이 우리가 찾고자하는 모델이고, 파란색 점들이 데이터입니다.&lt;/p&gt;

&lt;p&gt;즉, 선형회기의 목적은 위의 $\beta$라는 parameter들을 구하는 것입니다. 그리고 MLE를 적용하면, 우리가 관심있는 모델인 $f$에 대한 parametmer들인 $\beta$의 posterior 분포를 구하는 것이라고 할 수 있습니다. 우리는 이러한 모델을 만드는 과정을 probability density esitimation의 문제로서 다시 정의할 수 있습니다. 즉히, 모델과 그 모델에서 사용되는 파라미터들을 선택하는 일련의 과정을 모델링을 하는 것으로 이 모델을 $h$라는 가정을 하고, 주어진 데이터를 가장 잘 표현하는 $h$를 찾는 것이라고 할 수 있스니다. 그러므로 다음과 같이 우리는 likelihood함수를 최대화하는 $h$를 찾아야합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;max \sum_{i=1}^n logP(x_i; h)&lt;/script&gt;

&lt;p&gt;그리고 Supervised learning이 다음과 같이 conditional probability 문제로서 주어진 input에 대한 output의 확률을 예측하는 것으로 다음과 같이 표현될 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(y|X)&lt;/script&gt;

&lt;p&gt;그렇기 때문에 우리는 위의 문제를 supervised learning의 영역에서 표현을 하면 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;max \sum_{1}^n logP(y_i | x_i;h)&lt;/script&gt;

&lt;p&gt;이제 우리는 $h$라는 모델 또는 함수를 선형회귀 모델로 바꿀 것입니다. 이에 대해서 우리는 하나의 가정을 사용할 것인데, 이는 매우 합리적이라고 할 수 있습니다. 즉, 주어진 데이터의 측정치는 모두 독립적이고, 동일한 확률분포에 의해서 나타나는 확률변수라는 점입니다. 이는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables&quot;&gt;identical independent distribution&lt;/a&gt;라는 정의로서 자세한 설명은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables&quot;&gt;위키&lt;/a&gt;를 참조하시기 바랍니다. 그리고 우리가 예측하고자하는 값인 $(y)$는 평균이 0인 정규분포의 noise를 갖는다고 할 것입니다. 이러한 가정과 함께 우리는 $X$가 주어질 때 $y$를 예측하는 문제를 동일하게 $X$가 주어질 때 정규분포로부터 $y$를 얻기위한 정규분포의 평균을 예측하는 문제로 바꿀 수 있습니다. 이에 대한 수식적인 표현은 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) = \frac{1}{\sqrt{2\times \pi \times \sigma^2}} \times exp(-\frac{1}{2 \times \sigma^2}\times(y - \mu)^2)&lt;/script&gt;

&lt;p&gt;여기서 $\mu$가 우리가 예측하고자하는 정규분포의 평균이고, $\sigma$는 표준편차입니다. 그리고 우리는 이 함수를 likelihood function으로 사용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;앞선 문제의 경우에서 우리는 $h$를 표현하는 데 사용되는 $a$, $b$라는 파라미터를 구하는 것이 목적입니다. ($a$는 직선의 기울기에, $b$는 $y$절편입니다.) 그러므로 위의 식은 아래와 같이 다시 나타낼 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y|x; h) = \frac{1}{\sqrt{2 \times \sigma^2 \times \pi}}e^{-\frac{(y - (a\times x + b))^2}{2\sigma^2}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;log p(y|x, \theta) = -\frac{(y - (ax + b))^2}{2\sigma^2} + C&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;그리고 $log p(y&lt;/td&gt;
      &lt;td&gt;x, \theta)$를 최소화하는 $a$와 $b$를 찾는 것이 우리의 목표입니다. 이 때, 다른 항들은 $a$와 $b$의 변화에 영향을 받지 않으므로 결과적으로 $-(y - (ax + b))^2$에 대한 최소화를 하는 것과 동일하다고 볼 수 있습니다. 그리고 이에 대해서 우리는 최소자승법(least squares)이라고 부르는 방법을 많이 사용합니다. 이렇게 함수의 최소값을 찾는 것은 gradient descent라는 방식으로 하여금 조금씩 기울기를 따라 내려가면서 최소값을 찾습니다. 기울기를 따라가는 것이기 때문에 미분이 사용되며, 특정 변수에 대한 미분이므로 편비분을 하게 됩니다. 따라서, 위의 문제에 대해서는 다음과 같이 편미분이 이루어집니다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$$ \frac{\partial (logP(y&lt;/td&gt;
      &lt;td&gt;x;h))}{\partial a} = -2(y - (ax + b))x $$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$ \frac{\partial (logP(y&lt;/td&gt;
      &lt;td&gt;x;h))}{\partial b} = -2(y - (ax + b)) $$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;이렇게 임의의 점(또는 초기의 시작점이 주어질 수도 있습니다.)에서 시작하여 gradient의 반대 방향(-가 앞에 존재하기 때문)으로 조금씩 이동하는 방식을 &lt;strong&gt;stochastic gradient descent (SGD)&lt;/strong&gt;라 합니다.&lt;/p&gt;

&lt;p&gt;이를 활용하여 다음과 같이 python으로 구해보도록 하겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# gradient descent 계산&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stepsize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0001&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;theta_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stepsize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_grad&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stepsize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_grad&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;theta_list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;********* Results ***********&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'a_hat: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b_hat: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;********* ******* ***********&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        

&lt;span class=&quot;c&quot;&gt;# plot을 위한 부분&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# plt.plot(x, a*x + b, label='target')&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_hat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'regression'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;********* Results ***********
a_hat:  1.0110228329088864
b_hat:  4.498430138396311
********* ******* ***********
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;output_32_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과에서 알 수 있듯이, 우리는 $a$는 1이고, $b$는 5인 target으로부터 데이터를 생성하여 선형모델을 구하는 것을 진행하였습니다. 그리고 그 target이 되는 parameter인 $a, b$에 대해서 각각의 예측값인 $\hat{a}, \hat{b}$는 매우 유사하게 예측이 되었음을 확인할 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Information Theory</title>
   <link href="http://wonchul-kim.github.io/machine%20learning/2020/12/04/information/"/>
   <updated>2020-12-04T00:00:00+09:00</updated>
   <id>http://wonchul-kim.github.io/machine%20learning/2020/12/04/information</id>
   <content type="html">&lt;h1 id=&quot;정보이론-information-theory-120분&quot;&gt;정보이론 (Information Theory) (120분)&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;정보이론&lt;/strong&gt;은 개념적으적으로 설명하면, noisy channel을 통과하는 data와 관련된 수학의 한 부분으로서, 어떠한 메세지에 존재하는 정보의 양을 구체화하는 것을 토대로 합니다. 그리고 이를 더욱 일반적으로 앞서 배운 내용을 토대로 설명하자면, 어떠한 사건과 random variable에 존재하는 정보의 양을 구체화하는 것이며, 이를 &lt;strong&gt;entorpy&lt;/strong&gt;라고 정의하고, 이는 probability를 사용하면 수치적으로 계산이 가능합니다. 이러한 정보와 entropy를 계산하는 것은 머신러닝에서 매우 유용하게 적용되고, 예를 들어서 feature selection, decision tree, classification와 같은 model을 설계할 때 많이 사용되고 있습니다. 따라서, 머신러닝을 하고자 하는 분들에게 있어서 정보와 entropy를 이해하는 것은 매우 중요하다고 할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;1-정의&quot;&gt;1. 정의&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;정보이론&lt;/strong&gt;은 의사소통을 위한 정보의 정량화와 관련된 연구 분야입니다. 수학의 하위 분야로, 데이터 압축과 신호처리의 한계와 같은 주제와 관련이 있습니다. 그리고 이 분야는 미국 전화 회사인 벨 연구소에서 일했었던, Claude Shannon이 제안하고 개발하였고, 다음이 가장 잘 설명한 부분이라 가져왔습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Information theory is concerned with representing data in a compact fashion (a task known as data compression or source coding), as well as with transmitting and storing it in a way that is robust to errors (a task known as error correction or channel coding).&lt;br /&gt;
                                                — Page 56, Machine Learning: A Probabilistic Perspective, 2012.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;정보이론의 기초 개념은 어떠한 사건, random variable, 분포에 존재하는 정보의 양에 대한 정량화입니다. 그리고 이러한 정보의 정량화는 확률을 사용함으로서 가능하므로 확률과 정보이론의 관계는 매우 강하다고 할 수 있습니다. 이러한 정보의 정량화는 머신러닝과 인공지능에서 널리 사용되고 있으며, 예를 들어서 decision tree를 설계하거나 classifier 모델을 최적화하는데에 사용됩니다. 이렇게 머신러닝과 정보이론이 가까운 이유는 다음과 같은 구절에서 확인할 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Why unify information theory and machine learning? Because they are two sides of the same coin. […] Information theory and machine learning still belong together. Brains are the ultimate compression and communication systems. And the state-of-the-art algorithms for both data compression and error-correcting codes use the same tools as machine learning. &lt;br /&gt;
— Page v, Information Theory, Inference, and Learning Algorithms, 2003.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2-사건에-대한-정보량-계산&quot;&gt;2. 사건에 대한 정보량 계산&lt;/h2&gt;

&lt;p&gt;정보이론(information theory)에서 정보(information)는 특정한 관찰에 의해 얼마만큼의 정보를 획득했는지 수치로 정량화한 값을 의미합니다. 예를 들어, 딥러닝에서는 모델 학습에 있어서 얼마나 영향력 있는지, 정보의 파급력 또는 놀람의 정도(surprising degree)로 해석할 수 있습니다. 즉, 정보량은 자주 발생하는 관찰이나 사건에 대해서는 작은 값을 갖고 자주 발생하지 않는 관찰이나 사건에 대해서는 큰 값을 갖게 된다고 할 수 있습니다. 예를 들어서, 공정 데이터기반 불량 검출 문제에서는 정상 생산품의 데이터가 불량 생산품의 데이터에 비해 훨씬 많습니다. 그렇다면, 상대적으로 발생할 확률이 적은 불량 생산품의 관찰이 우리에게 더 많은 정보를 제공할 수 있는 것입니다. 마찬가지로 유전자 정보 기반 특정 질병 예측 문제에서 상대적으로 얻기 어려운 비정상군의 유전자 데이터는 모델 학습에 있어서 더 중요하고 의미있는 데이터들입니다.&lt;/p&gt;

&lt;p&gt;이러한 정량화를 뒷받침하는 직관은 어떠한 사건에 의해서 발생할 수 있는 또는 발생할 수 없는 정보가 얼마나 놀라운 것인지 또는 유용한지를 판단하는 척도가 된다는 것입니다. 즉, 확률이 작은 사건들(거의 발생하지 않을 사건들)들은 더 놀라운 정보가 될 것이고, 반대로 확률이 큰 사건들(대부분 발생할 사건들)에 대해서는 놀랍지 않을 것입니다. 다시 말해서, 놀라울 수록 정보가 많은 것이고, 놀랍지 않을 수록 정보가 더 적다고 할 수 있습니다. 직관적으로 풀어썼기 때문에 이해가 잘 가지 않을 수 있지만, 다음과 같이 정리해보겠습니다. (정보이론에서 실제로 정보에 놀라운 정도, 즉 영어를 그대로 사용하면 surprise로서 표현하고 있습니다.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Low probability event: high information = surprising&lt;/li&gt;
  &lt;li&gt;High probability event: low information = not surprising&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;좀 더 설명하자면, 희귀한 사건들은 더 불확실하여 우리가 예측하기 힘들기 때문에 더 놀라운 것이고, 예측하기 힘들다는 점에서 우리가 얻을 수 있는 정보가 더 많다고 할 수 있습니다. 따라서, 정보이론을 통해서 어떤 경우에도 존재하는 정보의 양을 계산할 수 있습니다. 이러한 정보량을 &lt;em&gt;Shannon information, self-information,&lt;/em&gt; 또는 간단히 &lt;em&gt;information&lt;/em&gt;이라고 명명하고, 다음과 같이 나타냅니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I(x) = -log(p(x))&lt;/script&gt;

&lt;p&gt;$I(x)$는 $x$에 대한 정보양이고, $log()$는 밑이 2이고, $p(x)$는 사건 $x$에 대한 확률을 의미합니다. 이 때, $log()$의 밑이 2인 이유는 정보의 양을 측정하는 단위가 bit (binary digits)로 되어 있기 때문입니다. 이는 소음이 존재하는 통신내에서 발생하는 사건과 비트의 수를 직접적으로 표현하고자 하는 것으로 해석할 수 있습니다.&lt;/p&gt;

&lt;p&gt;앞에 -는 정보양의 결과가 항상 0 또는 양수를 가지기 위함입니다. 정보는 어떠한 사건이 발생할 확률이 1일 경우에는 0이 되고, 이는 무조건 그 사건이 발생한다는 것을 의미하기 때문에 전혀 놀랍지 않은, 즉 정보가 없다는 것을 의미합니다. 그리고 확률에 대한 $log$ 함수이기 때문에 확률이 $0 \leq p(x) \leq 1$이므로, 이에 대한 정보량은 (0, $\infty$]의 값을 갖습니다.&lt;/p&gt;

&lt;p&gt;예를 들어서, 다음과 같은 그래프처럼 나타날 수 있습니다.&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
&lt;a href=&quot;https://imgur.com/yIi8gbI&quot;&gt;&lt;img src=&quot;https://imgur.com/yIi8gbI.png&quot; width=&quot;400px&quot; title=&quot;source: imgur.com&quot; /&gt;&lt;/a&gt;&lt;/div&gt;

&lt;p&gt;$p(A) = 0.99$인 일어날 확률이 높은 사건(A)이 있습니다. 이 사건을 통해서 얻을 수 있는 정보량 또는 사건으로부터 인한 놀람의 정도는 다음과 같습니다. (즉, 정보량은 다음과 같습니다.)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I(A) = -log(p(A)) = -log0.99 = 0.01&lt;/script&gt;

&lt;p&gt;반면, $p(B) = 0.01$으로 일어날 확률이 낮은 사건(B)의 정보량은 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I(B) = -log(P(B)) = -log0.01 = 4.61&lt;/script&gt;

&lt;p&gt;따럿, I(A)는 I(B)보다 훨씬 작은 값(정보량)을 갖습니다.&lt;/p&gt;

&lt;p&gt;이번에는 파이썬으로 코드를 작성하면서 예제를 풀어보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[예제 1]&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;한 개의 동전을 던지는 문제에서 앞면과 뒷면이 나올 확률은 각각 50%입니다. 이를 코드를 통해 확인하면 다음과 같습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 사건에 대한 확률값&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# information&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'p(x)={:.3f}, 정보량: {:.3f} bits'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;p(x)=0.500, 정보량: 1.000 bits
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;즉, 위의 예제에서의 결과는 확률이 0.5로 나오는 하건은 1bit의 정보량을 담고 있다고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번에는 각각의 면이 나올 확률이 1/6으로 동일한 6면의 주사위를 던져보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[예제 2]&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 사건에 대한 확률&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'p(x)={:.3f}, 정보량: {:.3f} bits'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;p(x)=0.167, 정보량: 2.585 bits
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;따라서, 위의 두 예제로 우리는 발생활 확률이 작을수록 정보량은 늘어난다는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;위의 예제를 그래프로 나타내면, 다음과 같이 정보이론에서 확률과 정보량이 반대라는 직관에 대해서 좀 더 쉽게 이해할 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 비교할 사건 확률의 리스트&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 정보량 계산&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;info&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# plot probability vs information&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'.'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Probability'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Information'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;output_10_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 지금까지 $log$의 밑으로서 2를 사용하였지만, 자연로그 또는 10을 사용하여도 계산하는 데에는 무방하지만, 앞서 설명하였듯이 bit가 정보의 단위로서 이를 직접적으로 표현하는 것이므로 대부분의 경우 밑이 2인 $log$를 많이 사용합니다.&lt;/p&gt;

&lt;h2 id=&quot;3-random-variable에-대한-정보량-계산&quot;&gt;3. random variable에 대한 정보량 계산&lt;/h2&gt;

&lt;p&gt;이번에는 이산확률변수(discrete random variable)의 평균 정보량, 평균적인 놀람의 정도, 불확실성 정도를 나타내는 &lt;strong&gt;entropy&lt;/strong&gt;를 정의해보겠습니다. 확률공간 $\Omega$와 이산확률변수 $X = x_1,…, x_N, x_i ∈ R, 1 \leq i \leq N$이 주어져있다고 하겠습니다. 여기서 확률변수 $X = x_1,…, x_N$의 표기법은 확률론에서 주로 사용하는 방법으로 엄밀히 말해서 $X(\Omega) = x_1,…, x_N$을 의미합니다. 그리고&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(X = x_i) := \omega \in \Omega : X(\omega) = x_i&lt;/script&gt;

&lt;p&gt;으로 표기합니다.&lt;/p&gt;

&lt;p&gt;사실 random variable에 대한 정보량을 계산하는 것은 random variable에 대한 사건의 확률분포의 정보량을 계산하는 것과 동일하다고 할 수 있습니다. 그리고 이는 &lt;em&gt;information entropy, Shannon entropy,&lt;/em&gt; 또는 &lt;em&gt;entropy&lt;/em&gt;라고 부릅니다. 따라서, 이들은 모두 불확실성(uncertainty)와 관련이 되어 있습니다. 그리고 이 entropy의 직관은 random variable에 대한 확률분포로부터 나타나는 사건을 나타내거나 전송하기 위해서 필요한 평균 비트 수를 의미하고, 다음의 원문을 참조하여 이해하면 도움이 될 것입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;… the Shannon entropy of a distribution is the expected amount of information in an event drawn from that distribution. It gives a lower bound on the number of bits […] needed on average to encode symbols drawn from a distribution P . &lt;br /&gt;
— Page 74, Deep Learning, 2016.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;따라서, $K$개의 discrete state를 가지는 random variable($X$)에 대애서 다음과 같이 entropy를 계산할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(X) = -\sum^{K}_{i=1} p(k_i) \times log(p(k_i))&lt;/script&gt;

&lt;p&gt;앞선 사건에 대한 정보량인 $I()$와 마찬가지로 -의 부호를 가지고 있으며, 각각의 사건에 대한 확률과 확률의 $log$를 곱한 값의 총합으로 구해집니다. 그리고 마찬가지로 $log$의 밑은 2를 많이 사용합니다. 위의 계산식을 보면, 직관적으로 확률이 1인 단일 사건을 가지는 random variable에 대한 entropy를 계산할 때 나타나며, 가장 큰 entropy는 모든 사건의 확률이 동일한 경우에 대한 random variable의 entropy을 계산할 때 구해집니다.&lt;/p&gt;

&lt;p&gt;여기서 $p(k_i) := P(X = x_i)$입니다. 즉, 평균 정보량 $H(X)$는 $X$분포에서 $h$의 기댓값인 $E[−logP(X)]$을 의미합니다.&lt;/p&gt;

&lt;p&gt;예를 들어서, 다음의 $X = 0, 1$인 확률 공간에서 확률값이 다른 3가지 예를 살펴보도록 하겠습니다. $X = 0, 1$이므로 평균 정보량 $H[X]$는 다음과 같이 쓸 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(X) = −[P(X = 0)logP(X = 0) + P(X = 1)logP(X = 1)]&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;첫 번째 경우
&lt;script type=&quot;math/tex&quot;&gt;P(X = 0) = 0.5&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X = 1) = 0.5&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(X) = −(0.5log0.5 + 0.5log0.5) = 0.69&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;두 번째 경우
&lt;script type=&quot;math/tex&quot;&gt;P(X = 0) = 0.8&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X = 1) = 0.2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(X) = −(0.8log0.8 + 0.2log0.2) = 0.50&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;세 번째 경우
&lt;script type=&quot;math/tex&quot;&gt;P(X = 0) = 1&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X = 1) = 0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(X) = −(1log1 + 0log0) = 0&lt;/script&gt;

&lt;p&gt;여기서 $0log0 := 0$으로 정의합니다. 위 경우에서 첫 번째 경우인 $P(X = 0) = 0.5, P(X = 1) = 0.5$ 일 때 평균 정보량이 가장 많았고, 세 번째 경우인 $P(X = 0) = 1, P(X = 1) = 0$ 일 때 평균 정보량이 0으로 가장 적었습니다. 즉, 불확실성이 없다고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;다음으로는 일반적인 $X = x_1,…, x_N$의 경우에 대해서 평균 정보량 $H(X)$가 최대가 되는 $p_i$ 조합을 찾아보도록 하겠습니다. $p_i$들이 확률변수 $X$가 가지는 확률 값들이기 때문에 다음 조건을 만족합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_i^N p_i = 1&lt;/script&gt;

&lt;p&gt;위 조건을 만족시키는 $p_i$들 중에서 $H(X)$가 최대값을 갖는 조합은 라그랑쥬 승수법으로 얻을 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_i = \frac{1}{N}, 1 \leq i \leq N&lt;/script&gt;

&lt;p&gt;결론적으로 이산확률변수 $X$가 균일분포(uniform distribution)일 때 평균 정보량 $H(x)$이 최대값을 갖개 됩니다. 그리고 연속확률변수일 때는 정규분포(normal distribution)일 때 평균 정보량이 최대값을 갖게 되므로, 위 예에서 첫 번째 경우인 $P(X = 0) = 0.5, P(X = 1) = 0.5$일 때의 평균 정보량인 0.69가 최댓값입니다.&lt;/p&gt;

&lt;p&gt;이번에는 코드를 통해서 앞선 예제와 마찬가지로, 1/6의 모두 동일한 확률로 6면이 나오는 주사위를 던질 때의 variable에 대한 entropy를 계산해보겠습니다. 각각의 결과는 1/6의 확률을 가지므로 uniform probability distribution입니다. 그러므로 앞선 &lt;strong&gt;[예제2]&lt;/strong&gt;에서 정보량을 계산한 값과 동일할 것으로 예상할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[예제 3]&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 사건 횟수&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 사건 하나당 확률&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# entropy&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entropy: {:.3f} bits'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;entropy: -2.585 bits
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;동일한 확률분포의 사건이 도출되는 random variable이 아닌 치우친 확률 분포와 같이 하나의 사건이 지배적인 경우를 가지는 random variable도 존재할 것이다. 그리고 마찬가지로 지배적인 사건에 대해서는 전혀 놀랍지 않으므로 정보량 또한 적을 것이므로 entropy는 낮을 것이고, 하나의 사건에 대해서 치우치지 않고, 모든 사건이 동일하거나 비슷한 확률분포를 가질 수록 entropy는 크게 계산됩니다. 이를 정리하면 다음과 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Skewed Probability Distribution (unsurprising): Low entropy.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Balanced Probability Distribution (surprising): High entropy.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그렇다면, 이번에는 확률분포에 따른 entropy가 어떻게 변화하는지 그리고 서로 어떤 관계인지에 대해서 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[예제 4]&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# entropy&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;events&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;events&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 비교할 확률 리스트&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 확률분포&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_distr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 각각의 분포에 대한 entropy&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ents&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_distr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;# plot probability distribution vs entropy&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'.'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'probability distribution'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entropy (bits)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;output_19_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 예제를 보면, 두 사건에 대한 확률이 [0, 1]과 같이 치우친 확률분포를 가진 경우에서 [0.5, 0.5]의 uniform distribution을 가지는 확률분포로 갈수록 entropy가 증가하는 것을 확인할 수 있습니다. 다시 말해서, 어떠한 확률분포에서 평균적으로 발생하는 사건이 예상 가능 (또는 놀랍지 않은, not surprising)하다면 낮은 entropy로 계산이 됩니다. 반면에, 발생한 사건이 예상 가능하지 않은 (또는 놀라운, surprising) 경우에 대해서는 높은 entropy가 나옵니다. 그리고 위의 코드에서 entropy를 계산하는 함수에서 ets는 확률이 0이 되어 $log()$의 결과가 무한대가 가는  경우를 피하기 위함입니다.&lt;/p&gt;

&lt;p&gt;지금까지 살펴본 정보이론, 즉 random variable에 대한 entropy를 계산하는 것은 앞으로의 머신러닝 또는 딥러닝에서 많이 사용되는 mutual information (information gain)과 같이 또 다른 정보량을 계산하는 방법에 기본이 되는 이론이라고 할 수 있습니다. 또한 두 확률분포의 차이점을 계산할 수 있는 cross-entropy와 KL-divergence에서도 기본이 되는 개념으로서 정보이론은 매우 중요하기 때문에 반드시 이해하고 가시길 바랍니다.&lt;/p&gt;

&lt;h2 id=&quot;4-kl-divergence&quot;&gt;4. KL Divergence&lt;/h2&gt;

&lt;p&gt;Kullback-Leibler Divergence, KL divergence는 어떤 확률 분포가 다른 확률분포와 얼마나 다른지를 알려주는 지표라고 할 수 있습니다. 이에 대한 표기는 다음과 같이 나타낼 수 있습니다. 예를 들어서, 두 확률 분포를 $Q와 P$라고 하겠습니다. 이에 대한 KL divergence는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;KL(P\parallel Q)&lt;/script&gt;

&lt;p&gt;입니다. “”$\parallel$”” 표기는 ‘divergence’를 의미하고, 위의 표기는 P의 분포가 Q와 얼마나 다른지를 의미합니다. 그리고 KL divergence는 다음과 같이 계산을 할 수 있도록 정의되고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;KL(P \parallel Q) = –\sum_x P(x) \times log(frac{Q(x)}{P(x)})&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= \sum_x P(x) \times log(frac{P(x)}{Q(x)})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;위와 같은 계산식에 대한 KL divergence의 직관은 두 확률 분포의 차이를 나타내는 것으로 다음과 같습니다. P로 부터 일어나는 사건의 확률이 커지고, Q의 확률이 작아질 때 KL divergence는 커집니다. 마찬가지로 반대의 상황인 P의 확률이 작아지고, Q가 커질수록 KL divergence는 커질 것입니다.&lt;/p&gt;

&lt;p&gt;따라서, KL divergnece는 이산 확률분포와 연속 확률분포사이의 차이를 알고자 할 때도 사용할 수 있으며, 다음과 같이 정의되어 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;One way to measure the dissimilarity of two probability distributions, p and q, is known as the Kullback-Leibler divergence (KL divergence) or relative entropy. &lt;br /&gt;
— Page 57, Machine Learning: A Probabilistic Perspective, 2012.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 때, $log$는 위에서 설명한데로 2를 밑으로 하는 것이 보통이며, 자연로그로서 밑을 e로 하여도 무방합니다. 결과적으로 두 확률분포에 대한 KL divergence의 값이 0이라면, 두 분포는 동일하다고 할 수 있으며, 그렇지 않으면 KL divergence는 0보다 큰 수를 갖습니다.&lt;/p&gt;

&lt;p&gt;그리고 KL divergence는 다음과 같은 중요한 특징을 가지고 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;KL(P\parallel Q) \neq KL(Q\parallel P)&lt;/script&gt;

&lt;p&gt;즉, KL divergence는 계산하는 데에 있어서 symmetric이 아니라는 점입니다. (한글로는 좌우대칭이라고 할 수 있겠습니다.)&lt;/p&gt;

&lt;p&gt;또한, KL divergence는 ‘relative entropy’라고도 언급되며, 많은 책에서도 다음과 같이 함꼐 정의되어 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is known as the relative entropy or Kullback-Leibler divergence, or KL divergence, between the distributions p(x) and q(x). &lt;br /&gt;
— Page 55, Pattern Recognition and Machine Learning, 2006.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위에 긴글로 KL divergence를 설명하였지만, 단순하게 정리하자면 KL divergence는 두 분포의 차이를 알려주는 것이라고 할 수 있습니다. 그리고 하나의 분포를 알고 있고 알지 못하는 분포에 대해서 추정을 하는 경우에도 사용이 가능하기 때문에 더욱 유용하게 딥러닝과 머신러닝에서 사용되는 측면이 있습니다. 이제부터는 코드를 통해서 KL divergence를 직접 구현해보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[에제 5]&lt;/strong&gt;
다음과 같이 세 개의 종류의 색깔이 있습니다. 그리고 우리는 두 가지의 다른 분포로 세 번 색을 선택하는 사건에 대해서 KL divergence를 적용하여 보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# plot을 위한 library&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# log2계산을 위한 library&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;먼저 확률 분포를 정의합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;events&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'green'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'black'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.45&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.70&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;정의한 확률분포에 대해서 각 확률분포는 확률의 정의에 따라 합이 모두 1입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'P={} Q={}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;P=1.0 Q=1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;정의한 사건에 대해서 히스토그램으로 나타내면 다음과 같습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;events&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;events&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;output_28_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;실제로 확률분포에 따른 각각의 사건이 일어나는 정도와 차이를 눈으로 확인할 수 있습니다. 이번에는 KL divergence를 통해서 그 차이를 지표로 나타내어 보겠습니다.&lt;/p&gt;

&lt;p&gt;먼저, KL divergence에 대한 식을 정의합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;kl_divergence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
                   
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;kl_pq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kl_divergence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'KL(P || Q): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.3&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f bits'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kl_pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;kl_qp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kl_divergence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'KL(Q || P): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.3&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f bits'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kl_qp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;KL(P || Q): 1.662 bits
KL(Q || P): 1.694 bits
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;위와 같이 KL divergence를 지표로 나타내어 보았습니다. 그리고 앞서 설명하였듯이, KL divergence는 symmetric하지 않기 때문에 서로 주어진 확률분포가 어떤 것이냐에 따라서 서로 다른 KL divergence를 갖는 다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;5-cross-entropy&quot;&gt;5. Cross-entropy&lt;/h2&gt;

&lt;p&gt;Cross-entropy는 앞서 설명한 KL divergence와 매우 유사한 개념이라고 할 수 있습니다. 정확히는 개념뿐만 아니라 계산식으로서 KL divergence로 유도할 수도 있습니다. 구체적으로 말하자면, KL divergence는 두 확률분포에서 기준이 되는 확률분포를 통해서 다른 확률분포를 나타내기 위한 여분의 평균 bits를 측정하는 것이라고 할 수 있으며, 다음과 같이 책에서 정의하고 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In other words, the KL divergence is the average number of extra bits needed to encode the data, due to the fact that we used distribution q to encode the data instead of the true distribution p. &lt;br /&gt;
— Page 58, Machine Learning: A Probabilistic Perspective, 2012.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그렇기 때문에 KL divergence는 relative entropy라고 언급되기도 하는 것입니다.&lt;/p&gt;

&lt;p&gt;이에 반해 Cross-entropy는 여분의 평균 bits가 아닌 전체 bits를 측정하는 것이라고 할 수 있습니다. 이 차이는 오히려 한글로 설명하는 데에 있어서 더 오해를 부를 수 있기 때문에 다음과 같이 영어 그대로의 표현으로 정의를 살펴보겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cross-Entropy: Average number of total bits to represent an event from Q instead of P.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Relative Entropy (KL Divergence): Average number of extra bits to represent an event from Q instead of P.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이제부터는 Cross-entropy를 어떻게 구할 수 있는지에 대해서 다루어보겠습니다. 앞서 말했듯이, Cross-entropy는 KL divergence를 이용하여 유도할 수 있습니다. 결과적으로는 Cross-entropy인 $H(P, Q)는 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(P, Q) = H(P) + KL(P \parallel Q)&lt;/script&gt;

&lt;p&gt;즉, Corss-entropy는 어떤 확률분포에 대한 entropy와 KL divergence의 합으로서 구할 수 있습니다. 각각의 entropy와 KL divergence를 구하는 방법은 위에서 설명하였기 때문에 생략하곘습니다. 그리고 KL divergence와 마찬가지로 Cross-entropy도 symmetric하지 않습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(P, Q) \neq H(Q, P)&lt;/script&gt;

&lt;p&gt;이번에는 코드를 통해서 Cross-entropy를 직접 구현하여 보겠습니다. 앞선 &lt;strong&gt;[예제 5]&lt;/strong&gt;와 동일한 예를 사용하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[예제 6]&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;먼저 확률 분포를 정의합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# plot을 위한 library&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# log2계산을 위한 library&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;events&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'red'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'green'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'black'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.45&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.70&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;그리고 Cross-entropy를 계산하는 식을 다음과 같이 함수로서 만듭니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cross_entropy0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;이에 대해서 Cross-entropy는 다음과 같이 나타납니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ce_pq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_entropy0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'H(P, Q): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.3&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f bits'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ce_pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ce_qp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_entropy0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'H(Q, P): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.3&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f bits'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ce_qp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;H(P, Q): 3.257 bits
H(Q, P): 3.013 bits
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;이 때, 동일한 확률분포, 즉 자기 자신을 자신에게 비교하면 어떤 Cross-entropy가 나올지 확인해보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ce_pp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_entropy0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'H(P, P): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.3&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f bits'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ce_pp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ce_qq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_entropy0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'H(Q, Q): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.3&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f bits'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ce_qq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;H(P, P): 1.595 bits
H(Q, Q): 1.319 bits
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;이번에는 앞서 설명하였듯이,  KL divergence와 entropy의 앞으로서 Cross-entropy를 구하여보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;kl_divergence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cross_entropy1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kl_divergence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;en_p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'H(P): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.3&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f bits'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;en_p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;kl_pq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kl_divergence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'KL(P || Q): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.3&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f bits'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kl_pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ce_pq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_entropy1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'H(P, Q): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.3&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f bits'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ce_pq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;H(P): 1.595 bits
KL(P || Q): 1.662 bits
H(P, Q): 3.257 bits
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Classification</title>
   <link href="http://wonchul-kim.github.io/machine%20learning/2020/12/04/classification/"/>
   <updated>2020-12-04T00:00:00+09:00</updated>
   <id>http://wonchul-kim.github.io/machine%20learning/2020/12/04/classification</id>
   <content type="html">&lt;h1 id=&quot;classification-120분&quot;&gt;Classification (120분)&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;이란 말그대로 범주화/분류화를 하는 것을 의미합니다. 데이터에 빗대어서 설명하자면, 여러가지 내용을 포함하는 데이터가 있을 때, 이 데이터를 해석하여 이해하기 쉽도록 정리하는 것이라고도 할 수 있습니다. 예를 들어, 실생활에서 많이 사용되는 이메일에 스팸을 자동으로 필터링해주는 기능이 있습니다. 과거에는 스팸을 신고하고 신고한 메일주소에서 수신되는 메일에 대해서만 스팸으로 범주화하였지만, 최근에는 머신러닝을 통하여 메일의 제목 또는 발송 주소 등의 데이터를 해석하여 스팸을 자동으로 필터링해주고 있습니다. 즉, 주어진 데이터가 있는데 이 데이터는 어떤 범주에 속하는지를 판단해주는 기능을 합니다. 이렇듯, classification은 실생활에서 매우 근접하게 사용되고 있는 머신러닝의 영역 중 하나로서 주어진 데이터의 판별을 위한 방법으로 많이 사용되고 있습니다.&lt;/p&gt;

&lt;p&gt;본 강의는 머신러닝에서의 classification에 대해서 배울 것이며, 그 기초가 되는 &lt;strong&gt;Naive Classifier&lt;/strong&gt;에 대해서 먼저 알아보도록 하겠습니다. Naive classifer는 주어진 문제에 대해서 어떤 가정도 하지 않은 모델로서 성능을 비교하고자 하는 다른 모델들의 기준이 되는 역할을 하는 classifier를 의미합니다. 이러한 Naive classifer를 설계하는 데에는 주어지는 데이터셋과 성능지표에 따라 여러 가지 방법이 존해합니다. 그리고 가장 보편적으로 사용되는 성능 지표로는 주어진 데이터로부터 임의로 선택된 class의 label을 추측하는 데에 있어서 얼마나 정확하게 선택한 class에 대해서 label을 많이 맞추는지의 분류 정확도(classification accuracy)를 사용합니다.&lt;/p&gt;

&lt;h2 id=&quot;1-naive-classifier&quot;&gt;1. Naive classifier&lt;/h2&gt;

&lt;p&gt;Classification을 통한 주어진 데이터를 판단하는 문제는 데이터로부터의 어떤 class를 모델에 input으로 넣었을 때 그에 맞는 label을 얻는 것이고, classificaion을 위한 모델은 학습용 데이터에 대해서 학습을 진행하고 테스트용 데이터를 통해서 검증을 함으로서 성능을 결정합니다. 그리고 이에 대한 성능인 정확도(accuracy)는 예측을 하여 맞은 횟수를 예측을 한 총 횟수로 나눈 것으로 나타냅니다. 하지만, 이 정확도는 어떤 classificaion을 위한 모델에 대한 성능을 수치로서 나타낼 뿐이며, ‘이 모델이 성능이 좋은가? 좋지 못한가?’에 대해서는 답이 되지 못합니다. 그렇기 때문에 비교의 기준이 되는 모델이 필요한 것이고, naive classifier가 이 기준이 되는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;naive classifier는 기본적으로 예측을 하기 위해서 어떠한 기법도 사용하지 않는 무작위한 또는 동일한 예측을 하도록 합니다.&lt;/em&gt;&lt;/strong&gt; 그렇기 때문에 말 그대로 naive라는 표현을 하는 것이고, 다시 말해서 domain에 대해서 어떠한 사전지식을 사용하지 않으며 예측을 하기 위해서 어떠한 learning도 없습니다. 그리고 naive classifier는 앞서 말했듯이 기준점이 되는 것이므로 설계한 classification model에 대해서 최소 성능(lower bound)이 된다고도 할 수 있습니다. 즉, classification model을 설계하고자 한다면 최소한 naive classifier보다는 성능이 좋게 나와야합니다. 예를 들어서 naive classifier보다 성능이 좋지 못하다면, 설계한 classification model은 분류라는 성능이 없다고 보아도 됩니다. 그렇다면 navie classifier는 어떻게 설계를 해야하는지에 대해서 알아보겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;random class 예측&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Randomly selected class 예측&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Majoriy class 예측&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위와 같이 여러 naive classifier가 존재하며 성능이 동일하지 않을 것입니다. 그리고 우리는 앞으로 설계할 classifier의 성능을 최대한 좋게 만드는 것이 목적이기 때문에 비교대상이 되는 naive classifier도 마찬가지로 주어진 문제에서 가장 성능이 좋은 것을 사용해야합니다.&lt;/p&gt;

&lt;p&gt;먼저, 앞으로의 naive classifier를 설명하기 위한 데이터셋을 다음과 같이 하도록 하겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 테스트 데이터셋&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;class_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;class_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_1&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 각 분포에 대한 프린트&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;' Class 0: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;' Class 1: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;우리는 두 가지로 구분이 되는 데이터셋(&lt;code class=&quot;highlighter-rouge&quot;&gt;class 0&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;class 1&lt;/code&gt;)을 설정하였습니다. 그리고 각각의 &lt;code class=&quot;highlighter-rouge&quot;&gt;class&lt;/code&gt;에 대해서 데이터셋의 비율은 1:3으로 분포가 이루어져 있도록 설정하였습니다. 즉, 전체 데이터의 &lt;code class=&quot;highlighter-rouge&quot;&gt;class&lt;/code&gt; 중에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;class 0&lt;/code&gt;이 20%이고, &lt;code class=&quot;highlighter-rouge&quot;&gt;class 1&lt;/code&gt;이 80%의 비율로 구성되어 있다는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;그리고 우리는 naive classfier의 성능을 검증할 수 있는 확률을 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\hat{y} = y)&lt;/script&gt;

&lt;p&gt;이는 다음과 같이 각각의 경우에 대한 확률을 통해서 계산됩니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\hat{y} = y) = P(\hat{y} = 0) \times P(y = 0) + P(\hat{y} = y) \times P(y = 1)&lt;/script&gt;

&lt;p&gt;이를 통해서 우리는 주어진 데이터에 따르는 모델의 성능을 확인할 수 있습니다. 특히, 이렇게 확률을 활용하면 계산이 매우 간단하다는 점에서 우리는 대부분의 다른 방식의 naive classifier에도 통용이 가능하다는 점에서 이점을 가지고 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;1-1-random-guess-예측&quot;&gt;1-1. Random Guess 예측&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;random-guess strategy&lt;/strong&gt;은 가능한 class에 대해서 예측을 할 때, 무작위로 답을 고르는 것입니다. 각각의 class에 대하여 무작위로 추측하는 것은 가능한 각 class의 label에 대해서 uniform probability distribution을 통해 이루어지고, 앞선 예제와 같이 두 가지의 class를 가지는 경우에는 각각의 class에 대해서 0.5의 확률을 갖게 됩니다. 또한, 앞서 설명하였듯이 우리는 &lt;code class=&quot;highlighter-rouge&quot;&gt;class 0&lt;/code&gt;과 &lt;code class=&quot;highlighter-rouge&quot;&gt;class 1&lt;/code&gt;에 대해서 expected probability가 각각 0.25, 0.75인 것을 알고 있습니다. 그러므로 우리는 이 방식에 따른 평균 성능을 다음과 같이 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;P(\hat{y} = y) = P(\hat{y} = 0) \times P(y = 0) + P(\hat{y} = 1) \times P(y = 1)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= 0.5 \times 0.2 + 0.5 \times 0.8&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= 0.5&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;따라서, 주어진 문제에 대해서 uniform probability distribution에 의해서 무작위로 class의 label을 추측하는 것의 성능은 50%의 classification accuracy를 갖는다는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;다음은 0.5의 확률로 random guess를 하는 파이썬 코드입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;randomly_predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;위와 같이 하나의 모듈로서 작성한 것은 iteration을 통해 연속적으로 random guess를 해야하는 경우에 편의성을 제공하기 위함입니다. 여러분들도 앞으로는 하나의 기능을 수행하는 함수에 대해서는 모듈로서 작성하여 불러서 사용하는 것을 권장합니다. 즉, 다음과 같이 1000번의 예측을 한느 경우에 대해서는 위와 같이 function을 작성하여 각각의 예측마다 불러서 활용할 수 있도록 call-back function으로 적용합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;먼저 계산에서 사용할 수식이 들어 있는 라이브러리를 불러옵니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;randomly_guess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;무작위한 수를 생성했을 때, 0.5보다 작으면 0을 return하고, 0.5보다 큰 경우에는 1을 return하는 함수입니다. 함수명에서도 알 수 있듯이 무작위하게 0과 1을 return해주는 것을 알 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;class_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;class_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 성능값 계산&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randomly_guess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;acc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;acc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'평균 정확도: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;따라서, 1000번의 예측을 했을 때 모델의 성능은 기존에 우리가 확률을 이용하여 예상한 값인 0.5와 매우 비슷하다는 것을 확인할 수 있습니다. 이는 1000번이라는 실험 횟수가 적당했다는 것을 의미하고, 10번 또는 횟수가 적어질수록 0.5에서 멀어지는 값이 나올 것입니다. 즉, 주어진 데이터를 통해서 모델의 정확도의 성능은 우리가 확률을 통해서 계산하는 값으로 수렴하기 위해서는 그만큼 충분한 실험 횟수가 필요하다는 것을 알 수 있습니다. 9가 나왔습니다. 이 에제의 경우에는 기존의 한 번의 예측에 대한 정확도가 1000번의 예측에 정확도와 매우 비슷하다는 것을 확인할 수 있습니다. 이는 &lt;a href=&quot;https://ko.wikipedia.org/wiki/%ED%81%B0_%EC%88%98%EC%9D%98_%EB%B2%95%EC%B9%99&quot;&gt;Law of large numbers&lt;/a&gt;라는 법칙으로서 참조하시기 바랍니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Low of large numbers란 위의 참조에서 더욱 자세히 알 수 있지만, 간단히 설명하자면 다음과 같습니다. 어떠한 큰 모집단에서 무작위로 뽑은 표본의 평균이 전체 모집단의 평균과 가까울 가능성이 높다는 통계와 확률 분야의 기본 개념이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;1-2-randomly-selected-class-예측&quot;&gt;1-2. Randomly Selected Class 예측&lt;/h3&gt;

&lt;p&gt;이번에는 다른 방식으로 naive classifier를 설계해보도록 하겠습니다. 우리는 학습용 데이터셋에서 관측 데이터를 무작위로 선택할 수 있고 각각의 예측에 대해서 그 값을 결과로 제시할 수 있습니다. 그리고 아마도 학습용 데이터셋을 활용하는 방식이 오히려 &lt;strong&gt;1-1&lt;/strong&gt;처럼 무작위적으로 추측을 하는 방식보다는 좀 더 좋은 결과를 보여줄 수 있을 것입니다.&lt;/p&gt;

&lt;p&gt;이번에도 마찬가지로 확률을 활용하여서 접근하고자 하는 방식에서 기대되는 성능을 계산할 수 있습니다. 만약 uniform probability distribution으로 학습용 데이터셋으로부터 관측되는 데이터 샘플을 선택한다면, &lt;code class=&quot;highlighter-rouge&quot;&gt;class 0&lt;/code&gt;의 확률은 20%이고, &lt;code class=&quot;highlighter-rouge&quot;&gt;class 1&lt;/code&gt;의 확률은 80%가 될 것입니다. 그리고 이는 각각 독립적입니다. 이러한 것을 고려한다면, 다음과 같이 계산식을 세울 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;P(\hat{y} = y) = P(\hat{y} = 0) \times P(y = 0) + P(\hat{y} = 1) \times P(y = 1)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= 0.2 \times 0.2 + 0.8 \times 0.8&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= 0.68&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;즉, 약 68%의 정확도를 예상할 수 있다는 것이고, 이는 앞선 무작위로 추측하는 방식(50%)보다 더 좋은 결과라는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;randomly_predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;selected_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selected_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;이렇게 function으로 만들었기 때문에 1000번의 예측을 하는 반복문에서 활용하기 용이합니다. 그리고 앞선 예제와 마찬가지로 평균 성능을 계산하여 기존에 우리가 예측한 정확도가 나오는지 확인하겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;먼저 계산에서 사용할 수식이 들어 있는 라이브러리를 불러옵니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# randomly selected class로부터 예측값을 얻어내는 func.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;randomly_select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;selected_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selected_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 데이터셋&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;class_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;class_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;0과 1에 대한 데이터 셋을 1:3의 비율로 만듭니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 성능값 계산&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randomly_select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;acc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;acc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'평균 정확도: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;앞서, 우리는 확률을 이용한 계산에서 정확도가 약 68%가 나오는 것을 기대하였고, 1000번의 예측에 대한 평균 정확도는 위의 결과와 같이 비슷하다는 것을 확인할 수 있었습니다. 첫번쨰의 방법은 정말 단순하게 적용한 것이고, 두 번째에는 첫 번쨰 방법에서 학습용 데이터셋을 활용한다는 점이 개선되었습니다. 그렇다면, 앞선 둘의 공통점인 uniform probability distribution을 활용하지 않는다면 더 개선된 정확도를 볼 수 있을지 다음에서 확인해보도록 하겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;3-majority-class-예측&quot;&gt;3. Majority Class 예측&lt;/h3&gt;

&lt;p&gt;이전에는 모두 uniform probability distribution을 사용하여 class의 label을 예측하였습니다. 하지만, 저희가 정의한 데이터셋은 각각의 두가지의 &lt;code class=&quot;highlighter-rouge&quot;&gt;class&lt;/code&gt;가 존재하고 각각이 차지하는 비율이 1:1이 아닙니다. 즉, &lt;strong&gt;1-1&lt;/strong&gt;과 &lt;strong&gt;1-2&lt;/strong&gt;에서 가정하였던 uniform probability distrbution은 좋지 않은 추측이었다고 할 수 있습니다. 그렇기 때문에 이번에는 이를 개선한 naive classifier를 소개하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;대신에 우리는 다수의 class를 예측하여 적어도 학습용 데이터셋에서 다수의 class가 차지하는 비율만큼 높은 정확도를 얻을 수 있습니다. 예를 들어서, 학습용 데이터셋에서 &lt;code class=&quot;highlighter-rouge&quot;&gt;class 1&lt;/code&gt;이 차지하는 비율이 80%이고, &lt;code class=&quot;highlighter-rouge&quot;&gt;class 1&lt;/code&gt;을 모두 예측한다면, 우리는 적어도 정확도가 80%보다 높은 naive classifier를 설계할 수 있습니다. 그리고 앞선 방식들과 마찬가지로 확률을 활용하여 이를 확인하도록 하겠습니다. 주의할 점은 &lt;code class=&quot;highlighter-rouge&quot;&gt;class 0&lt;/code&gt;의 확률을 0으로 할 것이고, &lt;code class=&quot;highlighter-rouge&quot;&gt;class 1&lt;/code&gt;의 확률은 1로 하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;P(\hat{y} = y) = P(\hat{y} = 0) \times P(y = 0) + P(\hat{y} = 1) \times P(y = 1)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= 0.0 \times 0.2 + 1.0 \times 0.8&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= 0.8&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;위의 계산을 통해서 우리는 다수의 class를 예측하게 된다면 80%라는 높은 정확도를 가질 수 있음을 확인하였습니다. 이는 앞선 두 방식보다도 높은 성능을 나타내지만, 데이터에서 각각의 &lt;code class=&quot;highlighter-rouge&quot;&gt;class&lt;/code&gt;가 차지하는 비율이 한쪽으로 치우치는 경우가 심할 수록 높은 정확도를 가진다고 할 수 있습니다. 그렇기 때문에 데이터가 구성되는 비율이 어떻게 되어 있는지에 대해서 판단을 하고 사용을 해야한다는 것을 다시 한번 확인할 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;select_majority&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;이 때, scipy의 python library에서 mode()라는 function이 사용되는 데, 이는 주어진 argument에서 가장 보편적인 값을 찾아주는 것입니다. 따라서, 데이터를 argument로 넘기면 가장 다수를 차지하는 값을 찾아줍니다. 그리고 마찬가지로 여러 번의 예측 횟수를 통해서 평균 정확도를 계산할 것입니다. 하지만, 앞선 두 방식처럼 단순히 예측횟수를 늘리지 않을 것이며, 데이터셋의 크기만큼만 반복할 것입니다. 이는 앞선 두 방식에서는 무작위로 선택한다는 점이 있지만 이번에는 무작위로 하는 행위가 포함되어 있지 않기 때문입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;select_majority&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 데이터셋&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;class_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;class_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;0과 1에 대한 데이터 셋을 1:3의 비율로 만듭니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 예측값&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select_majority&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 정확도 계산&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'정확도: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;위의 결과를 통해서 정확도가 75%로서 우리가 예측한 값도 동일하게 나온 것을 확인할 수 있습니다. 이는 앞선 두 방식보다도 높은 것이므로, 우리는 앞서 말했듯이 여러 naive classifier를 설계할 수 있지만, 그 중에서 가장 성능이 좋은 것을 선택해야하므로 다수의 class를 활용하는 세 번째 방식을 택해야할 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;2-naive-bayes-classifier&quot;&gt;2. Naive Bayes Classifier&lt;/h2&gt;

&lt;p&gt;앞서 배운 naive classifier는 사실 데이터 자체를 그대로 사용하고 있으며, 데이터에서 얻을 수 있는 다른 정보들은 사용하지 않았습니다. 아마도 그렇기 때문에 다른 고차원적인 classifier의 성능을 판별할 수 있는 기준이 되는 것이기도 하다고 생각합니다. 이번에는 데이터에서 얻을 수 있는 정보를 활용하는 &lt;strong&gt;naive bayes classifier&lt;/strong&gt;에 대해서 알아보도록 하겠습니다. naive bayes classifier는 이름에서도 알 수 있듯이, bayes theorem의 심플하면서도 강력한 계산식을 활용하는 classifier라고 할 수 있습니다. 그리고 bayes theorem에서 가장 중요한 부분이 condiditional probability를 다루어서 계산식을 정리한다는 점이고, 마찬가지로 classification 문제에서도 주어진 데이터에 대해서 conditional probability를 정의함으로서 classificaion을 수행합니다.&lt;/p&gt;

&lt;p&gt;설명하기에 앞서, 앞선 설명에서처럼 수식을 다루는데에 있어서 observation이나 input에 해당하는 데이터는 $X$라고 지칭하고, 모델을 통해서 생성되는 또는 예측되는 결과값을 $y$라고 명명하도록 하겠습니다. 따라서, 모델을 $X$에 대한 $y$로의 mapping이라고 생각하면 함수 $f$라고 생각할 수 있기 때문에 일반적으로 다음과 같이 보편적으로 간단하게 표현하기도 합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = f (X)&lt;/script&gt;

&lt;p&gt;그리고 $f$를 만들기 위한 또는 학습시기키 위한 지표가 있어야 하며, 이 지표는 cost 또는 loss라고 말하며 classification에 대한 error를 낮추는 것을 목표로 합니다.&lt;/p&gt;

&lt;p&gt;앞서 말했듯이, bayes theorem을 활용하는 classifier이기 때문에 데이터를 확률적인 측면으로 재해석할 필요가 있습니다. 그리고 이때 conditional probability가 고려되는 것이며, 이 conditional probability를 계산하기 위한 방법이 naive bayes인 것입니다. 따라서, 우리가 데이터로부터 알고자 하는 것은 데이터가 주어질 때, 이 데이터가 어느 범주에 속하느냐는 conditional probability라는 것입니다. 예를 들어서, 다음과 같이 $y_1 , y_2 , · · · , y_k$ 에 대해서 $X_1 , X_2 , · · · , X_n, X = {x_1, x_2, ,,, x_n}$의 데이터가 주어졌을 때, 우리는 다음과 같이 conditional probability를 정의할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P (y_i |x_1 , x_2 , · · · , x_n )&lt;/script&gt;

&lt;p&gt;이에 대해서 bayes theorem은 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}&lt;/script&gt;

&lt;p&gt;그리고 이러한 이론을 앞선 classification 문제에 적용하면 다음과 같이 계산이 가능하도록 우리가 원하는 결과를 얻을 수 있는 것입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(y_i|x_1, x_2, ..., x_n) = \frac{P(x_1, x_2, ..., x_n|y_i)\times P(y_i)}{P(x_1, x_2, ..., x_n)}&lt;/script&gt;

&lt;p&gt;먼저, 다음과 같이 $P(y_i)$는 주어진 데이터셋으로부터 쉽게 얻을 수 있는 확률입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(y_i) = \frac{y_i에 해당하는 데이터 샘플}{전체 데이터 샘플}&lt;/script&gt;

&lt;p&gt;그리고 우리는 각각의 데이터는 서로 독립적으로 서로에게 영향을 미치지 않는다고 가정을 하여 다음과 같은 이득을 얻을 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;위의 계산식에서 분모에 해당하는 부분는 상수로 취급을 할 수 있게 되기 때문에 단순한 nomalization으로 생각하면 됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;conditional probability는 다음과 같이 계산이 가능하도록 됩니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P (y_i |x_1 , x_2 , · · · , x_n ) = P (x_1 , x_2 , · · · , x_n |y_i) × P (y_i)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P (y_i |x_1 , x_2 , · · · , x_n ) = P (x_1|y_i)\times P(x_2|y_i)\times ...\times P(x_n|y_i)\times P(y_i)&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_blobs&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_blobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'input 데이터: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'output 데이터: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'o'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;edgecolor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;k&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;$X_1$&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;$X_2$&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input 데이터:  (100, 2)
output 데이터:  (100,)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;output_37_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에서 알 수 있듯이, &lt;code class=&quot;highlighter-rouge&quot;&gt;make_blobs&lt;/code&gt;를 활용하여 2개의 &lt;code class=&quot;highlighter-rouge&quot;&gt;class&lt;/code&gt;로 구분이 가능한 데이터 셋을 생성하였습니다.&lt;/p&gt;

&lt;p&gt;이번에는 앞서 말했듯이, 주어진 데이터 셋에 대한 확률을 정의하는 모듈을 만들어 보도록 하겠습니다. 이 때, 우리는 데이터에 대한 확률을 정의하는 데에 있어서 정규분포를 이용할 것이며, 이는 python에서 Scipy에서 제공하는 정규분포 함수를 활용하여 다음과 같이 데이터의 평균과 분산을 통해서 구현이 가능합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;make_distribution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;distr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distr&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;이제는 각각의 &lt;code class=&quot;highlighter-rouge&quot;&gt;class&lt;/code&gt;에 대해 해당하는 conditional probabiliyt를 정의하도록 하겠습니다. 이를 위해서 앞서 만든 데이터셋을 각각의 &lt;code class=&quot;highlighter-rouge&quot;&gt;class&lt;/code&gt;로 구분해야합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'class 0에 대한 input: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'class 1에 대한 input: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class 0에 대한 input:  (50, 2)
class 1에 대한 input:  (50, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;그리고 prior probability는 다음과 같이 쉽게 구할 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;p_y_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_y_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'P(y_0) = {}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_y_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'P(y_1) = {}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_y_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;P(y_0) = 0.5
P(y_1) = 0.5
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;위에서 정의한 계산식에 맞추어 각각의 데이터를 분리하여 다음과 같이 분포로서 확률을 정의합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;dist_X_1_y_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_distribution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dist_X_2_y_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_distribution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dist_X_1_y_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_distribution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dist_X_2_y_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_distribution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-1.5632888906409914 0.787444265443213
4.426680361487157 0.958296071258367
-9.681177100524485 0.8943078901048118
-3.9713794295185845 0.9308177595208521
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;마지막으로, conditionaly probability를 계산하는 함수를 다음과 같이 만듭니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;conditional_prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prior&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prior&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;전체 데이터 중에서 10번째 데이터에 대해서는 다음과 같이 구할 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_y_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conditional_prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_y_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist_X_1_y_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist_X_2_y_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_y_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conditional_prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_y_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist_X_1_y_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist_X_2_y_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'P(y=0| {}) = {}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_y_0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'P(y=1| {}) = {}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_y_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Truth: y= '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;P(y=0| [-10.03640801  -5.5691209 ]) = 6.471562626475902e-98
P(y=1| [-10.03640801  -5.5691209 ]) = 0.0819979322731152
Truth: y=  1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;따라서 위의 결과로서 데이서 셋에서 10번쨰에 해당하는 $X$는 확률이 더 높은 &lt;code class=&quot;highlighter-rouge&quot;&gt;y=1&lt;/code&gt;인 경우에 대해서 더 높게 나왔기 때문에 &lt;code class=&quot;highlighter-rouge&quot;&gt;Truth&lt;/code&gt;값과 동일하게 &lt;code class=&quot;highlighter-rouge&quot;&gt;class 1&lt;/code&gt;에 속한다고 할 수 있습니다.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Bayes Theorem</title>
   <link href="http://wonchul-kim.github.io/machine%20learning/2020/12/04/bayes/"/>
   <updated>2020-12-04T00:00:00+09:00</updated>
   <id>http://wonchul-kim.github.io/machine%20learning/2020/12/04/bayes</id>
   <content type="html">&lt;h1 id=&quot;베이즈-이론bayes-theorem-120분&quot;&gt;베이즈 이론(Bayes Theorem) (120분)&lt;/h1&gt;

&lt;h2 id=&quot;1-정의&quot;&gt;1. 정의&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;베이즈 이론&lt;/strong&gt;은 조건부 확률을 계산하기 위한 원칙을 제공하는 방법이라고 할 수 있습니다. 앞서 chapter에서 배웠듯이 조건부 확률은 어떠한 사건이 일어날 확률을 다른 사건이 일어난 조건하에서 생각해야 하기 때문에 직관적으로는 이해할 수 있으나 수치로 나타내어 계산하는 것이 복잡할 수도 있습니다. 그리고 이러한 복잡한고 애매한 계산을 하나의 이론으로서 정립한 것이 베이즈 이론입니다.&lt;/p&gt;

&lt;p&gt;먼저, 이번 chapter에서는 조건부 확률을 표현하기 위해서 dependent random variable인 $X$와 $Y$로부터 일어나는 사건 $A$와 $B$에 대해서 어떤 사건이 일어난 조건하에 다른 사건이 일어날 확률로서 정의하여 다음과 같이 나타낼 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B) = \frac{P(A \cap B)}{P(B)}&lt;/script&gt;

&lt;p&gt;그리고 조건부 확률은 대칭성이 없습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B) \neq P(B|A)&lt;/script&gt;

&lt;p&gt;그렇기 때문에 위의 두 관계는 대칭성을 떠나 서로가 서로를 구하는 데에 계산에 활용됩니다. 그리고 이 정의를 &lt;strong&gt;베이즈 이론&lt;/strong&gt;이라고 합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}&lt;/script&gt;

&lt;p&gt;즉, 베이즈 이론은 조건부 확률을 계산하는 규칙과 joint probability를 사용하는 대안을 제공하는 이론이라고 할 수 있습니다. 이 이론은 특히 joint probability를 구하기 힘든 경우나 구하고자 하는 조건부 확률의 역을 구하기 힘든 경에 대해서 매우 유용하게 사용됩니다.&lt;/p&gt;

&lt;p&gt;예를 들어서, 베이즈 이론에서 분모인 $P(B)$를 구하기 힘든경에 대해서는 joint probability를 사용하지 않고 조건부 확률을 구할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(B) = P(B|A) \times P(A) + P(B|not A) \times P(not A)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B) = \frac{P(B|A) \times P(A)}{P(B|A) \times P(A) + P(B|not A) \times P(not A)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(not A) = 1 - P(A)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(B|not A) = 1 - P(not B|not A)&lt;/script&gt;

&lt;p&gt;이렇게 베이즈 이론은 조건부 확률을 계산하는 규칙을 제공함으로서 간단하지만 매우 중요한 역할을 하고 있습니다. 특히, 현실 세계에서 우리가 나타내고자 하는 확률들은 대부분 하나의 사건이 아닌 여러 사건이며, 서로 독립적이지 않기 때문에 조건부 확률로서 많이 나타납니다. 그리고 이러한 조건부 확률이 서로가 얽히고 얽혀있기 때문에 더욱더 베이즈 이론처럼 정립된 계산 방법이 더욱 유용하게 작용합니다. 그리고 현실에서는 확률로서 구하지 못하는 경우도 존재하기 때문에 서로를 대체하여 계산할 수 있는 계산 방법을 제공한다는 점에서 매우 중요합니다.&lt;/p&gt;

&lt;h2 id=&quot;2-용어&quot;&gt;2. 용어&lt;/h2&gt;

&lt;p&gt;앞서서는 베이즈 이론이 무엇인지 그리고 계산식을 배웠습니다. 이번에는 계산식에서 각각의 확률 $P$가 나타내는 것이 무엇인지를 알아보고 베이즈 이론이 조건부 확률을 계산하는 원칙을 제공하는 만큼 이에 대한 각각의 정식적인 명칭을 알아보도록 하겠습니다. (명칭에 대해서는 일반적으로 설명하도록 하겠습니다. 베이즈 이론이 매우 오랫동안 여러 분야에 사용되고 통상적으로 활용되는 만큼 그 명칭이 변하여 사용처/분야에 따라 바뀐 경우도 존재합니다.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;$P(A&lt;/td&gt;
          &lt;td&gt;B)$: Posterior probability&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$P(A)$: Prior probability&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;$P(B&lt;/td&gt;
          &lt;td&gt;A)$: Likelihood&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;$P(B)$: Evidence&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Posterior = \frac{Likelihood \times Prior}{Evidence}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Example 1)&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;이메일을 받았을 경우 스팸 감지기가 이 메일을 스팸함에 넣었을 때, 이 메일이 스팸일 확률을 구하는 문제입니다. 이에 앞서 주어진 조건들은 다음과 같습니다. 우리가 스팸을 받을 확률인 $P(spam)$는 5%이고, 스팸 감지기가 메일을 받았을 경우에 스팸을 감지할 확률인 $P(detected&lt;/td&gt;
      &lt;td&gt;spam)$는 95%라고 합니다. 그리고 받은 메일이 스팸이 아닌 경우에 대해서 스팸이라고 하는 확률인 $P(detected&lt;/td&gt;
      &lt;td&gt;not spam)$는 0.4%라고 합니다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;먼저, 위의 문제에 대해서 우리가 구하고자 하는 조건부 확률에 대한 베이즈 이론을 나타내면 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(spam|detected) = \frac{P(detected|spam) \times P(spam)}{P(detected)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \frac{0.95 \times 0.05}{P(detected)}&lt;/script&gt;

&lt;p&gt;하지만, 우리는 $P(detected)$를 알지 못합니다. 그렇기 때문에 앞서 배운 베이즈 이론이 중요하게 작용합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(detected) = P(detected|spam) \times P(spam) + P(detected|not spam) \times P(not spam)&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;우리는 $P(detected&lt;/td&gt;
      &lt;td&gt;not spam)$이 0.4%이고, $P(not spam) = 1 - P(spam)$인 것을 알기 때문에 다음과 같이 간단하게 계산할 수 있습니다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(not spam) = 1 - P(spam)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= 1 - 0.05 = 0.95&lt;/script&gt;

&lt;p&gt;따라서, 우리는 $P(detected)$를 다음과 같이 구할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(detected) = 0.95 \times 0.05 + 0.004 \times 0.95&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= 0.0475 + 0.0038 = 0.0513&lt;/script&gt;

&lt;p&gt;즉, 받는 메일이 스팸인지 아닌지를 떠나서 2%에 대해서 스팸으로서 감지가 된다는 것을 확인할 수 있습니다. 그리고 이를 이용하여 본래의 베이즈 이론에 적용하면 다음과 같이 문제를 해결할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(spam|detected) = \frac{0.95 \times 0.05}{0.0513}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \frac{0.0475}{0.0513} = 0.9259259259259259&lt;/script&gt;

&lt;p&gt;이로써 스팸함에 있는 메일이 있다면 약 92.5% 확률로 그 메일이 스팸이라는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번에는 위의 문제를 python 코드를 활용하여 풀어보도록 하겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bayes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_b_given_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_b_given_not_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'''
        - Args: P(A), P(B|A), P(B|not A)
        - return: P(A|B)
    '''&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# P(not A)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;not_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_a&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# P(B)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_b_given_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_b_given_not_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;not_a&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# P(A|B)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p_a_given_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_b_given_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_b&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_a_given_b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# P(A)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# P(B|A)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_b_given_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.95&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# P(B|not A)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p_b_given_not_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.004&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# P(A|B)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bayes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_b_given_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_b_given_not_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'P(A|B) = {:.2f}&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;따라서, python코드로도 위에서 정의한 개념을 이용하여 간단하게 변수를 지정하여 쉽게 구할 수 있다는 것을 배울 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;3-베이즈-이론과-머신러닝&quot;&gt;3. 베이즈 이론과 머신러닝&lt;/h2&gt;

&lt;p&gt;베이즈 이론은 관측한 데이터를 가장 잘 표현하는 모델을 선택하는 과정으로 머신러닝에서 적용된다고 할 수 있습니다. 다시 말해서, linear regression과 logistic regression과 같은 많은 머신러닝 모델에 기초가 되는 Maximum a Posterior나 MAP probabilistic framework에서 베이즈 이론이 초석으로서의 역할을 합니다. 이렇게 머신러닝 속 베이즈 이론은 Bayes optimcal classifier를 활용하여 다음의 새로운 값을 예측하는 데에 있어서 정확도를 높이는 최적화를 어떻게 할 것인가에 대한 개념도 제공합니다. 따라서, 이번에는 머신러닝에서의 modeling과 prediction 과정을 베이지안 관점에서 풀어보도록 하겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;3-1-모델링에서의-베이즈-이론&quot;&gt;3-1. 모델링에서의 베이즈 이론&lt;/h3&gt;

&lt;p&gt;베이즈 이론은 응용 머신러닝(applied machine learning)에서 관측한 데이터와 모델과의 관계를 규정할 수 있다는 점에서 굉장히 유용한 도구로서 활용됩니다. 이러한 이유는 머신러닝에서 modeling이 관측된 데이터를 틀이잡힌 관계로서 규정할 수 있기 때문에 중요하며, 이 역할을 베이즈 이론이 수행하기 때문입니다. 다시 말해서, 모델(model)은 데이터에 존재하는 관계에 대한 가정/추측(hypothesis)으로서 생각할 수 있습니다. 예를 들어, input($X$)와 output($y$)와의 관계를 들 수 있습니다. 따라서, 응용 머신러닝은 주어진 데이터에 대해서 여러 다른 가정 또는 모델을 분석하고 테스트 하는 것이 핵심이며, 결국에는 데이터의 관계를 가장 잘 표현할 수 있는 것을 찾고자 합니다. 그리고 Bayes theorem은 데이터($D$)와 가정/모델($h$)의 관계를 확률로서 제공하며, 다음과 같이 나타낼 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(h|D) = \frac{P(D|h) \times P(h)}{P(D)}&lt;/script&gt;

&lt;p&gt;위의 식을 풀어쓰면, 관측된 데이터가 주어질 때의 가정이 사실일 확률은 주어진 가정에 대해서 관측된 데이터가 표현될 확률과 데이터에 관계없이 가정이 사실일 확률을 곱하고 가정에 관계없이 관측된 데이터가 나타날 확률로 나눈 것으로 계산됩니다. 이는 다음의 문구에서 가져온 개념입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Bayes theorem provides a way to calculate the probability of a hypothesis based on
its prior probability, the probabilities of observing various data given the hypothesis,
and the observed data itself. &lt;br /&gt;
— Page 156, Machine Learning, 1997.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그리고 위의 식은 다음과 같이 쪼개어 명명됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;$P(h&lt;/td&gt;
          &lt;td&gt;D)$: posterior probability of the hypothesis&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;$P(h)$: Prior probability of the hypothesis&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이러한 계산 체계가 중요한 이유는 머신러닝에 관한 문제를 modeling하는 데에 있어서 단계적으로 쪼개어서 풀어볼 수 있기 때문입니다. 만약 가정에 관하여 prior domain knowledge가 잘 못 되어 있다면, 이는 prior probability에 문제가 있을 것입니다. 그렇지 않으면 우리는 모든 가정에 대해서 동일한 prior probability를 갖고 있다는 게 됩니다. 그리고 만약 관측된 데이터에 대한 확률이 커질수록 데이터가 주어졌을 때의 가정에 대한 확률은 작아집니다. 반대로 가정에 관한 확률과 가정이 주어졌을 때 관측되는 데이터의 확률이 커질수록 데이터가 주어졌을 때의 가정에 관한 확률이 커질 것입니다.&lt;/p&gt;

&lt;p&gt;그리고 앞서 말했듯이, 응용 머신러닝에서 주어진 데이터에 대해서 여러 모델들을 테스트한다는 것은 여러 가정들($h_1, h_2, …, \in H)$의 확률을 평가하는 것입니다. 따라서, 데이터의 확률은 각각의 가정에 대해서 이미 주어진 사실과 같으므로 상수가 되어 계산식에서 생략되어 좀더 간단한 식으로 표현될 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;max h \in H P(h|D) = P(D|h) \times P(h)&lt;/script&gt;

&lt;p&gt;만약 테스트해야할 가정에 대해서 어떤 prior 정보도 없다면, 우리는 uniform probability로서 prior probability를 대채하여 다음과 같이 계산할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;max h \in H P(h|D) = P(D|h)&lt;/script&gt;

&lt;p&gt;즉, 머신러닝의 여러 problem에서의 목적은 관측된 데이터를 가장 잘 표현할 수 있는 가정을 찾는 것이고, 베이즈 이론은 정규화된 계산식을 제공하므로서 해결책을 제공합니다. 그리고 이는 결국에는 모델과 데이터를 가장 잘 설명하는 파라미터들을 찾는 최적화 과정에서의 토대를 제공하며, 이를 &lt;em&gt;density estimation&lt;/em&gt;이라고 합니다.&lt;/p&gt;

&lt;h3 id=&quot;3-2-density-estimation&quot;&gt;3-2. Density Estimation&lt;/h3&gt;

&lt;p&gt;일반적인 modeling problem은 데이터에 대해서 joint probability distribution을 어떻게 예측하고 평가하는 지에 관환 것입니다. 그리고 density estimation은 관측된 데이터에 대한 joint probability distribution을 가장 잘 설명하는 여러 확률 분포들과 parameter들을 선택하는 과정으로서, 여러 가지 해결책이 있지만 다음과 같이 두 가지가 대표적입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Maximum a Posterior (MAP)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Maximum Likelihood Estimation (MLE)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;사실, 위의 두 가지의 방법은 모두 최적화를 하는 것으로서, 결국에는 여러 확률 분포와 parameter들을 탐색하여 주어진 데이터에 맞는 최적의 확률분포와 parameter를 선택하는 것입니다. 이 때, MLE의 목적은 likelihood function을 최대화하는 parameter($\theta$)를 찾는 것입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;max P(X;\theta)&lt;/script&gt;

&lt;p&gt;그리고 MAP는 이를 bayesian probability의 과점에서 최적화 과정으로 풀어나가는 것로서, likelihood function을 최대화하는 대신에 bayesian posterior probability를 최대화하는 것을 목적으로 합니다.&lt;/p&gt;

&lt;h3 id=&quot;3-3-maximum-a-posterior&quot;&gt;3-3. Maximum a Posterior&lt;/h3&gt;

&lt;p&gt;앞서서 우린느 베이즈 이론은 conditional probability를 계산하는 규칙을 제공하고 있다는 것을 알 수 있었고, 그러한 식에서 각각의 용어를 정리하였습니다. 그리고 이번에는 소제목에서도 알 수 있듯이 posterior probability를 최대화하는 방법을 배울 것입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;위의 베이즈 이론에서 $P(A&lt;/td&gt;
      &lt;td&gt;B)$가 바로 posterior probability입니다. 그리고 분모에 있는 $P(B)$는 어떻게 보면, 단순히 normalization을 하기 위한 변수라고 생각할 수 있기 때문에 생략이 가능하다고 볼 수 있습니다. 그렇게 되면 posterior probability는 다음과 같은 관계를 가지게 됩니다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B) \propto P(B|A) \times P(A)&lt;/script&gt;

&lt;p&gt;그리고 이를 단순화하면 다음으로까지 표현이 가능합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(A|B) = P(B|A) \times P(A)&lt;/script&gt;

&lt;p&gt;위의 식은 어떻게 보면, 마음대로 분모를 생략하고 계산하기 때문에 불합리하다고 볼 수 있습니다. 하지만, 베이즈 이론과 달리 &lt;strong&gt;Maximum a Posteriror&lt;/strong&gt;는 정확한 변수들의 수치를 구하고자 하는 관점이 아닌, 서로간의 비율을 통해서 posterior probability를 최대화하는 관점이기 때무에 위에서처럼 계산을 해도 무방하다고 할 수 있습니다. 따라서, 우리는 이 관계식을 활용하여 주어진 데이터 ($X$)를 가장 잘 설명할 수 있는 parameters ($\theta$)와 분산을 추정하는 것이 목적입니다. 이는 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta|X) = p(X|\theta) \times P(\theta)&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;그리고 이는 $ max h \in H P(h&lt;/td&gt;
      &lt;td&gt;D) = P(D&lt;/td&gt;
      &lt;td&gt;h)$ 와 동일하다고 볼 수 있습니다. (단순히 $\theta$와 $h$가 혼용되어 동일한 의미를 뜻합니다.) 이 때, posterior probability를 완벽하게 계산하는 것은 대부분의 상황에서 무리이기 때문에 posterior probability distribution을 전체적으로 모두 다 표현하는 것이 목적이 아닙니다. 대신에 분산을 표현하는 데에 있어서 매우 유용한 평균(mean)을 추정하는 것을 목표로 합니다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;이러한 MAP 이론은 앞서서 살펴혼 MLE와 매우 유사하다는 것을 알 수 있습니다. 사실상, 문제에서 데이터를 표현하기 위한 $\theta$의 모든 수치가 동일하다면, MLE와 MAP는 동일한 결과를 도출합니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이번에는 위에서 설명한 내용을 ‘동전 던지기’라는 예에 빗추어 설명해보겠습니다.일반적으로 동전을 던졌을 때, 질량의 분포가 균등하게 이루어져 있다면 앞면이 나올 확률은 0.5라고 알고 있습니다. 이 말이 검증되기 위해 동전을 100번 던졌을 때, 그 결과로 70번의 앞면이 나왔는데, 이 경우에 Maximum Likelihood Estimation에서는 앞면이 나올 확률 $P(T)$를 0.5 혹은 0.7과 같이 가정하여 그 가정에 대한 확률을 구하였습니다. 하지만 이렇게 추측하였을 때에는 이전에 우리가 알고 있던 사실인 ‘앞면이 나올 확률은 0.5 이다’가 전혀 반영되지 않고 순수하게 측정 된 결과로만 확률을 추측했다는 것을 알 수 있습니다. 반면에 Posteriori를 구할 때에는 bayes theorem을 이용하여 사전에 우리가 알고 있는 정보를 활용하여 다음과 같이 계산합니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(T|E) = \frac{P(E|T)P(T)}{P(E)}&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;알고자 하는 확률 ‘100번 동전을 던진 시행에서 70번의 앞면이 나왔을 때, 동전의 앞면이 나올 확률’ 은 $P(T&lt;/td&gt;
      &lt;td&gt;E)$로 표현이 됩니다. 이 확률은 Bayes Theorem을 통해서 우변과 같은 유도가 가능합니다. 이론 $T$에 특정 가정을 대입하고, 그에 대한 확률을 계산하는 방식입니다. 만약 $T = 0.5$라는 가정을 기준으로 계산하면, 다음과 같습니다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(T = 0.5|E = 0.7) = \frac{P(E = 0.7|T = 0.5) \times P(T = 0.5)}{P(E = 0.7)}&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;여기서 $P(E = 0.7&lt;/td&gt;
      &lt;td&gt;T = 0.5)$는 Likelihood Function에서 쉽게 구할 수 있고, $P(E = 0.7)$는 변수에 특정 값을 대입했을 때 나온 값이 아닌 상수이기 때문에, $P(T = 0.5)$는 우리가 이전에 알고 있던 ‘동전을 던졌을 때 앞면이 나올 확률은 0.5 이다’라는 명제에 대한 사전확률만 주어진다면 Posteriori를 계산 할 수 있습니다. 이제 가정을 변수 $x$라고 생각을 하면 다음과 같이 식이 바뀝니다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(T = x|E = 0.7) = \frac{P(E = 0.7|T = x) \times P(T = x)}{P(E = 0.7)}&lt;/script&gt;

&lt;p&gt;위와 같이 변수 $x$에 대해 식을 표현하면, $x$에 대한 함수가 되어, $x$의 변화에 따른 최대값을 구할 수 있습니다. 이때 최대값을 구하는 과정이 Maximum a Posteriori입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 2)&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 예제와 코드를 통해서 MAP를 통해서 주어진 문제를 어떻게 해결할 수 있는지에 대해서 알아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;우리는 동전을 던지는 행위를 할 것입니다. 하지만, 동전의 앞면과 뒷면이 나올 확률에 대해서는 알지 못합니다. 즉, 동전의 모양이 이상하여 앞면과 뒷면이 나올 확률이 우리가 알던 0.5가 아닐 수도 있다는 이야기입니다. 그렇다면, 우리는 어떻게 확률을 알 수 있을까요?&lt;/p&gt;

&lt;p&gt;가장 쉬운 방법은 동전을 여러 번 던져보고 이에 대해서 평균적으로 확률을 정의하는 것입니다. 이는 다음과 같습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 동전을 던지는 횟수&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 0.7의 확률이라고 임의로 정한 것&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_arr&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;위의 결과로 볼 때, 앞면이 2번 나오고, 뒷면이 4번 나왔으므로 결론적으로 우리가 추측하는 확률은 다음과 같습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta} = 0.4&lt;/script&gt;

&lt;p&gt;어떻게 보면, 매우 합리적인 결과라고 할 수도 있습니다. 왜냐하면, 우리가 기존에 기대한 또는 가정한 $\theta$의 값도 0.7이었기 때문입니다.&lt;/p&gt;

&lt;p&gt;이번에는 이에 대한 문제를 MAP를 활용하여 풀어보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;먼저, 동전 던지기에 대한 확률 분포를 다음과 같이 가정하겠습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|\theta) = \theta^x(1 - \theta)^{1 - x}&lt;/script&gt;

&lt;p&gt;사실 앞선 chapter에서 배운 MAP도 이 문제를 푸는 데에 있어서 매우 적합합니다. 하지만, 이는 사람이 추론하는 것과 같은 직관적인 방식은 아니라고 할 수 있습니다. 전형적으로 우리는 매번 믿음이 달라집니다. 특히, 우리가 더 많은 선지식(prior kowledge)을 가지고 있다면 어떠한 사건에 대해서 우리의 믿는 조건을 변경할 수도 있고, 더 나은 추측(posterior)이 가능해집니다. 그리고 이러한 것을 Bayesian statistics라고 합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;총 6번의 동전전기기를 시행하도록 하겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_arr&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([1., 1., 1., 1., 1., 1.])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;이러한 결과가 나올 경우, 우리는 항상 앞면만 나온다는 말도 안되는 상황이라고 말할 수 있습니다. 직관적으로 우리는 동전을 던질 때 앞과 뒤의 면이 나올 확률은 각각 0.5라는 것을 알 수 있습니다. 그러므로 우리는 우리가 알지 못하는 확률 또는 우리가 구하고자 하는 확률인 $p(\theta)$에 대한 prior 분산을 0.5라고 할 수 있는 것입니다. 이렇게 함으로서 우리는 이러한 prior belief를 기반으로 더 나은 추측을 할 수 있습니다. 하지만, 이 prior knowledge는 데이터의 양이 늘어갈수록 영향이 줄어듬을 알아야합니다. 이는 베이즈 이론에 의해서 다음과 같기 때문입니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\theta | x) = \frac{p(\theta, x)}{p(x)} = \frac{p(x | \theta)p(\theta)}{p(x)}&lt;/script&gt;

&lt;p&gt;그렇다면, MAP를 활용한 최적화의 식은 다음과 같이 나타납니다.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$$ \hat{\theta}&lt;em&gt;{MAP} = argmax&lt;/em&gt;{\theta} log p(\theta&lt;/td&gt;
      &lt;td&gt;x) $$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$$ argmax_{\theta} log p(x&lt;/td&gt;
      &lt;td&gt;\theta) + log p(\theta) $$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;MLE와 비교했을 때, MAP는 $p(\theta)$라는 하나의 변수(term)가 더 존재합니다. &lt;strong&gt;사실상, 주어진 문제에 대해서 uniform prior를 가정하였다면, MAP는 MLE와 동일해집니다.&lt;/strong&gt; 왜냐하면, 마지막의 $log p(\theta)$가 변수가 아닌 상수가 되기 때문입니다.&lt;/p&gt;

&lt;p&gt;앞서 설명하였듯이, 우리는 동전을 던졌을 때 나올 수 있는 결과에 대한 확률 분포를 다음과 같이 가정하였습니다. 이는 베르누이 분포라고 명명하며, 간단히 설명하자면 매 시행마다 오직 두가지의 가능한 결과만을 나올 때 사용하는 분포입니다. 자세한 사항은 &lt;a href=&quot;https://ko.wikipedia.org/wiki/%EB%B2%A0%EB%A5%B4%EB%88%84%EC%9D%B4_%EB%B6%84%ED%8F%AC&quot;&gt;위키_베르누이 분포&lt;/a&gt;에서 살펴볼 수 있습니다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|\theta) = \theta^x(1 - \theta)^{1 - x}&lt;/script&gt;

&lt;p&gt;그리고 베르누이 분포에서 $\theta: \theta$ ~ $Beta(\alpha, \beta)$를 따르도록 정의되어 있고, 이로 인해 나오는 값이 poterior probability입니다. 사실 $Beta$가 conjugacy에 의해서 어떻게 수식적으로 계산이 가능한지에 대해서 알면 베르누이 분포에 대해서 왜 $\theta$가 저렇게 되는지 알 수 있지만, MAP에 대한 설명에 대해서는 도움이 되지 않기 때문에 생략하도록 하겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0.75
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;먼저, 위에서처럼 $\theta = 0.7$로 함으로서 동전의 앞/뒤가 나올 확률이 서로 다르게 하여 데이터 ($X$)를 생성하였습니다. 이에 대한 확률은 위와 같습니다.&lt;/p&gt;

&lt;p&gt;다음으로는 여러 가지의 확률이 나올 수 있도록 $Beta(\alpha. \beta)$를 (2, 2)를 초기값으로 하여 바꾸어서 어떻게 MAP가 변화하는지에 대해서 살펴보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Beta_X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Beta_Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Beta_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Beta_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Beta_Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Beta_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Beta_Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0.494949494949495
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;output_24_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과에서 알 수 있드시, $\alpha = 2, \beta = 2$인 경우에 대한 MAP는 0.5입니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;즉, 동전의 앞면과 뒷면이 나올 확률이 동일한 경우에는 0.5가 MAP입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;22&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Beta_X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Beta_Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Beta_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Beta_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Beta_Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Beta_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Beta_Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0.42004200420042004
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;output_26_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과에서 알 수 있드시, $\alpha = 22, \beta = 30$인 경우에 대한 MAP는 0.42입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Beta_X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Beta_Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Beta_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Beta_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Beta_Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Beta_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Beta_Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0.5568556855685569
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;output_28_1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과에서 알 수 있드시, $\alpha = 50, \beta = 40$인 경우에 대한 MAP는 0.556입니다.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>블로그 시작</title>
   <link href="http://wonchul-kim.github.io/etc2020/12/03/start/"/>
   <updated>2020-12-03T00:00:00+09:00</updated>
   <id>http://wonchul-kim.github.io/etc2020/12/03/start</id>
   <content type="html">&lt;p&gt;Start!!!&lt;/p&gt;
</content>
 </entry>
 

</feed>
