---
layout: post
title: Denoising Diffusion Probabilistic Model (DDPM)
category: Computer Vision
tag: [diffusion model]
---

# [Denoising Diffusion Probabilistic Model](https://arxiv.org/pdf/2006.11239.pdf)

- **GANs**와 달리 `adversarial training` 없다.
- Normal distribution / Gaussian distribution으로 noise 생성

## Abstract
`
Our best results are obtained by training on a weighted variational
bound designed according to a novel connection between diffusion probabilistic
models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a
generalization of autoregressive decoding
`

#### `inception score`

생성된 이미지에 대한 다양성과 품질을 동시에 측정하는 지표이지만, 품질을 평가하는 것에는 제한이 존재하므로 다른 지표와 함께 사용하는 것이 효율적이다. 

#### `FID (Fréchet Inception Distance)`

생성된 이미지와 실제 이미지간의 차이를 측정하는 지표로, 두 이미지의 feature 분포간의 `Fréchet Distance`를 계산하여 두 분포간의 유사성을 나타낸다. 

#### `weighted variational bound`

Probabilistic Model의 학습에서 사용되는 loss function 중의 하나로, `variational bound`는 실제 확률분포와 근사화된 분포간의 차이를 의미한다. 이 때, `weighted`라는 변수를 사용하여 주어진 loss에 대해서 가중치를 적용하여 중요도에 대한 정보도 사용하겠다는 것이다. 

> `Variational Inference`: 확률 분포의 추정을 다루기 위한 통계적 추론 기법 중 하나에 속한다. 특히, 베이지안 통계에서 posterior distribution을 근사화하는 데 사용한다. 예를 들어, 주어진 데이터에 대해서 posterior distribution을 직접적으로 계산하는 것은 불가능하기 때문에 쉽게 계산 가능한 분포군 중에서 가까운 분포가 되도록 파라미터를 조정하고 찾는 과정에서 사용한다. 

#### `denosing score matching with Langevin dynamics`

- Langevin dynamics는 다음의 SDE(Stochastic Differential Equation, 확률미분방정식)을 의미

$$
d\mathbf{x} = d\mathbf{w} + \frac{1}{2}\nabla_xlogp(\mathbf{x})
$$

위 식에서 $dt = \epsilon$으로 하여 simulation을 하면 다음과 같이 정의할 수 있다. 
$$
x_i = x_{i-1} + \frac{\epsilon}{2}\nabla_xlogp(x) + \sqrt{\epsilon}\mathbf{z}_i, \\
for \ i = 1, 2, 3, ..., T, \\
\mathbf{z}_i \sim N(0, \mathbf{I}) 
$$

이 떄, $\mathbf{z}_i$는 가우시안 분포로부터 정의되고, $T$가 매우 크고 $\epsilon$이 매우 작으면 원하는 분포인 $p(x)$로부터 sampling하는 것과 동일하다는 것이 다음의 Theorem을 조건으로 증명되었다고 한다. 

1. $p(x)$는 stochastic process $x_t$의 stationary distribution이 된다. 
2. $p(x_t|x_0)$은 $t \to \infty$일 때, $p(x_t|x_0) \to p(x)$이다. 

> 정확히 이해하기 힘들지만, 원하는 분포로부터 sampling을 하기는 힘들기 때문에 근사할 수 있는 식으로부터 몇 가지의 조건을 갖추면 원하는 분포로부터 sampling한 것과 동일한 효과를 얻을 수 있다! 

* `score mathcing`이란 모델의 gradient와 데이터 분포의 gradient가 일치하도록 학습하는 과정을 의미

#### `progressive decompression scheme as a generalization of autoregressive decoding`

말은 어렵게 적혀 있지만, **DDPM**의 핵심인 latent variables이라는 완전한 noise 영역에서 주어진 시간 $T$만큼 점차적으로 noise를 줄여서 원하는 이미지를 생성/복원하는 decoding의 전체 과정을 의미한다. 

## 1. Introduction

#### `When the diffusion consists of small amounts of Gaussian noise, it is sufficient to set the sampling chain transitions to conditional Gaussians too, allowing for a particularly simple neural network parameterization.`

> 음,,, 단순히 $T$가 충분히 커야 sampling하는 근사 분포가 실제 분포에서 sampling하는 것과 동일해질 수 있다는 뜻이 아닐까?한다

`
Despite their sample quality, our models do not have competitive log likelihoods compared to other likelihood-based models (our models do, however, have log likelihoods better than the large estimates annealed importance sampling has been reported to produce for energy based models and score matching [11, 55]). We find that the majority of our models’ lossless codelengths are consumed to describe imperceptible image details (Section 4.3). We present a more refined analysis of this phenomenon in the language of lossy compression, and we show that the sampling procedure of diffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit ordering that vastly generalizes what is normally possible with autoregressive models.`

#### `log likelihoods better than ...`

`log likelihood`는 데이터가 근사한 분포에 의해서 (여기서는 학습한 모델에서 나오는 분포에 의해서) 얼마나 잘 표현되는지를 나타내는 지표이다. 따라서, 다른 log-likelihood를 기반으로 하는 모델에 비해서는 성능이 좋지 않지만, score matching과 energy-based 모델에 비해서는 성능이 좋다는 것을 의미한다. 

#### `lossless codelengths`

모델에서 비손실 압축을 위해 적용된 코드의 길이를 의미하는 듯 하다. 즉, 눈에 잘 띄지 않는 이미지의 세부정보까지 표현하기 위한 부분이 모델의 대부분을 차지한다는 것을 뜻한다.  

#### `lossy compression`

손실 압축을 의미하며, 복원할 때 손실이 있을 수 없는 단점이 존재한다. 


<img src='/assets/computer_vision/diffucion_model/forward.png'>

#### Forward Diffusion Process

$$
q(\mathbf{x_{1:T}}|\mathbf{x_0}) \coloneqq \prod_{t=1}^{T}{q(\mathbf{x_t}|\mathbf{x_{t-1}})} \\
q(\mathbf{x_{t}}|\mathbf{x_{t-1}}) \coloneqq N(\mathbf{x_t};\sqrt{1 - \beta_t}\mathbf{x_{t-1}}, \beta_t\mathbf{I})
$$

- 시간 $t$마다 `gaussian distribution`에서 reparameterized로 sampling하여 noise를 추가한다. 
- $\sqrt{1 - \beta_t}$: scaling하여 variance가 발산하는 것을 방지

#### Reverse Diffusion Process 


### References:
- [Denoising Diffusion Probabilistic Model](https://arxiv.org/pdf/2006.11239.pdf)
- https://happy-jihye.github.io/diffusion/diffusion-1/
- https://lilianweng.github.io/posts/2021-07-11-diffusion-models/