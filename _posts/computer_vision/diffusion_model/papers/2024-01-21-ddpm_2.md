---
layout: post
title: Denoising Diffusion Probabilistic Model (DDPM)
category: Computer Vision
tag: [diffusion model, ddpm]
---

# [Denoising Diffusion Probabilistic Model](https://arxiv.org/pdf/2006.11239.pdf)

### Forward Process

- Markoov Process & Gaussian Distribution을 따른다.
$$
q(\mathbf{x_t} | \mathbf{x_{t-1}}) = \mathcal{N}(\mathbf{x_t}; \sqrt{- \beta_t}\mathbf{x_{t-1}}, \beta_t\mathbf{I})
$$

- **VAE**에서 제안한 **Reparameterization Trick**으로 구현
$$
\mathbf{x_t} = \sqrt{- \beta_t}\mathbf{x_{t-1}} + \beta_t\epsilon, \\ \epsilon \sim \mathcal{N}(0, \mathit{I})
$$

<img src='/assets/computer_vision/diffucion_model/papers/ddpm/reparameterization.png'>

- $\beta_t$에 의해서 $t-1$ 시점의 이미지에서 $t$ 시점의 이미지가 결정되는 것이고, **DDPM** 논문에서는 이를 `Noise Schedule`이라고 함
    - $T = 100, \beta_0 = 0.0001, \beta_T = 0.02$
    - $\beta_t$는 시간에 linear하게 증가

    ```python
    T = 1000
    betas = torch.linspace(0.0001, 0.02, T)
    ```

- 전체 $T$까지 순차적으로 실행하는 것이 아니라, 다음의 수식을 이용해서 한번에 수행
$$
q(\mathbf{x_t} | \mathbf{x_0}) = \mathcal{N}(\mathbf{x_t}; \bar{\alpha_t}\mathbf{x_0}, (1 - \bar{\alpha_t})\mathbf{I}) \\
\mathbf{x_t} = \sqrt{\bar{\alpha_t}}\mathbf{x_0} + \sqrt{1 - \bar{\alpha_t}}\epsilon, \\ \epsilon \sim \mathcal{N}(0, \mathit{I}) \\
\alpha_t = 1 - \beta_t \\
\bar{\alpha_t} = \prod^{t}_{i=1}\alpha_t
$$

<img src='/assets/computer_vision/diffucion_model/papers/ddpm/noise_schedule.png'>

- 결국 `Forward Process`를 통해서 마지막으로 얻는 noise image는 완전한 normal distribution 또는 gaussian distribution을 따른다. 즉, $\mathcal{N}(0, \mathit{I})$으로 평균은 1에 가까워지고, 분산은 0에 가까워진다.

    ```python
    T = 1000
    betas = torch.linsapce(0.0001, 0.02, T)
    alphas = 1. - betas 
    alphas_bar = torch.comprod(alphas, axis=0)
    sqrt_alphas_bar = torch.sqrt(alphas_bar)
    sqrt_one_minus_alphas_bar = torch.sqrt(1 - alphas_bar)
    ```

### Reverse Process

$$
p_\theta(\mathbf{x_{t-1}}|\mathbf{x_{t}}) = \mathcal{N}(\mathbf{x_{t-1}}; \mathbf{\mu_\theta(\mathbf{x_{t}}, t), \sum_\theta(\mathbf{x_{t}}, t)})
$$

- 이미지를 복원하는 Markov Process

- **DDPM**에서는 분산은 고정하고, 평균을 학습

- **Generative Models**는 데이터의 분산을 학습하는 것이기 떄문에 `log-likelihood`를 `Loss function`(e.g. $max.log(p_\theta(\mathbf{x_0}))$) 으로 사용
    - 이 때, ***untractable**하기 떄문에 `Prox Loss Variational Lower Bound`를 사용
    - `VLB`는 모든 구간에서 실제 `likelihood`보다 작도록 설정

        <img src='/assets/computer_vision/diffucion_model/papers/ddpm/vlb.png'>

        `likelihood`를 $f(x)$라고 할 때, 계산이 가능한(***tractable***) $g(x)$를 최대화하면 $f(x)$와 매우 유사한 함수를 얻을 수 있다. 

    <img src='/assets/computer_vision/diffucion_model/papers/ddpm/loss.png'>

    - `Prior Matching Term`
        - $T$까지 `Forward Process`를 수행할 때의 최종 분포가 `Gaussian Distribution`과 동일하도록 한다. 
        - 이미 `Gaussian Distribution`으로 타겟을 정했기 때문에 <U>상수로 취급 가능</U>

    - `Reconstruction Term`
        - $\mathbf{x_1}$의 이미지에서 $\mathbf{x_0}$으로 복원할 때, `likelihood`를 최대화하도록 한다.
        - 전체 $T$에 비해 매우 작기 때문에 <U>상수로 취급 가능</U>

    - `Denoising Term`
        - 실제 `Reverse Process`인 $q(\mathbf{x_{t-1}}|\mathbf{x_{t}})$와 모델이 학습하는 분포인 $p_\theta(\mathbf{x_{t-1}}|\mathbf{x_{t}})$가 동일해지도록 한다.
        - 하지만, 실제 `Reverse Process`인 $q(\mathbf{x_{t-1}}|\mathbf{x_{t}})$을 알지 못한다. 
        - 이 때, $mathbf{x_0}$을 실제 `Reverse Process`의 수식에 컨디셔닝을 해도 `Reverse Process`의 `Markov`정의는 깨지지 않기 때문에 $q(\mathbf{x_{t-1}}|\mathbf{x_{t}}, \mathbf{x_0})$을 정의하고, 이는 `Bayes Rule`로 계산이 가능


        <img src='/assets/computer_vision/diffucion_model/papers/ddpm/denoising_term.png'>


        $$
        q(\mathbf{x_{t-1}}|\mathbf{x_{t}, \mathbf{x_0}}) = \mathcal{N}(\mathbf{x_{t-1}}; \mathbf{\tilde{\mu}}(\mathbf{x_{t}}, \mathbf{x_0}), \mathbf{\tilde{\beta_t}}\mathbf{I}) \\

        \tilde{\mu_t} = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x_t} - \frac{1 - \alpha_t}{\sqrt{1 - \alpha_t}}\mathbf{\epsilon_t}) \\
        \tilde{\beta_t} = \frac{1 - \tilde{\alpha_{t-1}}}{1 - \tilde{\alpha_t}}\beta_t
        $$


        - 원본 이미지인 $\mathbf{x_0}$와 $t$시점의 $\mathbf{x+t}$가 있으면, $\mathbf{x_{t-1}}$은 평균이 $\tilde{\mu_t} = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x_t} - \frac{1 - \alpha_t}{\sqrt{1 - \alpha_t}}\mathbf{\epsilon_t})$이고 분산이 $\tilde{\beta_t} = \frac{1 - \tilde{\alpha_{t-1}}}{1 - \tilde{\alpha_t}}\beta_t$인 `Gaussian`을 따른다. 

        - 이 때, 미지수는 $\mathbf{\epsilon_t}$이므로, 모델은 이를 예측한다. 


        <img src='/assets/computer_vision/diffucion_model/papers/ddpm/denoising_term_epsilon.png'>

### Architecture
<img src='/assets/computer_vision/diffucion_model/papers/ddpm/arch.png'>


## References:
- https://www.youtube.com/watch?v=Q_o0SpXv9kU&t=757s&ab_channel=%E2%80%8D%EA%B9%80%EC%84%B1%EB%B2%94%5B%EA%B5%90%EC%88%98%2F%EC%82%B0%EC%97%85%EA%B2%BD%EC%98%81%EA%B3%B5%ED%95%99%EB%B6%80%5D