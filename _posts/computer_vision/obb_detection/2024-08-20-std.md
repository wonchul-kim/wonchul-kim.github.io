---
layout: post
title: Spatial Transformer Decoupling for Oriented Object Detection
category: Computer Vision
tag: [obb detection, rotated, oriented]
---

# [Spatial Transformer Decoupling for Oriented Object Detection](https://arxiv.org/abs/2308.10561)[code](https://github.com/yuhongtian17/Spatial-Transform-Decoupling)

## Abstract

- **Vision Transformers** does not fully support rotation-sensitive prediction due to the lack of spatial invariances.
- proposes **Spatial Transform Decoupling** built upon stacked ViTs to predict position, size, and angle of bouding boxes. 
- By aggregating cascaded activiation masks (CAMs), STD gradually enhances features within RoIs.


## Introduction

- A straightforward regression and suboptimal loss functions for a spatial transform parameters ($x$, $y$ coordinates, $w$, $h$ of object size, and $\sigma$ of angle) result in discontinuous boundaries due to the inconsistency in angle represntation and periodicity. 

- This papers suggests to extract rotation-related features rather than developing on angle representations or training objectives. 

    - By allocating distinct feature maps to bounding box prediction head to predict parameters associated with diverse semantic interpretations (location, shape, and orientation)
    - Step-wise strategy that estimates the parameters associtated with particular spatial transfom at each stage improves the overall accuracy of the model.
    - Each STD uses self-attention modules through separate network branches.
    - Integrates CAMs to effectively suppress background information while hightlighting foreground objects. 

        > By refining features within regions of interest using CAMs, the feature representation for oriented objects is both decoupled and progressively enhanced. ??

        > CAMs offer spatially dense guidances, directing the attention maps to focus more on foreground objects rather than the background. ??


## The Proposed Method

<img src='/assets/computer_vision/obb_detection/std/std-tbam.png'>

- Primary innovation of STD resides within the detection head module

- Adopt **imTed** detector and substitue the backbone and head modules of the two-stage detection with Vision Transformer blocks pre-trained using the **MAE** method

    - backbone: ***ResNet-50*** -> ***ViT-small*** from **MAE** pre-trained encoder
    - detection head: ***FC layers*** -> ***4-layer Transformer blocks*** from the pre-trained decoder and it forms **MAEBBoxHead** module

    > imTEAD ??

    > MAE ??


### Decoupled Parameter Prediction

- Should use different feature maps to predict spatial transform paramters by `multi-branch networks`.

- Each layer of 4-layer Transformer blocks provide ($x$, $y$), $\sigma$, ($w$, $h$), and class score.

### Acitivation Masks

- To further regulate the decoupling process and imporve the accuracy 
- Provide desnce guidance for bounding box prediction by integrating `acitivation masks` at each stage. 
- `CAMs` are composed of binary values by only telling the background(0) and foreground(1).
    > Must be binary ??

    > it is generated by incorporating information from both the proposal and the predicted bounding box ?? proposal???


## Experiment

- `AdamW` optimizer 
    - learning rate: $1e^{-4}$/$2.5e^{-4}$ for **DOTA-v1.0**/**HRSC2016**
    - weight decay: 0.05
    - layer decay: 0.75/0.90 for **ViT**/**HiViT**



### [Prepare DOTA Dataset for `mmrotate`](https://github.com/open-mmlab/mmrotate/blob/main/tools/data/dota/README.md)




## References
- [Spatial Transformer Decoupling for Oriented Object Detection](https://arxiv.org/abs/2308.10561)
- [code](https://github.com/yuhongtian17/Spatial-Transform-Decoupling)
- [mmrotate](https://github.com/open-mmlab/mmrotate/tree/main)