---
layout: post
title: LOST
category: Computer Vision
tag: [register]
---

# [Localizing Objects with Self-Supervised Transformers and no Labels](https://arxiv.org/pdf/2109.14279)

* Unsupervised object localization 

* Transformers trained with self-supervised learning has obect localization properties without being trained.

* Analyze the similarity maps of the features using box counting;
    * a fractal analysis tool to identify tokens lying on foreground patches 
    * identifed tokens are clustered
    * tokens of each cluster are used to generate bounding boxes on foreground regions

* can be used 
    * for self-supervised learning for object detection
    * to yield consistent improvements on fully, semi-supervied object detection and unsupervised region proposal generation.


## Introduction

* **Object discovery is the problem of identifying and grouping objects/parts in a large collection of
images without human intervention.**

    * The first step in object discovery is to obtain region proposals and subsequently group them semantically.

* **LOST** and **TokenCut** leveraged the object segmentation properties of transformers trained using self-supervised learning (**DINO**) to obtain high quality object proposals.

    * However, both **LOST** and **TokenCut** assume the presence of a single salient object per image.


* Empirical observations from DINO:
    1) Patches within foreground objects have higher correlation with each other than the ones on the background
    2) The similarity map computed using the features of a foreground object with all the features in the image is usually more localized and less noisier than the one computed using the feature of a background. 

* Our algorithm analyzes the similarities between patches exhaustively using a fractal analysis tool called box counting. 
    * This analysis picks a set of patches that most likely lie on foreground objects. 
    * Next, we perform clustering on the patch locations to group patches belonging to a foreground object together. Each of these clusters is called pools. 
    * A binary mask is then computed for each pool and a bounding box is extracted. 

    > fractal analysis tool ???????????
    
    To prove the effectiveness of MOST, we demonstrate results on several object localization and discovery
benchmarks. On self-supervised pre-training for object detectors, using MOST yields consistent improvement across
multiple downstream tasks using 6× fewer boxes. When
compared against other self-supervised transformer-based
localization methods, MOST achieves higher recall with
and without additional training. We summarize the contributions of our work below.


## References

- [Localizing Objects with Self-Supervised Transformers and no Labels](https://arxiv.org/pdf/2109.14279) [code](https://github.com/rssaketh/MOST/)


## Code

#### `most`에 사용하기 위한 feature 전처리

```python
feat_out = {}
def hook_fn_forward_qkv(module, input, output):
    feat_out["qkv"] = output
model._modules["blocks"][-1]._modules["attn"]._modules["qkv"].register_forward_hook(hook_fn_forward_qkv)

attentions = model.get_last_selfattention(img[None, :, :, :])

scales = [args.patch_size, args.patch_size]

nb_im = attentions.shape[0]  
nh = attentions.shape[1]  # Number of heads
nb_tokens = attentions.shape[2]  # Number of tokens
qkv = (
    feat_out["qkv"]
    .reshape(nb_im, nb_tokens, 3, nh, -1 // nh) # (bs, nb_tokens, qkv, nh, head_dim)
    .permute(2, 0, 3, 1, 4) # (qkv, bs, nh, nb_tokens, head_dim)
)
q, k, v = qkv[0], qkv[1], qkv[2]
k = k.transpose(1, 2).reshape(nb_im, nb_tokens, -1) # (bs, nh, nb_tokens, head_dim) -> (bs, nb_tokens, nh, head_dim) -> (bs, nb_tokens, all-head_dim)
q = q.transpose(1, 2).reshape(nb_im, nb_tokens, -1) # (bs, nh, nb_tokens, head_dim) -> (bs, nb_tokens, nh, head_dim) -> (bs, nb_tokens, all-head_dim)
v = v.transpose(1, 2).reshape(nb_im, nb_tokens, -1) # (bs, nh, nb_tokens, head_dim) -> (bs, nb_tokens, nh, head_dim) -> (bs, nb_tokens, all-head_dim)

if args.which_features == "k":
    feats = k[:, 1:, :]
elif args.which_features == "q":
    feats = q[:, 1:, :]
elif args.which_features == "v":
    feats = v[:, 1:, :]
```

* `.reshape(nb_im, nb_tokens, 3, nh, -1 // nh)`으로 할 수 있는 이유는 `feat_out['qkv]`가 `(bs, nb_tokens, nh, head_dim)`으로 `qkv`가 연속되어서 구성되어 있기 때문에 `.reshape`으로 분리가 가능함.

* 이후에 `.permute`를 하는 이유도 다음의 작업에서 `reshape`을 적용할 때, 연속적인 부분으로 만들기 위함.

* `feats`를 만들때, `cls token`은 제외하기 위해서 `1:` 적용.

    * `cls token`은 해당 이미지에 대한 전체적인 정리이기 때문에 본 논문에서 원하는 패치마다의 특성이 아니므로 제외함


#### most - semantic affinity map

```python
A = (feats @ feats.transpose(1, 2)).squeeze()
```

* 위의 수식은 **cosine similarity**로 봐도 무방

    * 정확히는 normalized value를 위와 같이 했을 경우, **cosine similarity**

* `feats`는 `(bs, nb_tokens (excluding cls-token), head_dim)`

* `A`는 `(bs, nb_tokens, nb_tokens)`으로 patch간의 similarity를 의미함

    * 즉, `A[i][j]`는 i번째 patch와 j번째 patch에 대한 similarity를 의미함

* **attention**과 비슷하지만, **attention map**은 아님

    | 항목         | Attention Map                 | Semantic Affinity Map                     |
    | ---------- | ----------------------------- | ----------------------------------------- |
    | 무엇을 나타내는가? | 정보를 어디에 집중해서 받아들일지            | 얼마나 비슷한 의미를 갖는지                           |
    | 계산 시점      | Transformer 내부                | Transformer 외부, 최종 feature로               |
    | 대칭성        | 비대칭 (attn\[i,j] ≠ attn\[j,i]) | 대칭 (sim\[i,j] = sim\[j,i])                |
    | 해석         | 정보 흐름                         | semantic grouping                         |
    | 대표 사용      | 시각화, 이해, 해석                   | object discovery, clustering, retrieval 등 |


> `A[i]`를 `reshape(a, a)`하고 visualization하면, 입력 이미지에서의 객체처럼 보이는 이유는?
> * 대부분의 **ViT**는 object-level semantic을 attention에 encode하기 때문임
> * i번째 patch가 객체의 한 조각이라면, 그 patch는 객체의 다른 부분과 높은 similarity를 가짐 (하나의 객체이고, 각각 부분을 의미하므로)
>   * 만약 i번째 patch가 개의 귀라면, 개의 몸통, 얼굴과도 당연히 높은 similarity를 가짐 -> 객체 전체가 보이는 heatmap
>   * 만약 배경이라면, 주변 배경에만 similarity를 가짐 -> 전체적으로 어두운 map

> 하지만, 왜 전체 객체가 보이는지는 직관적으로 이해가 가지 않음. 각 패치가 갖는 정보(객체의 부분?)만 보여야하지 않나?
>   * 각각의 patch는 local 정보만 담고 있지 않으며, **DINO, iBOT, MAE** 등의 self-supervised ViT는 global context를 통합하는 방식으로 학습함
>   * 그렇기 때문에 전체 객체에 대한 정보 + 각 patch가 담당한 객체의 부분?에 대한 정보도 담고 있고, 이로 인해 similarity가 높게 나타나는 것
>       * `patch_i`가 object 내부면 → 전체 object가 밝게 나타고, `patch_i`가 background면 → 배경 주변만 밝게 나타남



```python
    A_clone = A.clone()
    A_clone.fill_diagonal_(0)
    A_clone[A_clone < 0] = 0

    cent = -torch.sum(A_clone > 0, dim=1).type(torch.float32)
    ...
    ...
    seed_value = [cent[l] for l in similars]
    pot_seed = max(seed_value)
```

> similarity가 가장 적은 seed로 확장을 시작하는 이유는?
> * 많은 경우 object 중심 seed는 attention이 퍼져 있고, 주변에 너무 많은 patch와 연결됨
> * 반면, object 내부에서 뾰족한 의미를 가진 seed는 연결이 좁고 강함
> * 이런 seed는 확장 시 더 명확한 마스크를 생성하게 됩니다.
> * 즉, 너무 많은 연결이 있는 seed보다, 밀도 높은 지역에서 시작한 seed가 더 좋은 box/mask를 유도하기 때문


```python
for ii, cl in enumerate(np.unique(seed_labels)):
    # First collect all seeds that belong to this cluster
    similars = [seeds[i] for i in np.where(seed_labels == cl)[0]]
    # Find the seed with the maximum outgoing degree
    seed_value = [cent[l] for l in similars]
    pot_seed = max(seed_value)
    # Find all pixels that have similarity with the seed with highest
    # degree
    seed = torch.tensor(similars[seed_value.index(pot_seed)])
    similars = [l for l in similars if A[seed, torch.tensor([l])] > 0]
    # Find the mask 
    M = torch.sum(A[similars, :], dim=0)
    # Detect the box using the mask
    pred, small_pred = detect_box(
        M, seed, dims, scales=scales, initial_im_size=init_image_size[1:])
```

* 각 seed가 유사하다고 판단한 patch들의 semantic affinity vector를 모두 더함
* 결국 object 영역 전체에 대해 high response를 갖는 `mask M` 생성
    * `M[j]`는 선택된 seed들과 j patch 간의 누적 similarity
    * j patch가 object 전체에 포함될 확률처럼 해석 가능



```python
def detect_box(A, seed, dims, initial_im_size=None, scales=None):
    ...
    ...
    
    # Compute connected components
    labeled_array, num_features = scipy.ndimage.label(correl.cpu().numpy() > 0.0)

    # Find connected component corresponding to the initial seed
    cc = labeled_array[np.unravel_index(seed.cpu().numpy(), (w_featmap, h_featmap))]

    ...
    ...
```

* `scipy.ndimage.label(input, structure=None)`

    * `input`은 binary mask

    * 이미지 내에서 connected components (서로 연결된 1들)을 하나의 덩어리(컴포넌트)로 보고 각각에 `label`을 붙여줌 

    * `label`은 순차적으로 각 덩어리마다 1, 2, 3, ...

    * 예)
        ```python
            import numpy as np
            from scipy.ndimage import label

            binary_img = np.array([
                [0, 1, 1, 0],
                [0, 1, 1, 0],
                [1, 0, 0, 1],
                [1, 0, 0, 1]
            ])

            labeled_array, num_features = label(binary_img)

            print(labeled_array)
            print("Number of components:", num_features)
        ```

        ```cmd
            [[0 1 1 0]
            [0 1 1 0]
            [2 0 0 3]
            [2 0 0 3]]

            Number of components: 3
        ```


