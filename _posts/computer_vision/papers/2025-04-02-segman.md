---
layout: post
title: SegMAN
category: Computer Vision
tag: [rein]
---


# [SegMAN: Omni-scale Context Modeling with State Space Models and Local Attention for Semantic Segmentation](https://arxiv.org/pdf/2412.11890)

- For high-quality semantic segmentation:
    - global context modeling
    - local detail encoding
    - multi-scale feature extraction 


- a novel linear-time model comprising
    - a hybrid feature encoder
    - a decoder based on state space models
        - integrates sliding local attention with dynamic state space models
        - enabling highly efficient global context modeling while preserving fine-grained local details
        - MMSCopE module in decoder enhances multi-scale context feature extraction and adaptively scales with the input resolution



- `Global context modeling` establishes rich contextual dependencies regardless of spatial distance, enabling overall scene understanding [13].

- `Local detail encoding` endows fine-grained feature and boundary represenatations, crucial for semantic meanings [59].

- `Context modeling based on multiple intermediate scales` 


> Recent methods of semantic segmentation have been unable to simultaneously encapsulate all three of these capabilities effectively. For instance, VWFormer [57] introduces a varying window attention (VWA) mechanism that effectively captures multi-scale information through cross-attention between local windows and multiple windows with predefined scales. However, at higher input resolutions, the global context modeling capability of VWA diminishes because the predefined window sizes fail to maintain full feature map coverage. In addition, larger windows significantly increase the computational cost as self-attention exhibits quadratic complexity. Likewise, EDAFormer [60] proposes an embedding-free spatial reduction attention (SRA) mechanism to implement global attention efficiently and an all-attention decoder for global and multi-scale context modeling. However, fine-grained details are lost due to its reliance on downsampled features for token-to-region attention. Moreover, the absence of dedicated local operators in the feature learning

- aim to encapsulate omni-scale context modeling for a varying input resolution within a semantic segmentation network. 

    1. a hybrid feature encoder that integrates Local Attention and State Space models (LASS) in the token mixer
    2. a decoder with a Mamba-based Multi-Scale Context Extraction (MMSCopE) module

- In the SegMAN Encoder, LASS leverages a two-dimensional state space model (SS2D) [28] for global context modeling and neighborhood attention [19] (Natten) for local detail encoding, both functioning in linear time. The dynamic global scanning in our method always covers the full feature map, enabling robust global context modeling across varying input resolutions and all encoder layers. Meanwhile, by performing local attention within a sliding window, Natten exhibits an outstanding capability in fine-grained local detail encoding. 

- Our decoder complements the encoder with robust feature learning across multiple scales. The core MMSCopE module first aggregates semantic context from multiple regions of the encoder feature map. Then it uses SS2D to scan each regionally aggregated feature map to learn multiscale contexts at linear complexity. For higher efficiency, the feature maps at different scales are reshaped to the same resolution and concatenated so that scanning only needs to
be performed on a single feature map. Note that our method differs from existing methods in two aspects. First, unlike
multi-kernel convolution [2] and multi-scale window attention [57], our design adaptively scales with the input resolution. Second, by performing SS2D-based scanning over entire multi-scale feature maps, our method better preserves fine-grained details that are typically compromised by pooling operations [36, 63] or spatial reduction attention [60] in existing methods.









## Rereferences

- [SegMAN: Omni-scale Context Modeling with State Space Models and Local Attention for Semantic Segmentation](https://arxiv.org/pdf/2412.11890) [[code](https://github.com/yunxiangfu2001/SegMAN)]
