---
layout: post
title: Rein
category: Computer Vision
tag: [rein]
---


# [Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation](https://arxiv.org/pdf/2312.04265)


* Stronger: 이전의 DGSS 기법을 사용한 경우보다, 단순히 frozen backbone of VFMs를 사용한 경우가 성능이 더 좋음

> "while previous DGSS methods have showcased commendable results, they perform less effectively compared to frozen VFMs." 

* Fewer: 기존의 PEFT 기법들은 LLM이나 classification에 초점이 맞추어져 있기 때문에 객체탐지(객체별로 분류)를 위한 tuning에는 적합하지 않았음.

> "However, most existing parameter-efficient fine-tuning strategies, which fine-tune a large-scale model with fewer trainable parameters, are primarily designed for adapting large language models [28, 29, 46, 49, 51, 80, 85] or classification networks [7, 31]. These methods are not developed for refining features for distinct instances within a single image, thereby limiting their effectiveness in DGSS contexts."

* Superior: Robust and efficient fine-tuning approach, **Rein**
    - a set of randomly initialized tokens, each directly linked to instances, such as attention-like similarity map
    - shared weights across MLPs in different layers and learnable tokens my multiplying two low-rank metrices


#### 3.2 Core of Rein

- In the context of DGSS, an ideal $\Delta f_i$ should assist VFMs to bridge two types of gaps.

    - The first is gap in scene between pre-training dataset and target scene, exemplified by the contrast between ImageNet [13] and urban-scene images [12, 65]. 

    - The second is task divergence between pretraining and fine-tuning, such as the differences between masked image modeling and semantic segmentation tasks

        > 어떻게 보면, backbone과 head라는 서로 다른 역할을 수행하는 모듈인데, 그 gap을 줄인다는게 이해가 가지 않음.
        > 오히려, neck과 같은 모듈을 통해서 서로 잘 연결이 되도록 하거나 하나의 모듈로서 사용될 때 통합이 잘 되도록 하는 거에 신경써야하지 않나???????????????????????????????
        > 아니면, 말 그대로 backbone과 head가 서로 독립적으로 pre-trained되었으므로 차이가 있으니 이걸 줄이자???



