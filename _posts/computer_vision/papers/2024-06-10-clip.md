---
layout: post
title: Learning Transferable Visual Models From Natural Language Supervision
category: Computer Vision
tag: [clip]
---

- Contrastive learning
    - learn relation b/t texts and images
    - in the same domain
    - 매칭되는 feature끼리는 가까워지고, 나머지 feature에 대해서는 멀어지도록 학습

- The model is trained on full sentences instead of single word of class such as car, dog, etc. The intuition is that when trained on whole sentences, the model is able to learn a lot more things and find some pattern b/t images and texts.

- 기존의 이미지만으로 학습하는 모데로가 비교하여, 언어 모델과의 결합을 통해서 **general and robust** 모델을 만들고자 함.

- LLM이 성공한 이유와 견주어 모든 딥러닝 모델에서 이슈가 되는 점은 **큰 모델**과 **많은 데이터셋**의 사용.
    - 대용량의 이미지 데이터셋을 사람이 일일이 레이블링하며 구축하기에는 한계가 존재: 그렇기 때문에 image-text를 한쌍으로 인터넷으로부터 구축함으로서 대용량으로 구축 가능
        - 이 때, text는 label보다는 image에 대한 설명/정보

-  

- Loss function
    ```python
        def forward(self, batch):
            # Getting Image and Text Features
            image_features = self.image_encoder(batch["image"])
            text_features = self.text_encoder(
                input_ids=batch["input_ids"], attention_mask=batch["attention_mask"]
            )
            # Getting Image and Text Embeddings (with same dimension)
            image_embeddings = self.image_projection(image_features)
            text_embeddings = self.text_projection(text_features)

            # Calculating the Loss
            logits = (text_embeddings @ image_embeddings.T) / self.temperature
            images_similarity = image_embeddings @ image_embeddings.T
            texts_similarity = text_embeddings @ text_embeddings.T
            targets = F.softmax(
                (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1
            )
            texts_loss = cross_entropy(logits, targets, reduction='none')
            images_loss = cross_entropy(logits.T, targets.T, reduction='none')
            loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)
            return loss.mean()

    def cross_entropy(preds, targets, reduction='none'):
        log_softmax = nn.LogSoftmax(dim=-1)
        loss = (-targets * log_softmax(preds)).sum(1)
        if reduction == "none":
            return loss
        elif reduction == "mean":
            return loss.mean()
    ```
    > WHY self-similarity is used for logits?

    > WHY target is mean of texts and images' self-similarity?

    - in the best case scenario, text_embeddings and image_embedding matricies should be the same because they are describing similar things. Let’s think now: if this happens, what would the logits matrix be like? Let’s see with a simple example! 

        ```pytyhon
            import torch
            from torch import nn
            import torch.nn.functional as F
            import matplotlib.pyplot as plt

            batch_size = 4
            dim = 256
            embeddings = torch.randn(batch_size, dim)
            out = embeddings @ embeddings.T
            print(F.softmax(out, dim=-1))

            -----------
            # tensor([[1., 0., 0., 0.],
            #         [0., 1., 0., 0.],
            #         [0., 0., 1., 0.],
            #         [0., 0., 0., 1.]])
        ```

        - So logits, in the best case, will be a matrix that if we take its softmax, will have 1.0s in the diagonal (An identity matrix to call it with fancy words!). As the loss function’s job is to make model’s predictions similar to targets (at least in most cases!), we want such a matrix as our target. That’s the reason why we are calculating images_similarity and texts_similarity matrices in the code block above.

    - simpler way to calculate this loss in PyTorch: `nn.CrossEntropyLoss()(logits, torch.arange(batch_size))`. 
        - There is the possibility that two identical images with their similar captions exist in a batch (it is rare but it can happen). Taking the loss with this easier method will ignore this possibility and the model learns to pull apart two representations (assume them different) that are actually the same. Obviously, we don’t want this to happen so I calculated the whole target matrix in a way that takes care of these edge cases. 
        - Better understanding of what is happening in this loss function.

- Inference: give the model a piece of text and retrieve the most relevant images from an unseen validation (or test) set.


## References
- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
- [Flickr8k_dataset](https://github.com/goodwillyoga/Flickr8k_dataset)
- [CLIP github](https://github.com/openai/CLIP)
- [Simple Implementation of OpenAI CLIP model: A Tutorial](https://towardsdatascience.com/simple-implementation-of-openai-clip-model-a-tutorial-ace6ff01d9f2)
