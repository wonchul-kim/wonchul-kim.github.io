---
layout: post
title: End-to-End Object Detection with Transformers (DETR)
category: Computer Vision
tag: [object detection, transformer]
---

# [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)

> "DETR are a **set-based** global loss that forces unique predictions via bi-partite matching and a transformer encoder-decoder architecture."

- `anchor-based`의 `object detection`은 하나의 `ground truth`에 대해서 다수의 `bounding box`를 예측하는 **many-to-one**이기 때문에 `NMS`와 같은 후처리 과정이 필수적이다. 

- **DETR**은 하나의 `ground truth`에 대해서 하나의 `bounding box`를 예측하는 **one-to-one**으로 후처리 과정이 필요하지 않다. 

    - 즉, `object detection`을 `direct set prediction`으로 정의하여, `transformer`와 `bipartite matching loss`를 사용

    - `Faster R-CNN`과 비슷한 성능을 보이지만, `self-attention`을 통한 `global information`을 활용함으로서 크기가 큰 객체에 대해서는 더 좋은 성능을 보임


## Network

<img src='/assets/computer_vision/papers/detr/detr_net.png'>

#### Positional Encoding
> "In our model we use a **fixed** absolute encoding to represent these spatial positions. We adopt a generalization of the original Transformer [47] encoding to the 2D case [31]. Specifically, for both spatial coordinates of each embedding we independently use $\frac{d}{2}$ sine and cosine functions with different frequencies. We then concatenate them to get the final $d$ channel positional encoding.

* original transformer encoding

    $$
    PE_{(pos, 2i)} = sin(pos/10000^{\frac{2i}{d_{model}}})
    PE_{(pos, 2i + 1)} = cos(pos/10000^{\frac{2i}{d_{model}}})
    $$

    - pos: 특정 위치
    - $d_{model}$: 만들고자 하는 차원 수 (짝수만 가능)
    - e.g.) $i = 0$에 대해서 sin과 cos이 번갈아가면서 나타난다.





## Training

- optimizer: AdamW
- learning rate: 
    - transformer: $1e^{-4}$
    - backbone을 $1e^{-5}$
- dropout: 
    - transformer: 0.1
- grad clip: 0.1 (전체 모델)

- augmentation: 
    - horizontal flips
    - scales
    - crops
        - rescale: 800 ~ 1333
