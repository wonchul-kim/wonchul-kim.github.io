---
layout: post
title: Why JAX for RL?
category: Reinforcement Learning
tag: [jax]
---


# 강화학습에서 JAX로 코딩하는 이유

#### 1. 속도 (JIT 컴파일 + XLA 최적화)

* **RL**은 환경과의 `상호작용 → 학습 → 다시 상호작용` 과정을 반복하는데, 수많은 `gradient update`, `대규모 rollout` 처리가 필요하기 때문에 속도가 매우 중요.

* **JAX**는 `@jit`로 함수 전체를 `XLA`로 컴파일해 GPU/TPU에서 최적화된 코드로 실행 → **PyTorch**보다 빠른 경우 많음.

#### 2. 자동 벡터화 (vmap)

* **RL**에서 흔히 `N`개의 환경을 동시에 돌려서 데이터 수집(parallel rollout) 

* **PyTorch**는 보통 `for loop`로 `batch`를 돌려야 하지만, **JAX**는 `jax.vmap` 한 줄로 벡터화 가능 → 코드가 간단해지고 성능도 좋아짐.

#### 3. 분산 학습 지원 (pmap, multi-host TPU)

* **RL**에서 SOTA 연구는 보통 수백~수천 개 환경을 동시에 돌려야 함.

* **JAX**는 TPU 클러스터, 멀티 GPU 병렬화를 기본 지원(`jax.pmap`) → 대규모 실험에서 유리.

#### 4. 함수형 스타일 (stateless)

* **RL**연구는 보통 새로운 `loss`, 새로운 `모델 구조`, 새로운 `업데이트 규칙`이 바뀜

* **JAX**는 순수 함수(`functional programming`) 스타일 → 실험 코드가 모듈화되고 버그 줄어듦.

* **PyTorch**의 `stateful parameter` 관리보다 실험/재현에 유리.

#### 5. 강화학습 전용 생태계

* 이미 JAX 기반 RL 라이브러리들이 활발히 개발 중:

    * Brax (Google, TPU 가속 RL 환경)

    * Acme (DeepMind RL 라이브러리)

    * RLax (DeepMind RL 알고리즘 모듈)

    * jaxrl / jaxrl-minimal (Offline RL 포함)
