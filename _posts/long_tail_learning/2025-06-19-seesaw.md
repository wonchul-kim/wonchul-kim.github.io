---
layout: post
title: Long-tail Learning
category: Long-tail Learning
tag: [long-tail learning]
---

# [Seesaw Loss for Long-Tailed Instance Segmentation](https://arxiv.org/pdf/2008.10032)

* **Seesaw Loss**는 손실 계산 시 클래스별 데이터 frequency를 고려하여 동적으로 보정하여, 빈도가 많은 클래스의 영향력과 적은 클래스의 영향력을 균형있게 맞추려는 목적

* 클래스별 샘플 수에 따라 조정 인자(weight)를 만들어서, 자주 등장하는 클래스에 대해선 손실 감소, 적은 클래스에 대해선 손실 증가 

* 다음의 두 인자를 시소 균형처럼 조합해서 loss 계산

    1. Mitigation Factor
        * 빈도수 차이로 인한 과도한 영향력을 줄임
        * 자주 등장하는 클래스에 대해 손실을 줄여서, 모델이 이를 너무 우선시하지 않게 함

    2. Reweight Factor
        * 반대로 적은 클래스에 대해 손실의 영향력을 높여, 학습 중 더 집중되도록 함


#### **Focal Loss**와 비교!

| 구분          | Focal Loss                                     | Seesaw Loss                                         |
| ----------- | ---------------------------------------------- | --------------------------------------------------- |
| **목적**      | 어려운 샘플(예: 잘못 분류된 샘플)에 더 집중하도록 손실을 조절           | 긴 꼬리 문제, 즉 클래스 빈도 불균형 문제를 해결하는 데 집중                 |
| **핵심 아이디어** | 예측이 잘 되는 샘플(즉, 확신 높은 샘플)의 손실을 감소시켜 학습 집중도 향상   | 자주 등장하는 클래스(Head) 손실은 완화, 드물게 등장하는 클래스(Tail) 손실은 강조 |
| **조정 기준**   | 개별 샘플의 예측 확률에 기반해 손실을 가중치 조절                   | 각 클래스별 샘플 빈도(데이터 수)에 기반해 손실 가중치 조절                  |
| **적용 대상**   | 모든 클래스에 동일하게 적용되는 샘플 단위 손실 조절                  | 클래스별 빈도 불균형에 대응하는 클래스 단위 손실 조절                      |
| **대표 수식**   | $\text{FL}(p_t) = -(1 - p_t)^\gamma \log(p_t)$ | Cross-Entropy + 클래스별 seesaw 가중치 곱셈                  |
| **기대 효과**   | 어려운 샘플에 더 집중해 모델 성능 향상                         | 긴 꼬리 분포에서 tail 클래스 성능 극대화, head 클래스 과도한 학습 억제       |


* Focal Loss는 "어떤 샘플이 잘못 분류되었는지"를 기준으로 개별 샘플마다 손실 가중치를 바꿉니다.
    * 쉽게 말해, 모델이 예측을 잘 못하는 hard example에 집중하게 합니다.

* Seesaw Loss는 "어떤 클래스가 얼마나 데이터가 많은지"를 기준으로 클래스별 손실을 동적으로 조정합니다.
    * 자주 등장하는 클래스는 손실을 줄여서 과도한 영향력을 억제하고, 드문 클래스는 손실을 키워서 학습 신호를 강화합니다.

## References:
- [Seesaw Loss for Long-Tailed Instance Segmentation](https://arxiv.org/pdf/2008.10032)


