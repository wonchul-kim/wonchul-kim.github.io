---
layout: post
title: BAM: Bottleneck Attention Module
category: Computer Vision
tag: bam
---

# [BAM: Bottleneck Attention Module](https://arxiv.org/abs/1807.06514)

## Problem

`Channel Attention`을 적용하면 전체 feature map에 대해서 중요한 `channel`에 대한 중요도를 적용할 수 있지만, 적용한 이후의 feature map에 대해서 각각의 `channel`을 기준으로 여전히 HxW의 feature map에서는 중요하지 않은 정보가 포함되어 있을 것이다. 


## Solution

<img src='/assets/bam/channel_spatial.png'>

`Channel Attention`으로부터 전체 feature map에 대해 중요한 `channel`을 골라냈다면, 해당 feature map에 대해서 `Spatial Attention`사용하여 중요한 정보를 다시금 정리한다. 

- `Channel Attention`은 Image level로 무엇인지에 대해서만 집중
- `Spatial Attention`은 위치에 집중


### `Channel Attention`

`SENet`에서의 `channel attention`의 개념을 그대로 사용하되, 가중치를 학습 및 도출하는 과정에서 `batch normalization`을 추가하여, `spatial attention`의 결과와 scale을 맞춘다. 

### `Spatial Attention`

`channel attention`이 feature map을 1x1xC의 차원으로 만들었다면, `spatial attention`은 feature map을 WxHx1의 차원으로 만들어야 한다. 

`channel`의 차원을 변화시켜야하므로, `1x1 convolution`이 대표적이지만 receptive field가 1x1이기 때문에 서로 다른 위치에 있는 정보를 연산할 수 없다. 따라서, 일반 `convolution`보다는 receptive field가 넓은 `dilated convolution`을 사용한다. 

<img src='/assets/bam/spatial.png'>

위의 그림처럼, `1x1 convolution`으로 channel을 줄여서 연산량을 줄인다. 그리고 두 번의 `dilated convolution`을 통해서 위치 정보에 대한 학습을 하고, 다시 `1x1 convolution`으로 WxHx1의 차원으로 만들어 spatial attention score map을 생성한다. 

### `Channel Attention` + `Spatial Attention`

<img src='/assets/bam/bam.png'>

- output dim. of `channel attention`: 1x1xC -> width/height 방향으로 복사/붙이기 -> WxHxC
- output dim. of `spatial attention`: WxHx1 -> channel 방향으로 복사/붙이기 -> WxHxC

서로 다른 output dim.을 생성하기 때문에 복사/붙이기를 통해서 WxHxC이 차원으로 만들고, element-wise summation을 한다. 

그리고 `Sigmoid` layer를 통해서 0 ~ 1의 값을 갖는 attention score map을 최종적으로 만들어준다.

> 이는 input과 output의 dimension이 동일하게 나오도록 하기 때문에 block단위로 어떤 네트워크에도 중간에 삽입하여 사용할 수 있다.  

<img src='/assets/bam/bam_block.png'>

## Implementation

```python

class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)
        
class ChannelAttention(nn.Module):
    def __init__(self, gate_channel, r=16, num_layers=1):
        super(ChannelAttention, self).__init__()
        self.gate_c = nn.Sequential()
        self.gate_c.add_module( 'flatten', Flatten() )
        gate_channels = [gate_channel]
        gate_channels += [gate_channel//r] * num_layers
        gate_channels += [gate_channel]
        for i in range( len(gate_channels) - 2 ):
            self.gate_c.add_module( 'gate_c_fc_%d'%i, nn.Linear(gate_channels[i], gate_channels[i+1]) )
            self.gate_c.add_module( 'gate_c_bn_%d'%(i+1), nn.BatchNorm1d(gate_channels[i+1]) )
            self.gate_c.add_module( 'gate_c_relu_%d'%(i+1), nn.ReLU() )
        self.gate_c.add_module( 'gate_c_fc_final', nn.Linear(gate_channels[-2], gate_channels[-1]) )
    def forward(self, in_tensor):
        avg_pool = F.avg_pool2d( in_tensor, in_tensor.size(2), stride=in_tensor.size(2) )
        return self.gate_c( avg_pool ).unsqueeze(2).unsqueeze(3).expand_as(in_tensor)

class SpatialAttention(nn.Module):
    def __init__(self, channel, r=16, dilation_conv_num=2, dilation_val=4):
        super().__init__()
        self.gate_s = nn.Sequential()
        self.gate_s.add_module( 'gate_s_conv_reduce0', nn.Conv2d(channel, channel//r, kernel_size=1))
        self.gate_s.add_module( 'gate_s_bn_reduce0',	nn.BatchNorm2d(channel//r) )
        self.gate_s.add_module( 'gate_s_relu_reduce0',nn.ReLU() )
        for i in range( dilation_conv_num ):
            self.gate_s.add_module( 'gate_s_conv_di_%d'%i, nn.Conv2d(channel//r, channel//r, kernel_size=3, \
						padding=dilation_val, dilation=dilation_val) )
            self.gate_s.add_module( 'gate_s_bn_di_%d'%i, nn.BatchNorm2d(channel//r) )
            self.gate_s.add_module( 'gate_s_relu_di_%d'%i, nn.ReLU() )
        self.gate_s.add_module( 'gate_s_conv_final', nn.Conv2d(channel//r, 1, kernel_size=1) )
    def forward(self, in_tensor):
        return self.gate_s( in_tensor ).expand_as(in_tensor)
	
class BAM(nn.Module):
    def __init__(self, channel):
        super(BAM, self).__init__()
        self.channel_att = ChannelAttention(channel)
        self.spatial_att = SpatialAttention(channel)
    def forward(self,in_tensor):
        att = 1 + F.sigmoid( self.channel_att(in_tensor) * self.spatial_att(in_tensor) )
        return att * in_tensor
```