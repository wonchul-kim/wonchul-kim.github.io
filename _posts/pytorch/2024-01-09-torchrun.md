---
layout: post
title: Torchrun to Execute Distributed Training
category: Pytorch
tag: distributed training
---

# Distributed Training by PyTorch

### ì‹¤í–‰

```
torchrun --nproc_per_node=<nodeë§ˆë‹¤ì˜ gpu ê°¯ìˆ˜> train.py ...
```

* `...`: `train.py`ì— í•„ìš”í•œ arugments

* `nproc_per_node`: `node`(ì„œë²„) í•˜ë‚˜ë§ˆë‹¤ì˜ gpu ê°¯ìˆ˜
    > `node`ë§ˆë‹¤ gpu ê°¯ìˆ˜ê°€ ë‹¤ë¥´ë©´...?

* `train.py`: ì‹¤í–‰ íŒŒì¼


ì´ë ‡ê²Œ ì‹¤í–‰í•˜ë©´, `os.environ`ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤. 

```python
rank = os.environ['RANK']
world_size = os.environ['WORLD_SIZE']
gpu = os.environ['LOCAL_RANK']
```

### `torch.distributed`

`torch.distributed`ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ˆê¸°í™” ê³¼ì •ì´ í•„ìš”í•˜ë©°, `torch.distributed.init_process_group`ì„ ì´ìš©í•œë‹¤.

> torch.distributed.init_process_group(backend=None, init_method=None, timeout=datetime.timedelta(seconds=1800), world_size=-1, rank=-1, store=None, group_name='', pg_options=None)

- `backend`: `NCCL`ê³¼ `Gloo`ì´ ìˆìœ¼ë©°, 
    - `nccl`: GPUë¥¼ í™œìš©í•œ ë¶„ì‚° í•™ìŠµ
    - `gloo`: CPUë¥¼ í™œìš©í•œ ë¶„ì‚° í•™ìŠµ

    > ìš°ë¶„íˆ¬ì—ì„œë§Œ ê°€ëŠ¥!

- `init_method`: ë‹¤ë¥¸ `node`ì™€ì˜ í†µì‹ ì„ í•˜ê¸° ìœ„í•œ URL
    > 0-ìˆœìœ„ í”„ë¡œì„¸ìŠ¤ì˜ IP ì£¼ì†Œì™€ ì ‘ê·¼ ê°€ëŠ¥í•œ í¬íŠ¸ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ TCPë¥¼ í†µí•œ ì´ˆê¸°í™”ë¥¼ í•  ìˆ˜ ìˆê³ , ëª¨ë“  ì›Œì»¤ë“¤ì€ 0-ìˆœìœ„ì˜ í”„ë¡œì„¸ìŠ¤ì— ì—°ê²°í•˜ê³  ì„œë¡œ ì •ë³´ë¥¼ êµí™˜í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, **one-node multi-gpu**ì—ì„œëŠ” `localhost IP`ì¸ `127.0.0.1` í˜¹ì€ `0.0.0.0`ë¡œ ì„¤ì •í•˜ê³ , `port`ëŠ” '23456' ì„ ì‚¬ìš©í•œë‹¤.

- `world_size`: process ê°¯ìˆ˜

- `rank`: process ID


ì´ëŸ¬í•œ ì´ˆê¸°í™” ê³¼ì •ì€ ê° device(GPU)/processë§ˆë‹¤ ì§„í–‰ì´ ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•˜ë‚˜ì˜ nodeì— 2ê°œì˜ GPUê°€ ìˆë‹¤ë©´, ê°ê°ì˜ GPUë§ˆë‹¤ ì§„í–‰ë˜ë¯€ë¡œ 2ë²ˆì˜ ì´ˆê¸°í™” ê³¼ì •ì´ ì§„í–‰ëœë‹¤. 

```python
if "RANK" in os.environ and "WORLD_SIZE" in os.environ:
    print("case 1")
    args.rank = int(os.environ["RANK"])
    args.world_size = int(os.environ["WORLD_SIZE"])
    args.gpu = int(os.environ["LOCAL_RANK"])
elif "SLURM_PROCID" in os.environ:
    print("case 2")
    args.rank = int(os.environ["SLURM_PROCID"])
    args.gpu = args.rank%torch.cuda.device_count()
elif hasattr(args, "rank"):
    print("case 3")
    pass
else:
    print("case 4")
    print("Not using distributed mode")
    args.distributed = False
    return

args.distributed = True

torch.cuda.set_device(args.gpu)
args.dist_backend = "nccl"
print(f"| distributed init (rank {args.rank}): {args.dist_url}", flush=True)
torch.distributed.init_process_group(
    backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank
)
torch.distributed.barrier()
```

### Dataset

#### `DistributedSampler`ë¥¼ ì‚¬ìš©í•´ì•¼í•˜ë©°, ì•„ë˜ì˜ ì½”ë“œì—ì„œì²˜ëŸ¼ 

- `sampler`ì—ì„œ `shuffle`ì„ í•œë‹¤ë©´, `DataLoader`ì—ì„œëŠ” í•˜ì§€ ì•ŠëŠ”ë‹¤.
- `batch_size`ì™€ `num_worker`ë¥¼ `gpu` ê°¯ìˆ˜ë¡œ ë‚˜ëˆˆë‹¤.


```python
from torch.utils.data import DataLoader 
from torch.utils.data.distributed import DistributedSampler

train_sampler = DistributedSampler(dataset=train_dataset, shuffle=True)
val_sampler = DistributedSampler(dataset=val_dataset, shuffle=False)

train_dataloader = DataLoader(dataset=train_dataset,
                                batch_size=int(args.batch_size/args.world_size),
                                shuffle=False,
                                num_workers=int(len(args.device_ids)*4/args.world_size),
                                sampler=train_sampler,
                                pin_memory=True)

val_dataloader = DataLoader(dataset=val_dataset,
                                batch_size=int(args.batch_size/args.world_size),
                                shuffle=False,
                                num_workers=int(len(args.device_ids)*4/args.world_size),
                                sampler=val_sampler,
                                pin_memory=True)
```


ìœ„ë¥¼ ë³´ë©´, `num_workers`ëŠ” GPU ê°¯ìˆ˜ì˜ 4ë°°ë¥¼ í•˜ì˜€ëŠ”ë°, ì•„ë˜ì™€ ê°™ì€ ì´ì•¼ê¸°ê°€ ìˆë‹¤. 
> harsv Hars Vardhan says the below in [Guidelines for assigning num_workers to DataLoader](https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/4): 
I experimented with this a bit. I found that we should use the formula:
num_worker = 4 * num_GPU .
Though a factor of 2 and 8 also work good but lower factor (<2) significantly reduces overall performance. Here, worker has no impact on GPU memory allocation. Also, nowadays there are many CPU cores in a machine with few GPUs (<8), so the above formula is practical.


#### ë¦¬ëˆ…ìŠ¤ì—ì„œ í”„ë¡œì„¸ìŠ¤ ê°¯ìˆ˜ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒê³¼ ê°™ìœ¼ë©°, `num_workers`ëŠ” í”„ë¡œì„¸ìŠ¤ ê°¯ìˆ˜ë§Œí¼ ìµœëŒ€ë¡œ í• ë‹¹ì´ ê°€ëŠ¥í•˜ë‹¤.
```cmd
cat /proc/cpuinfo | grep processor
```

### Model

#### `DistributedDataParallel`
```python
from torch.nn.parallel import DistributedDataParallel as DDP 
model = model.cuda(args.rank)
model = DDP(module=model, device_ids=[args.rank])
```

### Train

í•™ìŠµì´ ì§„í–‰ë  ë•Œ, ë§¤ epochê°€ ì‹œì‘í•˜ëŠ” ì‹œì ì—ì„œ `sampler`ì˜ `set_epoch()`ë¥¼ ì‹¤í–‰í•´ì—¬ `shuffle`ì´ ë™ì‘í•˜ë„ë¡ í•´ì•¼í•œë‹¤.
```python
train_sampler.set_epoch(epoch)
```

## DEMO
```python
# dataset.py
class SimpleDataset(torch.utils.data.Dataset):
    def __init__(self):
        super().__init__()
        print(f"initializing {__class__.__name__}")
        
        self.data = np.arange(20).tolist() 
        
    def __len__(self):
        return len(self.data)
        
    def __getitem__(self, idx):
        
        return self.data[idx]
    

if __name__ == '__main__':
    import argparse
    import time 

    import torch

    from dist import init_distributed_mode, get_rank

    args = argparse.ArgumentParser()
    args.add_argument('--dist-url')
    args.add_argument('--local-rank')

    args = args.parse_args()
    init_distributed_mode(args, False)

    print(args)

    device_ids = [1, 2]
    batch_size = 4
    num_workers = len(device_ids)*4
    epochs = 3

    train_dataset = SimpleDataset()
    val_dataset = SimpleDataset()
    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset=train_dataset, shuffle=True)
    val_sampler = torch.utils.data.distributed.DistributedSampler(dataset=val_dataset, shuffle=False)

    train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                                batch_size=int(batch_size/args.world_size),
                                                shuffle=False,
                                                num_workers=int(num_workers/args.world_size),
                                                sampler=train_sampler)

    # train_batch_sampler = torch.utils.data.BatchSampler(train_sampler, int(batch_size/args.world_size), drop_last=True)
    # train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, 
    #                                                 batch_sampler=train_batch_sampler, 
    #                                                 num_workers=num_workers
    #                                             )

    val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset,
                                                batch_size=int(batch_size/args.world_size),
                                                shuffle=False,
                                                num_workers=int(num_workers/args.world_size),
                                                sampler=val_sampler,
                                                pin_memory=True
                                                )

    for epoch in range(1, epochs):
        train_sampler.set_epoch(epoch)
        
        print(f"EPOCH: {epoch}")
        print(f"=== train_dataloader for rank({get_rank()}):")
        train_batch_data = {}
        train_batch_data[get_rank()] = []
        tic = time.time()
        for batch_idx, batch in enumerate(train_dataloader):
            train_batch_data[get_rank()] += batch.numpy().tolist()
            print(f"rank({get_rank()} > batch: {batch_idx} > {batch}")
            
        print(">>> train-batch-data: ", train_batch_data)
        print(f"cost {time.time() - tic} ms")

        print(f"=== val_dataloader for rank({get_rank()}):")
        val_batch_data = {}
        val_batch_data[get_rank()] = []
        tic = time.time()
        for batch_idx, batch in enumerate(val_dataloader):
            val_batch_data[get_rank()] += batch.numpy().tolist()
            print(f"rank({get_rank()} > batch: {batch_idx} > {batch}")
            
        print(">>> val-batch-data: ", val_batch_data)
        print(f"cost {time.time() - tic} ms")
        
        print("====================================================================================")
```

```cmd
Namespace(dist_url='env://', local_rank=None, rank=1, world_size=2, gpu=1, distributed=True, dist_backend='nccl')Namespace(dist_url='env://', local_rank=None, rank=0, world_size=2, gpu=0, distributed=True, dist_backend='nccl')
initializing SimpleDataset

initializing SimpleDataset
initializing SimpleDatasetEPOCH: 1

=== train_dataloader for rank(1):
initializing SimpleDataset
EPOCH: 1
=== train_dataloader for rank(0):
rank(0 > batch: 0 > tensor([5, 2])
rank(0 > batch: 1 > tensor([19,  1])
rank(1 > batch: 0 > tensor([13, 11])
rank(0 > batch: 2 > tensor([4, 0])
rank(0 > batch: 3 > tensor([16, 15])
rank(0 > batch: 4 > tensor([ 6, 12])
rank(1 > batch: 1 > tensor([18,  9])
>>> train-batch-data:  {0: [5, 2, 19, 1, 4, 0, 16, 15, 6, 12]}
cost 3.4715726375579834 ms
=== val_dataloader for rank(0):
rank(1 > batch: 2 > tensor([ 7, 14])
rank(1 > batch: 3 > tensor([10,  3])
rank(1 > batch: 4 > tensor([17,  8])
rank(0 > batch: 0 > tensor([0, 2])
>>> train-batch-data:  {1: [13, 11, 18, 9, 7, 14, 10, 3, 17, 8]}
cost 5.21843695640564 ms
=== val_dataloader for rank(1):
rank(0 > batch: 1 > tensor([4, 6])
rank(0 > batch: 2 > tensor([ 8, 10])
rank(0 > batch: 3 > tensor([12, 14])
rank(0 > batch: 4 > tensor([16, 18])
rank(1 > batch: 0 > tensor([1, 3])
>>> val-batch-data:  {0: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]}
cost 3.8364644050598145 ms
====================================================================================
EPOCH: 2
=== train_dataloader for rank(0):
rank(1 > batch: 1 > tensor([5, 7])
rank(1 > batch: 2 > tensor([ 9, 11])
rank(1 > batch: 3 > tensor([13, 15])
rank(1 > batch: 4 > tensor([17, 19])
rank(0 > batch: 0 > tensor([ 8, 19])
>>> val-batch-data:  {1: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]}
cost 4.263621807098389 ms
====================================================================================
EPOCH: 2
=== train_dataloader for rank(1):
rank(0 > batch: 1 > tensor([10, 12])
rank(0 > batch: 2 > tensor([16,  7])
rank(0 > batch: 3 > tensor([ 2, 13])
rank(0 > batch: 4 > tensor([11,  9])
rank(1 > batch: 0 > tensor([1, 0])
>>> train-batch-data:  {0: [8, 19, 10, 12, 16, 7, 2, 13, 11, 9]}
cost 4.318690538406372 ms
=== val_dataloader for rank(0):
rank(1 > batch: 1 > tensor([14,  4])
rank(1 > batch: 2 > tensor([15,  6])
rank(1 > batch: 3 > tensor([17, 18])
rank(1 > batch: 4 > tensor([3, 5])
rank(0 > batch: 0 > tensor([0, 2])
>>> train-batch-data:  {1: [1, 0, 14, 4, 15, 6, 17, 18, 3, 5]}
cost 4.246103525161743 ms
=== val_dataloader for rank(1):
rank(0 > batch: 1 > tensor([4, 6])
rank(0 > batch: 2 > tensor([ 8, 10])
rank(0 > batch: 3 > tensor([12, 14])
rank(0 > batch: 4 > tensor([16, 18])
rank(1 > batch: 0 > tensor([1, 3])
>>> val-batch-data:  {0: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]}
cost 4.097018003463745 ms
====================================================================================
rank(1 > batch: 1 > tensor([5, 7])
rank(1 > batch: 2 > tensor([ 9, 11])
rank(1 > batch: 3 > tensor([13, 15])
rank(1 > batch: 4 > tensor([17, 19])
>>> val-batch-data:  {1: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]}
cost 4.337230443954468 ms
====================================================================================
```

ìœ„ì™€ ê°™ì´, 1 ~ 20ê¹Œì§€ì˜ dataì— ëŒ€í•´ì„œ GPU0, GPU1ì´ ê°ê° ë°°ì¹˜ë¥¼ ìƒì„±í•˜ë©´ì„œ ì§„í–‰ë˜ê³ , train_datasetì˜ ê²½ìš° `shuffle`ì„ í•˜ì—¬ ë§¤ epochë§ˆë‹¤ ìˆœì„œê°€ ë‹¤ë¥´ë‹¤.
ì½”ë“œëŠ” [github](https://github.com/wonchul-kim/distributed_training)ì—ì„œ ì°¸ê³  ê°€ëŠ¥í•©ë‹ˆë‹¤.

## references:

- [Guidelines for assigning num_workers to DataLoader](https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/4)

- [pytorch examples for ddp](https://github.com/pytorch/examples/blob/main/distributed/ddp/README.md)

- [ğŸ’¥ Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)

- [PYTORCHë¡œ ë¶„ì‚° ì–´í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œí•˜ê¸°](https://tutorials.pytorch.kr/intermediate/dist_tuto.html)

- [github](https://github.com/wonchul-kim/distributed_training)