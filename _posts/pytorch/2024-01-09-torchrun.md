---
layout: post
title: Torchrun to Execute Distributed Training
category: Pytorch
tag: distributed training
---

# Distributed Training by PyTorch

### 실행

```
torchrun --nproc_per_node=<node마다의 gpu 갯수> train.py ...
```

* `...`: `train.py`에 필요한 arugments

* `nproc_per_node`: `node`(서버) 하나마다의 gpu 갯수
    > `node`마다 gpu 갯수가 다르면...?

* `train.py`: 실행 파일


이렇게 실행하면, `os.environ`에는 다음과 같은 정보가 포함되어 있다. 

```python
rank = os.environ['RANK']
world_size = os.environ['WORLD_SIZE']
gpu = os.environ['LOCAL_RANK']
```

### `torch.distributed`

`torch.distributed`를 사용하기 위해서는 초기화 과정이 필요하며, `torch.distributed.init_process_group`을 이용한다.

> torch.distributed.init_process_group(backend=None, init_method=None, timeout=datetime.timedelta(seconds=1800), world_size=-1, rank=-1, store=None, group_name='', pg_options=None)

- `backend`: `NCCL`과 `Gloo`이 있으며, 
    - `nccl`: GPU를 활용한 분산 학습
    - `gloo`: CPU를 활용한 분산 학습

    > 우분투에서만 가능!

- `init_method`: 다른 `node`와의 통신을 하기 위한 URL
    > 0-순위 프로세스의 IP 주소와 접근 가능한 포트 번호가 있으면 TCP를 통한 초기화를 할 수 있고, 모든 워커들은 0-순위의 프로세스에 연결하고 서로 정보를 교환한다. 그러므로, **one-node multi-gpu**에서는 `localhost IP`인 `127.0.0.1` 혹은 `0.0.0.0`로 설정하고, `port`는 '23456' 을 사용한다.

- `world_size`: process 갯수

- `rank`: process ID


이러한 초기화 과정은 각 device(GPU)/process마다 진행이 된다. 예를 들어, 하나의 node에 2개의 GPU가 있다면, 각각의 GPU마다 진행되므로 2번의 초기화 과정이 진행된다. 

```python
if "RANK" in os.environ and "WORLD_SIZE" in os.environ:
    print("case 1")
    args.rank = int(os.environ["RANK"])
    args.world_size = int(os.environ["WORLD_SIZE"])
    args.gpu = int(os.environ["LOCAL_RANK"])
elif "SLURM_PROCID" in os.environ:
    print("case 2")
    args.rank = int(os.environ["SLURM_PROCID"])
    args.gpu = args.rank%torch.cuda.device_count()
elif hasattr(args, "rank"):
    print("case 3")
    pass
else:
    print("case 4")
    print("Not using distributed mode")
    args.distributed = False
    return

args.distributed = True

torch.cuda.set_device(args.gpu)
args.dist_backend = "nccl"
print(f"| distributed init (rank {args.rank}): {args.dist_url}", flush=True)
torch.distributed.init_process_group(
    backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank
)
torch.distributed.barrier()
```

### Dataset

#### `DistributedSampler`를 사용해야하며, 아래의 코드에서처럼 

- `sampler`에서 `shuffle`을 한다면, `DataLoader`에서는 하지 않는다.
- `batch_size`와 `num_worker`를 `gpu` 갯수로 나눈다.


```python
from torch.utils.data import DataLoader 
from torch.utils.data.distributed import DistributedSampler

train_sampler = DistributedSampler(dataset=train_dataset, shuffle=True)
val_sampler = DistributedSampler(dataset=val_dataset, shuffle=False)

train_dataloader = DataLoader(dataset=train_dataset,
                                batch_size=int(args.batch_size/args.world_size),
                                shuffle=False,
                                num_workers=int(len(args.device_ids)*4/args.world_size),
                                sampler=train_sampler,
                                pin_memory=True)

val_dataloader = DataLoader(dataset=val_dataset,
                                batch_size=int(args.batch_size/args.world_size),
                                shuffle=False,
                                num_workers=int(len(args.device_ids)*4/args.world_size),
                                sampler=val_sampler,
                                pin_memory=True)
```


위를 보면, `num_workers`는 GPU 갯수의 4배를 하였는데, 아래와 같은 이야기가 있다. 
> harsv Hars Vardhan says the below in [Guidelines for assigning num_workers to DataLoader](https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/4): 
I experimented with this a bit. I found that we should use the formula:
num_worker = 4 * num_GPU .
Though a factor of 2 and 8 also work good but lower factor (<2) significantly reduces overall performance. Here, worker has no impact on GPU memory allocation. Also, nowadays there are many CPU cores in a machine with few GPUs (<8), so the above formula is practical.


#### 리눅스에서 프로세스 갯수를 확인하기 위해서는 다음과 같으며, `num_workers`는 프로세스 갯수만큼 최대로 할당이 가능하다.
```cmd
cat /proc/cpuinfo | grep processor
```

### Model

#### `DistributedDataParallel`
```python
from torch.nn.parallel import DistributedDataParallel as DDP 
model = model.cuda(args.rank)
model = DDP(module=model, device_ids=[args.rank])
```

### Train

학습이 진행될 때, 매 epoch가 시작하는 시점에서 `sampler`의 `set_epoch()`를 실행해여 `shuffle`이 동작하도록 해야한다.
```python
train_sampler.set_epoch(epoch)
```

## DEMO
```python
# dataset.py
class SimpleDataset(torch.utils.data.Dataset):
    def __init__(self):
        super().__init__()
        print(f"initializing {__class__.__name__}")
        
        self.data = np.arange(20).tolist() 
        
    def __len__(self):
        return len(self.data)
        
    def __getitem__(self, idx):
        
        return self.data[idx]
    

if __name__ == '__main__':
    import argparse
    import time 

    import torch

    from dist import init_distributed_mode, get_rank

    args = argparse.ArgumentParser()
    args.add_argument('--dist-url')
    args.add_argument('--local-rank')

    args = args.parse_args()
    init_distributed_mode(args, False)

    print(args)

    device_ids = [1, 2]
    batch_size = 4
    num_workers = len(device_ids)*4
    epochs = 3

    train_dataset = SimpleDataset()
    val_dataset = SimpleDataset()
    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset=train_dataset, shuffle=True)
    val_sampler = torch.utils.data.distributed.DistributedSampler(dataset=val_dataset, shuffle=False)

    train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                                batch_size=int(batch_size/args.world_size),
                                                shuffle=False,
                                                num_workers=int(num_workers/args.world_size),
                                                sampler=train_sampler)

    # train_batch_sampler = torch.utils.data.BatchSampler(train_sampler, int(batch_size/args.world_size), drop_last=True)
    # train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, 
    #                                                 batch_sampler=train_batch_sampler, 
    #                                                 num_workers=num_workers
    #                                             )

    val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset,
                                                batch_size=int(batch_size/args.world_size),
                                                shuffle=False,
                                                num_workers=int(num_workers/args.world_size),
                                                sampler=val_sampler,
                                                pin_memory=True
                                                )

    for epoch in range(1, epochs):
        train_sampler.set_epoch(epoch)
        
        print(f"EPOCH: {epoch}")
        print(f"=== train_dataloader for rank({get_rank()}):")
        train_batch_data = {}
        train_batch_data[get_rank()] = []
        tic = time.time()
        for batch_idx, batch in enumerate(train_dataloader):
            train_batch_data[get_rank()] += batch.numpy().tolist()
            print(f"rank({get_rank()} > batch: {batch_idx} > {batch}")
            
        print(">>> train-batch-data: ", train_batch_data)
        print(f"cost {time.time() - tic} ms")

        print(f"=== val_dataloader for rank({get_rank()}):")
        val_batch_data = {}
        val_batch_data[get_rank()] = []
        tic = time.time()
        for batch_idx, batch in enumerate(val_dataloader):
            val_batch_data[get_rank()] += batch.numpy().tolist()
            print(f"rank({get_rank()} > batch: {batch_idx} > {batch}")
            
        print(">>> val-batch-data: ", val_batch_data)
        print(f"cost {time.time() - tic} ms")
        
        print("====================================================================================")
```

```cmd
Namespace(dist_url='env://', local_rank=None, rank=1, world_size=2, gpu=1, distributed=True, dist_backend='nccl')Namespace(dist_url='env://', local_rank=None, rank=0, world_size=2, gpu=0, distributed=True, dist_backend='nccl')
initializing SimpleDataset

initializing SimpleDataset
initializing SimpleDatasetEPOCH: 1

=== train_dataloader for rank(1):
initializing SimpleDataset
EPOCH: 1
=== train_dataloader for rank(0):
rank(0 > batch: 0 > tensor([5, 2])
rank(0 > batch: 1 > tensor([19,  1])
rank(1 > batch: 0 > tensor([13, 11])
rank(0 > batch: 2 > tensor([4, 0])
rank(0 > batch: 3 > tensor([16, 15])
rank(0 > batch: 4 > tensor([ 6, 12])
rank(1 > batch: 1 > tensor([18,  9])
>>> train-batch-data:  {0: [5, 2, 19, 1, 4, 0, 16, 15, 6, 12]}
cost 3.4715726375579834 ms
=== val_dataloader for rank(0):
rank(1 > batch: 2 > tensor([ 7, 14])
rank(1 > batch: 3 > tensor([10,  3])
rank(1 > batch: 4 > tensor([17,  8])
rank(0 > batch: 0 > tensor([0, 2])
>>> train-batch-data:  {1: [13, 11, 18, 9, 7, 14, 10, 3, 17, 8]}
cost 5.21843695640564 ms
=== val_dataloader for rank(1):
rank(0 > batch: 1 > tensor([4, 6])
rank(0 > batch: 2 > tensor([ 8, 10])
rank(0 > batch: 3 > tensor([12, 14])
rank(0 > batch: 4 > tensor([16, 18])
rank(1 > batch: 0 > tensor([1, 3])
>>> val-batch-data:  {0: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]}
cost 3.8364644050598145 ms
====================================================================================
EPOCH: 2
=== train_dataloader for rank(0):
rank(1 > batch: 1 > tensor([5, 7])
rank(1 > batch: 2 > tensor([ 9, 11])
rank(1 > batch: 3 > tensor([13, 15])
rank(1 > batch: 4 > tensor([17, 19])
rank(0 > batch: 0 > tensor([ 8, 19])
>>> val-batch-data:  {1: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]}
cost 4.263621807098389 ms
====================================================================================
EPOCH: 2
=== train_dataloader for rank(1):
rank(0 > batch: 1 > tensor([10, 12])
rank(0 > batch: 2 > tensor([16,  7])
rank(0 > batch: 3 > tensor([ 2, 13])
rank(0 > batch: 4 > tensor([11,  9])
rank(1 > batch: 0 > tensor([1, 0])
>>> train-batch-data:  {0: [8, 19, 10, 12, 16, 7, 2, 13, 11, 9]}
cost 4.318690538406372 ms
=== val_dataloader for rank(0):
rank(1 > batch: 1 > tensor([14,  4])
rank(1 > batch: 2 > tensor([15,  6])
rank(1 > batch: 3 > tensor([17, 18])
rank(1 > batch: 4 > tensor([3, 5])
rank(0 > batch: 0 > tensor([0, 2])
>>> train-batch-data:  {1: [1, 0, 14, 4, 15, 6, 17, 18, 3, 5]}
cost 4.246103525161743 ms
=== val_dataloader for rank(1):
rank(0 > batch: 1 > tensor([4, 6])
rank(0 > batch: 2 > tensor([ 8, 10])
rank(0 > batch: 3 > tensor([12, 14])
rank(0 > batch: 4 > tensor([16, 18])
rank(1 > batch: 0 > tensor([1, 3])
>>> val-batch-data:  {0: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]}
cost 4.097018003463745 ms
====================================================================================
rank(1 > batch: 1 > tensor([5, 7])
rank(1 > batch: 2 > tensor([ 9, 11])
rank(1 > batch: 3 > tensor([13, 15])
rank(1 > batch: 4 > tensor([17, 19])
>>> val-batch-data:  {1: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]}
cost 4.337230443954468 ms
====================================================================================
```

위와 같이, 1 ~ 20까지의 data에 대해서 GPU0, GPU1이 각각 배치를 생성하면서 진행되고, train_dataset의 경우 `shuffle`을 하여 매 epoch마다 순서가 다르다.
코드는 [github](https://github.com/wonchul-kim/distributed_training)에서 참고 가능합니다.

## references:

- [Guidelines for assigning num_workers to DataLoader](https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/4)

- [pytorch examples for ddp](https://github.com/pytorch/examples/blob/main/distributed/ddp/README.md)

- [💥 Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)

- [PYTORCH로 분산 어플리케이션 개발하기](https://tutorials.pytorch.kr/intermediate/dist_tuto.html)

- [github](https://github.com/wonchul-kim/distributed_training)