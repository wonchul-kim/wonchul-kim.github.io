---
layout: post
title: Torchrun to Execute Distributed Training
category: Pytorch
tag: distributed training
---

# Distributed Training by PyTorch

### ì‹¤í–‰

```
torchrun --nproc_per_node=<nodeë§ˆë‹¤ì˜ gpu ê°¯ìˆ˜> train.py ...
```

* `...`: `train.py`ì— í•„ìš”í•œ arugments

* `nproc_per_node`: `node`(ì„œë²„) í•˜ë‚˜ë§ˆë‹¤ì˜ gpu ê°¯ìˆ˜
    > `node`ë§ˆë‹¤ gpu ê°¯ìˆ˜ê°€ ë‹¤ë¥´ë©´...?

* `train.py`: ì‹¤í–‰ íŒŒì¼


ì´ë ‡ê²Œ ì‹¤í–‰í•˜ë©´, `os.environ`ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìžˆë‹¤. 

```python
rank = os.environ['RANK']
world_size = os.environ['WORLD_SIZE']
gpu = os.environ['LOCAL_RANK']
```

### `torch.distributed`

`torch.distributed`ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ˆê¸°í™” ê³¼ì •ì´ í•„ìš”í•˜ë©°, `torch.distributed.init_process_group`ì„ ì´ìš©í•œë‹¤.

> torch.distributed.init_process_group(backend=None, init_method=None, timeout=datetime.timedelta(seconds=1800), world_size=-1, rank=-1, store=None, group_name='', pg_options=None)

- `backend`: `NCCL`ê³¼ `Gloo`ì´ ìžˆìœ¼ë©°, 
    - `nccl`: GPUë¥¼ í™œìš©í•œ ë¶„ì‚° í•™ìŠµ
    - `gloo`: CPUë¥¼ í™œìš©í•œ ë¶„ì‚° í•™ìŠµ

    > ìš°ë¶„íˆ¬ì—ì„œë§Œ ê°€ëŠ¥!

- `init_method`: ë‹¤ë¥¸ `node`ì™€ì˜ í†µì‹ ì„ í•˜ê¸° ìœ„í•œ URL
    > 0-ìˆœìœ„ í”„ë¡œì„¸ìŠ¤ì˜ IP ì£¼ì†Œì™€ ì ‘ê·¼ ê°€ëŠ¥í•œ í¬íŠ¸ ë²ˆí˜¸ê°€ ìžˆìœ¼ë©´ TCPë¥¼ í†µí•œ ì´ˆê¸°í™”ë¥¼ í•  ìˆ˜ ìžˆê³ , ëª¨ë“  ì›Œì»¤ë“¤ì€ 0-ìˆœìœ„ì˜ í”„ë¡œì„¸ìŠ¤ì— ì—°ê²°í•˜ê³  ì„œë¡œ ì •ë³´ë¥¼ êµí™˜í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, **one-node multi-gpu**ì—ì„œëŠ” `localhost IP`ì¸ `127.0.0.1` í˜¹ì€ `0.0.0.0`ë¡œ ì„¤ì •í•˜ê³ , `port`ëŠ” '23456' ì„ ì‚¬ìš©í•œë‹¤.

- `world_size`: process ê°¯ìˆ˜

- `rank`: process ID


ì´ëŸ¬í•œ ì´ˆê¸°í™” ê³¼ì •ì€ ê° device(GPU)/processë§ˆë‹¤ ì§„í–‰ì´ ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•˜ë‚˜ì˜ nodeì— 2ê°œì˜ GPUê°€ ìžˆë‹¤ë©´, ê°ê°ì˜ GPUë§ˆë‹¤ ì§„í–‰ë˜ë¯€ë¡œ 2ë²ˆì˜ ì´ˆê¸°í™” ê³¼ì •ì´ ì§„í–‰ëœë‹¤. 

```python
if "RANK" in os.environ and "WORLD_SIZE" in os.environ:
    print("case 1")
    args.rank = int(os.environ["RANK"])
    args.world_size = int(os.environ["WORLD_SIZE"])
    args.gpu = int(os.environ["LOCAL_RANK"])
elif "SLURM_PROCID" in os.environ:
    print("case 2")
    args.rank = int(os.environ["SLURM_PROCID"])
    args.gpu = args.rank%torch.cuda.device_count()
elif hasattr(args, "rank"):
    print("case 3")
    pass
else:
    print("case 4")
    print("Not using distributed mode")
    args.distributed = False
    return

args.distributed = True

torch.cuda.set_device(args.gpu)
args.dist_backend = "nccl"
print(f"| distributed init (rank {args.rank}): {args.dist_url}", flush=True)
torch.distributed.init_process_group(
    backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank
)
torch.distributed.barrier()
```

### Dataset

#### `DistributedSampler`ë¥¼ ì‚¬ìš©í•´ì•¼í•˜ë©°, ì•„ëž˜ì˜ ì½”ë“œì—ì„œì²˜ëŸ¼ 

- `sampler`ì—ì„œ `shuffle`ì„ í•œë‹¤ë©´, `DataLoader`ì—ì„œëŠ” í•˜ì§€ ì•ŠëŠ”ë‹¤.
- `batch_size`ì™€ `num_worker`ë¥¼ `gpu` ê°¯ìˆ˜ë¡œ ë‚˜ëˆˆë‹¤.


```python
from torch.utils.data import DataLoader 
from torch.utils.data.distributed import DistributedSampler

train_sampler = DistributedSampler(dataset=train_dataset, shuffle=True)
val_sampler = DistributedSampler(dataset=val_dataset, shuffle=False)

train_dataloader = DataLoader(dataset=train_dataset,
                                batch_size=int(args.batch_size/args.world_size),
                                shuffle=False,
                                num_workers=int(len(args.device_ids)*4/args.world_size),
                                sampler=train_sampler,
                                pin_memory=True)

val_dataloader = DataLoader(dataset=val_dataset,
                                batch_size=int(args.batch_size/args.world_size),
                                shuffle=False,
                                num_workers=int(len(args.device_ids)*4/args.world_size),
                                sampler=val_sampler,
                                pin_memory=True)
```


ìœ„ë¥¼ ë³´ë©´, `num_workers`ëŠ” GPU ê°¯ìˆ˜ì˜ 4ë°°ë¥¼ í•˜ì˜€ëŠ”ë°, ì•„ëž˜ì™€ ê°™ì€ ì´ì•¼ê¸°ê°€ ìžˆë‹¤. 
> harsv Hars Vardhan says the below in [Guidelines for assigning num_workers to DataLoader](https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/4): 
I experimented with this a bit. I found that we should use the formula:
num_worker = 4 * num_GPU .
Though a factor of 2 and 8 also work good but lower factor (<2) significantly reduces overall performance. Here, worker has no impact on GPU memory allocation. Also, nowadays there are many CPU cores in a machine with few GPUs (<8), so the above formula is practical.


#### ë¦¬ëˆ…ìŠ¤ì—ì„œ í”„ë¡œì„¸ìŠ¤ ê°¯ìˆ˜ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒê³¼ ê°™ìœ¼ë©°, `num_workers`ëŠ” í”„ë¡œì„¸ìŠ¤ ê°¯ìˆ˜ë§Œí¼ ìµœëŒ€ë¡œ í• ë‹¹ì´ ê°€ëŠ¥í•˜ë‹¤.
```cmd
cat /proc/cpuinfo | grep processor
```

### Model

#### `DistributedDataParallel`
```python
from torch.nn.parallel import DistributedDataParallel as DDP 
model = model.cuda(args.rank)
model = DDP(module=model, device_ids=[args.rank])
```

### Train

í•™ìŠµì´ ì§„í–‰ë  ë•Œ, ë§¤ epochê°€ ì‹œìž‘í•˜ëŠ” ì‹œì ì—ì„œ `sampler`ì˜ `set_epoch()`ë¥¼ ì‹¤í–‰í•´ì—¬ `shuffle`ì´ ë™ìž‘í•˜ë„ë¡ í•´ì•¼í•œë‹¤.
```python
train_sampler.set_epoch(epoch)
```

## references:

- [Guidelines for assigning num_workers to DataLoader](https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/4)

- [pytorch examples for ddp](https://github.com/pytorch/examples/blob/main/distributed/ddp/README.md)

- [ðŸ’¥ Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)

- [PYTORCHë¡œ ë¶„ì‚° ì–´í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œí•˜ê¸°](https://tutorials.pytorch.kr/intermediate/dist_tuto.html)