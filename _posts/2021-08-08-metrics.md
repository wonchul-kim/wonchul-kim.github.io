---
layout: post
title: Metrics for evaluating algorithms
category: Lectures
tag: metrics
---

# Metrics to evaluate algorithm performance

구현한 알고리즘의 성능을 수치로 나타내기 위한 기법으로서 여러 종류가 존재하기 때문에 잘 선택해야한다!

## Confusion matrix

알고리즘의 결과(예측값, prediction)는 다음과 같이 실제의 값(ground truth)과 비교하면 다음과 같이 confusion matrix로 정리가 가능하다. 

^^|^^|    **PREDICTION**||
:-------------: | :--------: | :---------:| :---------:
^^              | ^^         | *positive* | *negative* |
**GROUND TRUTH**| *positive* |     TP     |     FN     |
^^              | *negative* |     FP     |     TN     |

* `True` & `False` : 예측값이 실제값과 같은지에 대한 판단  

* positive & negative : 예측값에 대한 판단

그리고 이를 바탕으로 다양한 알고리즘의 성능을 정량화가 가능하고, 알고리즘의 목적(task)에 따라서 다음과 같이 다양한 수치로 정의하여 알고리즘의 성능을 정의할 수 있다.  


## For object detectoin/segmentation

#### Precision (정확도)

<U>전체 검출 결과 중</U>에서 `True`를 예측한 비율

$$ Precision = \frac{TP}{TP + FP} = \frac{TP}{all \; detected} $$


#### Recall (재현율, or sensitivity(민감도), hit rate(적중률))

<U>정답 중</U>에서 `True`를 예측한 비율

$$ Recall = \frac{TP}{TP + FN} = \frac{TP}{all \; ground \; truth} $$

#### Precision-Recall curve (PR curve)

알고리즘의 성능을 대변하는 precision과 recall의 두 지표는 서로 <U>반비례</U> 관계를 가지기 때문에 이 둘을 모두 고려하는 precision-recall curve를 이용해야 한다.

이에 앞서, prediction에서 positive와 negative는 이를 구분할 수 있는 기준 및 수치인 `threshold`에 따라서 값이 달라지기 때문에 precision과 recall도 달라진다. 예를 들어, 어떤 물체를 탐지하는 데에 있어서 가능성(confidence)을 80%라고 할 때, `threshold`가 70이면 positive이지만, 90일 경우에는 negative가 된다. 

이러한 `threshold`를 0부터 1까지 정해진 단위간격(e.g. 0.1)으로 precision과 recall이 달라지는 두 관계를 나타낸 것이 precision-recall curve이고, $y$축은 precision이고, $x$축은 recall이다. 따라서, PR curve는 recall의 변화에 따른 precision이다. 


* **Average Precision (AP)**

    precision-Recall curve를 통하여 알고리즘의 성능을 정량적으로 나타내기 위한 개념으로서 그래프 선 아래 쪽의 면적(넓이)으로 계산한다. 이 때, AP가 높으면 높을수록 그 알고리즘의 성능이 전체적으로 우수하다는 의미로 통한다. 

    <U>특히, computer vision에서 object detection 또는 segmentation 분야에서는 주로 AP로 성능을 평가 및 비교하고, 이는 각 클래스 마다  얼마나 잘 탐지하는지를 말해준다.</U>

* **Mean Average Precision (mAP)**

    AP로 성능을 평가할 때, 물체의 클래스가 여러 개인 경우 각 클래스에 대한 AP의 평균으로 나눈 값으로 계산하고, 이 수치로 알고리즘의 전체 성능을 수치화하여 평가할 수 있다.   


## For binary classification

#### True Positive Rate (TPR, 적중률)
실제 `True`인 것을 알고리즘이 `True`라고 예측한 확률로서, 정답 중에서 `True`를 예측하는 확률로 <U>recall과 동일하다.</U> 

$$ TPR = \frac{TP}{TP + FN} $$

#### False Positive Rate (FPR, 오경보확률 or `False` Alarm Rate)
전체 `False`인 것 중에서 `True`로 잘 못 예측한 확률을 나타낸다. 

$$ FPR = \frac{FP}{FP + TN} = 1 - specificity$$

* **Specificity**
  
    실제 `False`인 것을 알고리즘이 `False`라고 예측할 확률이다. 

    <p align='center'><img src='./imgs/fpr_`threshold`.png' loc=center></p>

    * `threshold`를 높이면, positive rate가 커지기 떄문에 FPR(sensitivity)는 증가하지만, specificity는 감소한다.
  
    * `threshold`를 낮추면, positive rate가 작아지기 떄문에 FPR(sensitivity)는 감소하지만, specificity는 증가한다.


#### Receiver Operating Characteristic (ROC)

$x$축은 FPR, $y$축에 TPR로 한 그래포로 <U>classification 알고리즘의 성능을 정리하기 위한 도구로 사용된다.</U> 

이 때, `True`로 분류하는 확률의 `threshold`가 낮을수록 TPR은 높아지지만 동시에 FPR 역시 높아진다. 반대로 `True`로 분류하는 확률의 `threshold`가 높을수록 FPR은 낮아지지만 동시에 TPR역시 낮아진다. 따라서, TPR이 높을수록, 그리고 FPR이 낮을수록 좋은 알고리즘이기 때문에 이를 염두에 두고 threhold를 찾을 수 있다.  

* **Area Under ROC (AUROC)**

    AUC는 ROC curve 아래쪽의 면적(넓이)으로 계산이 되고, 클 수록 FPR과 관계없이 TPR이 높다고 볼 수 있으므로 좋은 알고리즘이라고 할 수 있다. 반대로, 최악의 모델은 AUROC가 0.5로 이는 기울기 1의 직선에 해당하는 것으로 가각 `True`와 `False`일 확률이 0.5이므로 어느 한쪽으로 결정을 할 수 없다는 것과 마찬가지이다. 그리고 AUROC가 0.5보다 낮으면 아이러니하게도 클래스를 반대로 예측한다는 것을 의미한다. 

    TPR은 전체 양성 샘플 중에 양성으로 예측된 샘플의 비율이기 때문에 TPR은 높을 수록 좋고, FPR은 전체 음성 샘플 중에 양성으로 잘못 예측된 것의 비율이기 때문에 FPR은 낮을 수록 좋다. 그리고 `threshold`가 커지면 양성으로 예측되는 경우가 적어지기 때문에 TPR이든 FPR이든 작아질 수 밖에 없다. 따라서, <U>FPR이 작아질 때 TPR이 천천히 작아진다면, 좋은 성능을 가진 알고리즘이고,</U> 이는 곧 곡선 아래의 넓이가 1에 가까워지는 것을 의미한다. 그렇기 때문에 AUC로 이진분류기의 성능을 평가할 수 있다. 

 

대개 AUC가 0.8이상이면 아주 훌륭한 성능을 가진 이진분류기라고 평가합니다. 0.7과 0.8 사이라면 좋은 이진분류기입니다. AUC가 0.5~0.7라면 있으면 도움은 되는 이진분류기입니다. AUC가 만약 0.5이하라면 쓸모없는 이진분류기입니다. 없는 것만 못합니다. 


#### $F_{\beta}$
F-베타 또한 이진 분류모델의 성능을 나타내는 지표값입니다. FBeta는 Recall과 Precision을 하나로 합쳐준 값이며 다음과 같이 계산됩니다.

$$ F_{beta} = (1 + \beta^2) \times \frac{precision \times recall}{(\beta^2 \times precision) + recall} $$

여기서 \beta는 Recall을 Precision보다 얼마나 중요하게 다룰지를 나타냅니다. \beta는 데이터의 특성과 분류 목적에 따라 정하면 됩니다. Recall과 Precision이 동일하게 중요한 경우 1을, Recall이 중요한 경우 1보다 큰 값을, Precision이 중요한 경우 0과 1 사이값을 선택하면 되겠습니다.

예를 들어 전염병 검사와 같이 Recall이 중요한 경우 (`False` Negative를 줄여야 하는 경우), 높은 \beta값을 선택해야 합니다.
하지만 동영상 추천과 같이 Precision이 중요한 경우 (`False` Positive를 줄여야 하는 경우), 낮은 \beta값을 선택해야 합니다. 왜냐하면 모델이 추천해준 동영상 목록 중 사용자가 관심 있는 동영상이 많고 관심 없는 동영상이 적어야 하기 때문입니다.

## References

* [The ultimate guide to binary classification metrics](https://towardsdatascience.com/the-ultimate-guide-to-binary-classification-metrics-c25c3627dd0a#9891)

* [[python] 쉽고 간단하게 마스크 착용 유무 판별기 만들기.txt](https://bskyvision.com/1082)

* [ROC curve](https://angeloyeo.github.io/2020/08/05/ROC.html)


## TO DO

* https://newsight.tistory.com/53
