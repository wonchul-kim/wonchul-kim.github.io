---
layout: post
title: RVT2
category: Reinforcement Learning
tag: [act]
---

# [RVT-2: Learning Precise Manipulation from Few Demonstrations](https://arxiv.org/pdf/2406.08545)


- solve multiple 3D manipulation tasks given language instructions. 

- from few demonstrations and solving them precisely. 

- Prior works, like PerAct [40] and RVT [17], have studied this problem, however, they often struggle with tasks requiring high precision.

- Using a combination of architectural and system-level improvements, we propose RVT-2, a multitask 3D manipulation model that is 6X faster in training and 2X faster in inference than its predecessor RVT. 


## Intro.
(1) handle multiple tasks
(2) require only a few demonstrations
(3) solve tasks with high precision

Starting with works like Transporter Networks [52] and IFOR [16] that studied planar pick-and-place tasks, recent
works have gone beyond the 2D plane and studied manipulation in 3D with a few examples [25]. 

By using camera images, Schoettler et al. [37] presents a residual reinforcement learning algorithm to accomplish industrial insertion tasks from visual sensory inputs. 

there are significant differences between ACT and RVT-2. Given language input, RVT-2 can solve different variations of a task while ACT does not take language as input and can only be trained with one variation of a task at a time. 

RVT-2 makes key-point based predictions while ACT makes continuous joint state predictions. 

RVT-2 takes point cloud as input while ACT works with multiview images.

Virtual Views for 3D Vision. The use of virtual views provides a strategic lever for exploiting well-established imagebased neural network architectures, such as convolutional neural networks and transformer models, for processing 3D scene
information. Prior works have shown the benefit of virtual
view rendering over sophisticated point-based methods in various vision tasks, from object recognition [43, 15, 20, 21], object detection [6], to 3D visual grounding [23]. The application
of virtual views in the field of robotics has been less explored.
Recently, RVT [17] leverages multi-view representation for
predicting robot actions for object manipulation. Our work
builds upon RVT using a series of architectural and systemlevel improvements to make it more performant and efficient.
## References

- [RVT-2: Learning Precise Manipulation from Few Demonstrations](https://arxiv.org/pdf/2406.08545)

- https://robotic-view-transformer-2.github.io/.