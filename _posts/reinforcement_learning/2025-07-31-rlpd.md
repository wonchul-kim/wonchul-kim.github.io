---
layout: post
title: RLPD
category: Reinforcement Learning
tag: [rl, rlpd]
---

# [Efficient Online Reinforcement Learning with Offline Data](https://arxiv.org/pdf/2302.02948)

Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. 

can we simply apply existing off-policy methods to leverage offline data when learning online? 
yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. 



## Intro.
One promising way to resolve this issue is via the inclusion of data generated by a previous policy or human expert
when training deep RL algorithms (often referred to as offline data). This can alleviate challenges due to sample efficiency and exploration by providing the algorithm with an initial dataset to “kick-start” the learning process,
either in the form of high-quality expert demonstrations, or even low-quality but high-coverage exploratory trajectories.


Some prior work has focused on using this data through pretraining, while other approaches introduce constraints when
training online to handle issues with distribution shift. 

standard off-policy algorithms should be able to take advantage of this offline data, and furthermore issues with distribution shift should be alleviated in this setting, as we can explore the environment online. T

can we simply apply existing off-policy methods to leverage offline data when learning online, without offline
RL pre-training or explicit imitation terms that privilege the prior offline data?

“symmetric sampling”, that performs well over a large variety of domains with no hyperparameter tuning. Then, we see that in complex settings (e.g., sparse reward, low volume of offline data, high dimensionality, etc.), it is vital that value functions are prevented from over-extrapolation. To this end, we provide a novel perspective on how Layer
Normalization (Ba et al., 2016) implicitly prevents catastrophic value over-extrapolation, thereby greatly improving
sample-efficiency and stability in many scenarios, while being a minimal modification to existing approaches. 

Then, to improve the rate at which the offline data are utilized, we incorporate and compare the latest advances in sample efficient model-free RL, and find that large ensembles are remarkably effective across a variety of domains. 


## Online RL with Offline Data

 
SAC (Haarnoja et al., 2018a;b)

First, we propose a simple mechanism for incorporating the prior data. 

After, we improve the rate the offline data are utilized by incorporating the latest approaches in sample efficient RL.

Finally, we highlight common design choices in recent deep RL that are in fact environment sensitive, and should be adjusted accordingly by practitioners.


### Design Choice 1: A Simple and Efficient Strategy to Incorporate Offline Data

simple approach that incorporates prior data which adds no computational overhead, yet is agnostic to the nature of the offline data. We call this ‘symmetric sampling’, whereby for each batch we sample 50% of the data from our replay buffer, and the remaining 50% from the offline data buffer, resembling the scheme used by Ross & Bagnell (2012). 

However, applying this approach to canonical off-policy methods, such as SAC (Haarnoja et al., 2018a), does not yield strong performance, as we see in Figure 1, and further design choices must be taken into consideration

### Design Choice 2: Layer Normalization Mitigates Catastrophic Overestimation

> Standard off-policy RL algorithms query the learned Qfunction for out-of-distribution (OOD) actions, which might
not be defined during learning. Consequently, there can be significant overestimation of actual values due to the use
of function approximation. In practice, this phenomenon leads to training instabilities and
possible divergence when the critic is trying the catch up with a constantly increasing value.

Critic divergence is a well-studied problem, particularly in the offline regime, where the policy cannot
generate new experience. In our problem setting, however, we can sample from the environment. Therefore, instead
of creating a mechanism that explicitly discourages OOD actions, which can be viewed as anti-exploration (Rezaeifar et al., 2022), we instead need to simply ensure that the learned functions do not extrapolate in an unconstrained
manner. To this end, we show that Layer Normalization (LayerNorm) (Ba et al., 2016) can bound the extrapolation of networks but, crucially, does not explicitly constrain the policy to remain close to the offline data. This in turn
does not discourage the policy from exploring unknown and potentially valuable regions of the state-action space.

In particular, we demonstrate that LayerNorm bounds the values and empirically prevents catastrophic value extrapolation. Concretely, consider a Q-function Q parameterized by $\theta$ , $w$, applying LayerNorm and intermediate representation $\psi_\theta(·, ·)$. For any $a$ and $s$, we consider LayerNorm without bias terms. This does not change the analysis, as it is a constant.:

$$
\| Q_{\theta, w}(s, a) \| = \| w^T \, \mathrm{relu}(\psi_\theta(s, a)) \| \\
\leq \|w\| \, \| \mathrm{relu}(\psi_\theta(s, a)) \| \\
\leq \|w\| \, \| \psi_\theta(s, a) \| \\
\leq \|w\| \\
$$

Therefore, as a result of Layer Normalization, the Q-values are bounded by the norm of the weight layer, even for actions outside the dataset. Thus, the effect of erroneous action extrapolation is greatly mitigated, as their Q-values are unlikely to be significantly greater than those already seen in the data. 

Indeed, referring back to Figure 2, we see that introducing LayerNorm into the critic greatly improves performance through mitigating critic divergence. 

To illustrate this, we generate a dataset with inputs $x$ distributed in a circle with radius 0.5 and labels $y = ∥x∥$. We
study how a standard two-layer MLP with ReLU activations
(common in deep RL) extrapolates outside of the data distribution, and the effect of adding LayerNorm. In Figure 3, the
standard parameterization leads to unbounded extrapolation
outside of the support, while LayerNorm bounds the values, greatly reducing the effect of uncontrolled extrapolation.


### Design Choice 3: Sample Efficient RL

We now have an online approach leveraging offline data that
also suppresses extreme value extrapolation, whilst maintaining the freedom of an unconstrained off-policy method.
However, a benefit of offline and constrained approaches is
that they have an explicit mechanism to efficiently incorporate prior data, such as through pre-training (Hester et al.,
2018; Lee et al., 2021), or an auxiliary supervision term
(Nair et al., 2018a; Rudner et al., 2021) respectively. In
our case, the incorporation of prior data is implicit through
the use of online Bellman backups over offline transitions.
Therefore, it is imperative that these Bellman backups are
performed as sample-efficiently as possible.
One way to achieve this is to increase the number of updates we perform per environment step (also referred to as
update-to-data (UTD) ratio), allowing the offline data to be

## References
 
- [Efficient Online Reinforcement Learning with Offline Data](https://arxiv.org/pdf/2302.02948)