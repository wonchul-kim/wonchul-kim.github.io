---
layout: post
title: RLPD
category: Reinforcement Learning
tag: [rl, rlpd]
---

# [Efficient Online Reinforcement Learning with Offline Data](https://arxiv.org/pdf/2302.02948)

* Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). 

* Inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy, can solve these issues.

* can we simply apply existing off-policy methods to leverage offline data when learning online? yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. 



## 1. Intro.

* The inclusion of data generated by a previous policy or human expert when training deep RL algorithms (often referred to as offline data) can alleviate challenges due to sample efficiency and exploration by providing the algorithm with an initial dataset to “kick-start” the learning process.

* **standard off-policy algorithm**을 사용해서 offline data를 학습하여 효율성을 높이고, distribution shift에 대한 이슈는 online 학습을 통해서 완화하고 있지만, 성능에 제한이 있다. 

* 그래서 본 논문에서는 `can we simply apply existing off-policy methods to leverage offline data when learning online, without offline RL pre-training or explicit imitation terms that privilege the prior offline data?`에 대한 물음과 답을 하고 있다.

> However, na¨ıvely applying existing online off-policy RL algorithms can result in comparatively poor performance, as we see in Figure 1 comparing ‘SAC + Offline Data’ with ‘IQL + Finetuning’

* **symmetric sampling**
    * performs well over a large variety of domains with no hyperparameter tuning
    * in complex settings (e.g., sparse reward, low volume of offline data, high dimensionality, etc.), it is vital that value functions are prevented from over-extrapolation. 
    * To this end, we provide a novel perspective on how Layer Normalization implicitly prevents catastrophic value over-extrapolation, thereby greatly improving sample-efficiency and stability in many scenarios, while being a minimal modification to existing approaches.


* Then, to improve the rate at which the offline data are utilized, we incorporate and compare the latest advances in sample efficient model-free RL, and find that large ensembles are remarkably effective across a variety of domains.

> large ensembles???
> * Q-function(critic) 네트워크를 여러 개(큰 수) 두고 학습하는 구조로 보통 DDPG/SAC에서도 2개의 Q-network를 써서 overestimation을 완화하는데, 여기서 말하는 large는 그 수를 4, 8, 10개 이상으로 늘려서 평균 또는 최솟값을 사용하는 방식
> * 이유: 
>   * Offline RL에서는 데이터 분포 바깥(out-of-distribution) 상태·행동에 대해 Q-value가 과대추정되는 문제가 심각
>   * Q-network를 여러 개 두면, 평균을 취해 추정치를 안정화하거나 최소값을 취해 보수적으로 예측 가능
>   * 이런 방식이 환경 종류에 상관없이 데이터 효율성을 높였다고 보고


* recent RL literature are in fact environment sensitive, showing the surprising result that environments which share similar properties in fact require entirely different choices, and recommend a workflow for practitioners to accelerate their application of our insights to new domains.

> workflow for practitioners???
> 환경에 따라 하이퍼파라미터나 알고리즘 설계 선택을 빠르게 조정하는 절차



## 4. Online RL with Offline Data
 
* SAC (Haarnoja et al., 2018a;b)

### Design Choice 1: A Simple and Efficient Strategy to Incorporate Offline Data

* no computational overhead

* agnostic to the nature of the offline data

* **symmetric sampling**
    * for each batch we sample 50% of the data from our replay buffer, and the remaining 50% from the offline data buffer, resembling the scheme used by Ross & Bagnell (2012). 

    * However, applying this approach to canonical off-policy methods, such as SAC (Haarnoja et al., 2018a), does not yield strong performance, as we see in Figure 1, and further design choices must be taken into consideration

### Design Choice 2: Layer Normalization Mitigates Catastrophic Overestimation

> Standard off-policy RL algorithms query the learned Qfunction for out-of-distribution (OOD) actions, which might
not be defined during learning. Consequently, there can be significant overestimation of actual values due to the use
of function approximation. In practice, this phenomenon leads to training instabilities and possible divergence when the critic is trying the catch up with a constantly increasing value.

Critic divergence is a well-studied problem, particularly in the offline regime, where the policy cannot generate new experience. In our problem setting, however, we can sample from the environment. Therefore, instead of creating a mechanism that explicitly discourages OOD actions, which can be viewed as anti-exploration (Rezaeifar et al., 2022), we instead need to simply ensure that the learned functions do not extrapolate in an unconstrained manner. To this end, we show that Layer Normalization (LayerNorm) (Ba et al., 2016) can bound the extrapolation of networks but, crucially, does not explicitly constrain the policy to remain close to the offline data. This in turn does not discourage the policy from exploring unknown and potentially valuable regions of the state-action space.

In particular, we demonstrate that LayerNorm bounds the values and empirically prevents catastrophic value extrapolation. Concretely, consider a Q-function Q parameterized by $\theta$ , $w$, applying LayerNorm and intermediate representation $\psi_\theta(·, ·)$. For any $a$ and $s$, we consider LayerNorm without bias terms. This does not change the analysis, as it is a constant.:

$$
\| Q_{\theta, w}(s, a) \| = \| w^T \, \mathrm{relu}(\psi_\theta(s, a)) \| \\
\leq \|w\| \, \| \mathrm{relu}(\psi_\theta(s, a)) \| \\
\leq \|w\| \, \| \psi_\theta(s, a) \| \\
\leq \|w\| \\
$$

Therefore, as a result of Layer Normalization, the Q-values are bounded by the norm of the weight layer, even for actions outside the dataset. Thus, the effect of erroneous action extrapolation is greatly mitigated, as their Q-values are unlikely to be significantly greater than those already seen in the data. 

Indeed, referring back to Figure 2, we see that introducing LayerNorm into the critic greatly improves performance through mitigating critic divergence. 

To illustrate this, we generate a dataset with inputs $x$ distributed in a circle with radius 0.5 and labels $y = ∥x∥$. We
study how a standard two-layer MLP with ReLU activations
(common in deep RL) extrapolates outside of the data distribution, and the effect of adding LayerNorm. In Figure 3, the
standard parameterization leads to unbounded extrapolation
outside of the support, while LayerNorm bounds the values, greatly reducing the effect of uncontrolled extrapolation.


### Design Choice 3: Sample Efficient RL

We now have an online approach leveraging offline data that
also suppresses extreme value extrapolation, whilst maintaining the freedom of an unconstrained off-policy method.
However, a benefit of offline and constrained approaches is
that they have an explicit mechanism to efficiently incorporate prior data, such as through pre-training (Hester et al.,
2018; Lee et al., 2021), or an auxiliary supervision term
(Nair et al., 2018a; Rudner et al., 2021) respectively. In
our case, the incorporation of prior data is implicit through
the use of online Bellman backups over offline transitions.
Therefore, it is imperative that these Bellman backups are
performed as sample-efficiently as possible.
One way to achieve this is to increase the number of updates we perform per environment step (also referred to as
update-to-data (UTD) ratio), allowing the offline data to be

## References
 
- [Efficient Online Reinforcement Learning with Offline Data](https://arxiv.org/pdf/2302.02948)