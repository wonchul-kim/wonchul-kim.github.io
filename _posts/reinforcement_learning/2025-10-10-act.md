---
layout: post
title: ACT
category: Reinforcement Learning
tag: [act]
---

# Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware

* present a low-cost system that performs end-to-end imitation learning directly from real demonstrations,
collected with a custom teleoperation interface. 

> Imitation learning, however, presents its own challenges, particularly in highprecision domains: errors in the policy can compound over time, and human demonstrations can be non-stationary. To address these challenges, we develop a simple yet novel algorithm, Action Chunking with Transformers (ACT), which learns a generative model over action sequences. 


## Intro.

* train an end-to-end policy that directly maps RGB images from commodity web cameras to the actions.

    * This pixel-to-action formulation is particularly suitable for fine manipulation, because fine manipulation often involves objects with complex physical properties, such that learning the manipulation policy is much simpler than modeling the whole environment. 
    
    * Designing a model accurate enough for planning would require significant research and task specific engineering efforts. In contrast, the policy of nudging and opening the cup is much simpler, since a closed-loop policy can react to different positions of the cup and lid rather than precisely anticipating how it will move in advance.

    > **Closed-Loop Policy**: 복잡한 물리 현상을 미리 예측하려고 시도하지 않는다. 대신, 실시간으로 카메라 피드백(시각 정보)을 받아들이고 그 정보에 반응한다. 간단하게 말하면, policy는 "이러한 상태(현재 컵의 위치 및 뚜껑의 각도)를 보니, 다음 행동은 이렇게 해야 목표 상태에 도달할 수 있겠군"이라는 조건-행동 매핑만 학습하며, 로봇은 컵이 살짝 움직이면 이를 보고 즉시 오차를 수정하며 궤적을 조정할 수 있다. 즉, 오차를 시스템 내에서 스스로 해결할 수 있게 된다.

        - 만약 로봇이 컵을 밀었는데 예상보다 덜 움직였다면, 물리 모델은 이를 예측하지 못한 실패로 간주하지만, 폐쇄 루프 정책은 "컵의 위치가 아직 목표 지점에 도달하지 않았다"는 시각 정보를 받고 다음 시간 단계에서 조금 더 밀라는 행동을 출력합니다.

        - 즉, 정책(policy)이 '컵의 움직임을 예측하는 물리 모델'의 역할을 상당 부분 대신하며, 이는 정책이 "다른 위치의 컵과 뚜껑에도 반응할 수 있기(can react to different positions of the cup and lid)" 때문에 가능


* Training an end-to-end policy, however, presents its own challenges. The performance of the policy depends heavily on the training data distribution, and in the case of fine manipulation, high-quality human demonstrations can provide tremendous value by allowing the system to learn from human dexterity. 

    * 그렇기 때문에 학습 데이터를 모으기 위해서 인간의 dexterity도 수집할 수 있는 **teleoperation system**을 구축

    * **Imitation learning algorithm**

        * Small errors in the predicted action can incur large differences in the state, exacerbating the “compounding error” problem of imitation learning [47, 64, 29]. 
        
        * To tackle this, we take inspiration from **action chunking**, a concept in psychology that describes how sequences of actions are grouped together as a chunk, and executed as one unit [35]. 
        
        * In our case, the policy predicts the target joint positions for the next k timesteps, rather than just one step at a time. This reduces the effective horizon of the task by k-fold, mitigating compounding errors. Predicting action sequences also helps tackle temporally correlated confounders [61], such as pauses in demonstrations that are hard to model with Markovian single-step policies. 
        
        * To further improve the smoothness of the policy, we propose temporal ensembling, which queries the policy more frequently and averages across the overlapping action chunks. 
        
        * We implement action chunking policy with Transformers [65], an architecture designed for sequence modeling, and train it as a conditional VAE (CVAE) [55, 33] to capture the variability in human data. 


    * 정리하자면, 

        * 모방학습의 한계 극복: ACT 알고리즘은 정밀 조작(precision and visual feedback) 작업에서 기존 모방 학습이 겪는 두 가지 주요 문제를 해결하는 것을 목표로 한다.

            1. 오차 누적 (Compounding Error): 예측된 행동의 작은 오차가 상태에서 큰 차이를 발생시켜 오류가 누적
            2. 시간 상관 교란 요인 (Temporally Correlated Confounders): 사람이 시연할 때 일어나는 '잠시 멈춤(pauses)' 같은 행동을, 현재 상태에만 의존하는 마르코프적인(Markovian) 단일 스텝 정책으로는 모델링하기 어렵움

        * 해결책: 
            1. **action chunking**: 정책은 한 번에 다음 한 단계의 행동($a_t$)만을 예측하는 대신, 미래 $k$ 타임스텝 동안의 목표 관절 위치($a_{t:t+k}$)를 하나의 **덩어리(chunk)**로 묶어 예측

                * 오차 누적 완화: 작업을 완료하는 데 필요한 전체 예측 구간(horizon)을 k배만큼 줄여 오차 누적을 완화
                * 장기 계획: 행동 덩어리 예측은 정책이 단기적인 반응뿐만 아니라 중기적인 목표와 계획을 반영
                * 비마르코프적 행동 처리: 시연 중간의 "멈춤"과 같이 시간적으로 상관관계가 있는 복잡한 행동 패턴을 덩어리 전체로 모델링

            2. **Transformer**와 **Temporal Ensembling**

                * **Transformer**: 행동 시퀀스를 효과적으로 모델링하기 위해 Transformer 아키텍처를 사용
                    
                    * 조건부 변분 자동 인코더 (CVAE): 인간 시연 데이터의 다양성을 포착하기 위해 정책을 **CVAE (Conditional Variational Autoencoder)**로 학습하여, 사람이 같은 작업이라도 여러 가지 방법(예: 배터리를 위에서 넣거나, 옆으로 밀어 넣거나)으로 수행할 수 있는데, CVAE는 이러한 데이터의 **변동성(variability)**을 확률적으로 모델링하여 로봇이 상황에 맞는 다양한 행동을 생성할 수 있게 함

                * **Temporal Ensembling**: 정책의 부드러움(smoothness)을 더욱 개선하기 위한 기법

                    * 정책을 더 자주 호출하여 행동 덩어리를 예측한 다음, 이 겹치는 행동 덩어리들을 **평균(averaging)**하여 최종 실행 행동을 결정

                    * 정책의 노이즈를 줄이고 행동을 더 부드럽고 안정적으로 함




## Related Work

### Imitation learning for robotic manipulation. 

* Behavioral cloning (BC) [44] is one of the simplest imitation learning algorithms, casting imitation as supervised learning from observations to actions. 

### Addressing compounding errors. 

* A major shortcoming of BC is compounding errors, where errors from previous timesteps accumulate and cause the robot to drift off of its training distribution, leading to hard-to-recover states [47, 64]. This
problem is particularly prominent in the fine manipulation setting [29]. 

* One way to mitigate compounding errors is to allow additional on-policy interactions and expert corrections, such as DAgger [47] and its variants [30, 40, 24]. 

* We propose to reduce the effective horizon of tasks through action chunking, i.e., predicting an action sequence instead of a single action, and then ensemble across overlapping action chunks to produce trajectories that are both accurate and smooth. 



##  ACTION CHUNKING WITH TRANSFORMERS 

* We record the joint positions of the leader robots (i.e. input from the human operator) and use
them as actions. It is important to use the leader joint positions instead of the follower’s, because the amount of force applied is implicitly defined by the difference between them, through the low-level PID controller. 

* The observations are composed of the current joint positions of follower robots and the image feed
from 4 cameras. 

* An action corresponds to the target joint positions for both arms in the next time step. 

* At test time, we load the policy that achieves the lowest validation loss and roll it out in the environment. 

#### Action Chunking and Temporal Ensemble
In our implementation, we fix the chunk size to be $k$: every $k$ steps, the agent receives an observation, generates the next $k$ actions, and executes the actions in sequence (Figure 5). This implies a k-fold reduction in the effective horizon of the task. Concretely, the policy models $\pi_\theta(a_{t:t+k}|s_t)$ instead of $\pi_\theta(a_t|s_t)$. 

Chunking can also help model non-Markovian behavior in human demonstrations. Specifically, a single-step policy would struggle with temporally correlated confounders, such as pauses in the middle of a
demonstration [61], since the behavior not only depends on the state, but also the timestep. Action chunking can mitigate this issue when the confounder is within a chunk, without introducing the causal confusion issue for history-conditioned policies [12].

A naïve implementation of action chunking can be suboptimal: a new environment observation is incorporated
abruptly every $k$ steps and can result in jerky robot motion. To improve smoothness and avoid discrete switching between executing and observing, we query the policy at every timestep. This makes different action chunks overlap with each other, and at a given timestep there will be more than one predicted action. We illustrate this in Figure 5 and propose a temporal ensemble to combine these predictions. Our temporal ensemble performs a weighted average over these predictions with an exponential weighting scheme $w_i = exp(−m ∗ i)$, where $w_0$ is the weight for the oldest action. The speed for incorporating
new observation is governed by $m$, where a smaller $m$ means faster incorporation. We note that unlike typical smoothing, where the current action is aggregated with actions in adjacent timesteps, which leads to bias, we aggregate actions predicted for the same timestep. This procedure also incurs no additional
training cost, only extra inference-time computation. 

* In practice, we find both action chunking and temporal ensembling to be important for the success of ACT, which produces precise and smooth motion.


#### Modeling human data
Another challenge that arises is learning from noisy human demonstrations. Given the same observation, a human can use different trajectories to solve the task. Humans will also be more stochastic in regions where precision matters less [38].
Thus, it is important for the policy to focus on regions where high precision matters. We tackle this problem by training our action chunking policy as a generative model. Specifically, we train the policy as a conditional variational autoencoder (CVAE) [55], to generate an action sequence conditioned on current
observations. 

The CVAE has two components: a CVAE encoder and a CVAE decoder, illustrated on the left and right side of Figure 4 respectively. The CVAE encoder only serves to train the CVAE decoder (the policy) and is discarded at test time. Specifically, the CVAE encoder predicts the mean and variance of the style variable z’s distribution, which is parameterized as a diagonal Gaussian, given the current observation and action sequence as inputs. For faster training in practice, we leave out the image observations and only condition on the proprioceptive observation and the action sequence. The CVAE decoder, i.e. the policy, conditions on both z and the current observations (images + joint positions) to predict the action sequence. At test time, we set z to be the mean of the prior distribution i.e. zero to deterministically decode. The whole model is trained to maximize the log-likelihood of demonstration action chunks, i.e.
$min_\theta − \sum_{s_t,a_{t:t+k}\in D} log \pi_\theta(a_{t:t+k}|s_t)$, with the standard VAE objective which has two terms: a reconstruction loss and a term that regularizes the encoder to a Gaussian prior. Following [23], we weight the second term with a hyperparameter $\beta$. Intuitively, higher $\beta$ will result in less information transmitted in $z$ [62]. Overall, we found the CVAE objective to be essential in
learning precise tasks from human demonstrations. 

#### Implementing ACT

The CVAE encoder is implemented with a BERT-like transformer encoder [13]. The inputs to the encoder are the current joint positions and the target action sequence of length $k$ from the demonstration dataset, prepended by a learned “[CLS]” token similar to BERT. This forms a $k+2$ length input (Figure 4 left). After passing through the transformer, the feature corresponding to “[CLS]” is used to predict the mean and variance of the “style variable” $z$, which is then used as input to the decoder. 


The CVAE decoder (i.e. the policy) takes the current observations and $z$ as the input, and predicts the next $k$ actions (Figure 4 right). We use ResNet image encoders, a transformer encoder, and a transformer decoder to implement the CVAE decoder. 

Intuitively, the transformer encoder synthesizes information from different camera viewpoints, the joint positions, and the style variable, and the transformer decoder generates a coherent action sequence. The observation includes 4 RGB images, each at 480 × 640 resolution, and joint positions for two robot arms
(7+7=14 DoF in total). ***The action space is the absolute joint positions*** for two robots, a 14-dimensional vector. Thus with action chunking, the policy outputs a $k$ × 14 tensor given the current observation. The policy first process the images with ResNet18 backbones [22], which convert 480 × 640 × 3 RGB images into 15 × 20 × 512 feature maps. We then flatten along the spatial dimension to obtain a sequence of 300 × 512.

To preserve the spatial information, we add a 2D sinusoidal position embedding to the feature sequence [8]. Repeating this for all 4 images gives a feature sequence of 1200 × 512 in dimension. We then append two more features: the current joint positions and the “style variable” z. They are projected from their original dimensions to 512 through linear layers respectively. Thus, the input to the transformer encoder is 1202×512. The transformer decoder conditions on the encoder
output through cross-attention, where the input sequence is a fixed position embedding, with dimensions k × 512, and the keys and values are coming from the encoder. This gives the transformer decoder an output dimension of k × 512, which is then down-projected with an MLP into k × 14, corresponding to the predicted target joint positions for the next k steps. We use L1 loss for reconstruction instead of the more common
L2 loss: we noted that L1 loss leads to more precise modeling of the action sequence. We also noted degraded performance when using delta joint positions as actions instead of target joint positions. We include a detailed architecture diagram in Appendix C.

The model has around 80M parameters, and we train from scratch for each task. The training takes
around 5 hours on a single 11G RTX 2080 Ti GPU, and the inference time is around 0.01 seconds on the same machine.