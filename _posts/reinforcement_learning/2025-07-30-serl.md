---
layout: post
title: SERL - A Software Suite for Sample-Efficient Robotic Reinforcement Learning
category: Reinforcement Learning
tag: [rl, serl]
---

# [SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning](https://arxiv.org/pdf/2401.16013)



* library containing a sample efficient off-policy deep RL method, together with methods for computing rewards and resetting the environment, a high-quality controller for a widely-adopted robot, and a number of challenging example tasks

## 4. Sample Efficient Robotic Reinforcement Learning in the Real-World

#### Core reinforcement learning algorithm: [RLPD](https://arxiv.org/pdf/2302.02948)

(1) it must be efficient and able to make multiple gradient updates per time step, 
(2) it must be able to incorporate prior data easily and then continue improving with further experience
(3) it must be simple to debug and build on for new users. 

* sample-efficient robotic learning
* RLPD is an off-policy actor-critic reinforcement learning algorithm that builds on the success of temporal difference algorithms such as soft-actor critic 

* three key changes: 
    1. high update-to-data ratio training (UTD)
    2. symmetric sampling between prior data and on-policy data, such that half of each batch comes from prior data and half from the online replay buffer
    3. layer-norm regularization during training. 
    

$$
\mathcal{L}_Q(\phi) = \mathbb{E}_{s, a, s'} \left[ \left( Q_\phi(s, a) - \left( r(s, a) + \gamma \mathbb{E}_{a' \sim \pi_\theta} \left[ Q_\phi(s', a') \right] \right) \right)^2 \right] 
$$
$$
\mathcal{L}_\pi(\theta) = -\mathbb{E}_s \left[ \mathbb{E}_{a \sim \pi_\theta(a)} [Q_\phi(s, a)] + \alpha\mathcal{H}(\pi_\theta(\cdot|s)) \right]
$$


* the actor loss uses entropy regularization with an adaptively adjusted weight $/alpha&

* Each update step uses a sample-based approximation of each expectation, with half of the samples drawn from the prior data (e.g., demonstrations), and half drawn from the replay buffer

* For efficient learning, multiple update steps are performed per time step in the environment, which is referred to as the updateto-date (UTD) ratio, and regularizing the critic with layer normalization allows for higher UTD ratios and thus more efficient training


#### Reward Specification with Classifiers: [VICE](https://arxiv.org/pdf/1805.11686)

the reward function can be provided by a binary classifier that takes in the state observation ùê¨
and outputs the probability of a binary ‚Äúevent‚Äù ùëí, corresponding to successful completion. The reward is then given by $r(\mathbb{s}) = log\mathcal{p}(e|\mathbb{s})$


This classifier can be trained either using handspecified positive and negative examples, or via an adversarial method called VICE (Fu et al., 2018). The latter addresses a reward exploitation problem that can arise when learning with classifier based rewards, and removes the need for negative examples in the classifier training set: when the RL algorithm optimizes the reward $r(\mathbb{s}) = log\mathcal{p}(e|\mathbb{s})$, it can potentially discover
‚Äúadversarial‚Äù states that fool the classifier $\mathcal{p}(e|\mathbb{s})$ to erroneously output high probabilities. VICE addresses this issue by adding all states visited by the policy into the
training set for the classifier with negative labels, and
updating the classifier after each iteration. In this way,
the RL process is analogous to a generative adversarial network (GAN) (Goodfellow et al., 2014), with the
policy acting as the generator and the reward classifier acting as the discriminator. Our framework thus
supports all three types of rewards.

#### Resets: forward-backward architecture


## References

- [SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning](https://arxiv.org/pdf/2401.16013)

- [Efficient Online Reinforcement Learning with Offline Data](https://arxiv.org/pdf/2302.02948)

- [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290)

- [Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition](https://arxiv.org/pdf/1805.11686)


- [Autonomous Reinforcement Learning: Formalism and Benchmarking](https://arxiv.org/abs/2112.09605)

- [Learning compound multi-step controllers under unknown dynamics](https://rll.berkeley.edu/reset_controller/reset_controller.pdf)

- [Reset-free reinforcement learning via multi-task learning: Learning dexterous manipulation behaviors without human intervention](https://arxiv.org/abs/2104.11203)