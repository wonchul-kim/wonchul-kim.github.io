---
layout: post
title: DDPG (Deep Deterministic Polcy Gradient)
category: Reinforcement Learning
tag: [rl, ddpg]
---

# [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971)



------------------------------------------------------------------------------
## 1. Intro.

* **Deep Q-learning (DQN)**은 `discrete` and `low-dimensional action spaces`에만 적용가능

* 대부분의 physical control tasks는 `continuous` and `high dimensional action spaces` 요구

* `model-free and off-policy actor-ciritic algorithm` for `high dimensional and continuous` action spaces


* from **DQN**, 
    * previously, non-linear function approximators was difficult and unstable. 
    * DQN is able to learn value functions using such function approximators in a stable and robust way

        1. the network is trained off-policy with samples from a replay buffer to minimize correlations between samples
        
        2. the network is trained with a target Q network to give consistent targets during temporal difference backups. 
        
    * batch normalization (Ioffe & Szegedy, 2015)


------------------------------------------------------------------------------
## 2. Background

* In general, the environment may be partially observed so that the entire history of the observation, action pairs $s_t = (x_1, a_1, ..., a_{t−1}, x_t)$ may be required to describe the state. Here, we assumed the environment is fully-observed so $s_t = x_t$.

* action is defined by a policy $\pi$, $\pi : \mathcal{S} \to \mathcal{P}(\mathcal{A})$

* Markov Decision Process with
    * state space: $\mathcal{S}$
    * action space: $\mathcal{S} \in \mathbb{R}^N$
    * inital state distribution: $p(s_1)$
    * transition dynamics: $p(s_{t+1}|s_t, a_t)$
    * reward function: $r(s_t, a_t)$

* The return from a state is defined as the sum of discounted future reward, $[R_t = \sum_{i=t}^{T} \gamma^{(i-t)} r(s_i, a_i)]$ with a discounting factor $\gamma \in [0, 1]$.
    * the return depends on the actions chosen, and therefore on the policy $\pi$, and may be stochastic.

* The goal in reinforcement learning is to learn a policy which maximizes the expected return from the start distribution, $J = \mathbb{E}_{r_i, s_i \sim E, a_i \sim \pi} [R_1]$.
    * discounted state visitation distribution for a policy $\pi$ as $\rho^\pi$.

* The action-value function describes the expected return after taking an action $a_t$ in state $s_t$ and thereafter following policy $\pi$:
$$Q^\pi(s_t, a_t) = \mathbb{E}_{r_i \ge t,\, s_{i>t} \sim E,\, a_{i>t} \sim \pi} \left[ R_t \mid s_t, a_t \right]$$


* Many approaches in reinforcement learning make use of the recursive relationship known as the Bellman equation:
$$
Q^\pi(s_t, a_t) = \mathbb{E}_{r_t, s_{t+1} \sim E} \left[ r(s_t, a_t) + \gamma \mathbb{E}_{a_{t+1} \sim \pi} \left[ Q^\pi(s_{t+1}, a_{t+1}) \right] \right] 
$$

* If the target policy is deterministic we can describe it as a function $\mu : S \leftarrow \mathcal{A}$ and avoid the inner expectation:
$$
Q^\mu(s_t, a_t) = \mathbb{E}_{r_t, s_{t+1} \sim E} \left[ r(s_t, a_t) + \gamma Q^\mu(s_{t+1}, \mu(s_{t+1})) \right] \tag{3}
$$

* The expectation depends only on the environment. This means that it is possible to learn $Q^\mu$ off-policy, using transitions which are generated from a different stochastic behavior policy $\beta$.

* **Q-learning (Watkins \& Dayan, 1992)**, a commonly used off-policy algorithm, uses the greedy policy, $\mu(s) = \arg\max_a Q(s, a).$ We consider function approximators parameterized by $\theta^Q$, which we optimize by minimizing the loss:
$$
L(\theta^Q) = \mathbb{E}_{s_t \sim \rho^\beta,\, a_t \sim \beta,\, r_t \sim E} \left[ \left( Q(s_t, a_t | \theta^Q) - y_t \right)^2 \right]
$$
where
$$
y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, \mu(s_{t+1}) | \theta^Q). \tag{5}
$$
While $y_t$ is also dependent on $\theta^Q$, this is typically ignored.


* The use of large, non-linear function approximators for learning value or action-value functions does not guarantee the performance, and practically learning tends to be unstable. 

* In order to scale **Q-learning**, two major changes: 
    * the use of a replay buffer
    * a separate target network for calculating yt. 
    

------------------------------------------------------------------------------
## 3. Algorithm


> It is not possible to straightforwardly apply Q-learning to continuous action spaces, because in continuous spaces finding the greedy policy requires an optimization of at at every timestep; this optimization is too slow to be practical with large, unconstrained function approximators and nontrivial action spaces. Instead, here we used an actor-critic approach based on the DPG algorithm (Silver et al., 2014). 

* DPG algorithm maintains a parameterized actor function $\mu(s|\theta^\mu)$ which specifies the current policy by deterministically mapping states to a specific action. 
* The critic $Q(s, a)$ is learned using the Bellman equation as in Q-learning. 
* The actor is updated by following the applying the chain rule to the expected return from the start distribution $J$ with respect to the actor parameters:

$$\nabla_{\theta^\mu} J \approx \mathbb{E}_{s_t \sim \rho^\beta} \left[ \nabla_{\theta^\mu} Q(s, a \mid \theta^Q) \Big|_{s = s_t,\, a = \mu(s_t \mid \theta_\mu)} \right] \\
= \mathbb{E}_{s_t \sim \rho^\beta} \left[ \nabla_a Q(s, a \mid \theta^Q) \Big|_{s = s_t,\, a = \mu(s_t)} \nabla_{\theta^\mu} \mu(s \mid \theta_\mu) \Big|_{s = s_t} \right]
$$


> As with Q learning, introducing non-linear function approximators means that convergence is no longer guaranteed. However, such approximators appear essential in order to learn and generalize on large state spaces. NFQCA (Hafner & Riedmiller, 2011), which uses the same update rules as DPG but with neural network function approximators, uses batch learning for stability, which is intractable for large networks. A minibatch version of NFQCA which does not reset the policy at each update, as would be required to scale to large networks, is equivalent to the original DPG, which we compare to here. Our contribution here is to provide modifications to DPG, inspired by
the success of DQN, which allow it to use neural network function approximators to learn in large state and action spaces online. We refer to our algorithm as Deep DPG (DDPG, Algorithm 1)

> One challenge when using neural networks for reinforcement learning is that most optimization algorithms assume that the samples are independently and identically distributed. Obviously, when the samples are generated from exploring sequentially in an environment this assumption no longer holds. Additionally, to make efficient use of hardware optimizations, it is essential to learn in minibatches, rather than online.

As in **DQN**, we used a replay buffer to address these issues. The replay buffer is:

* a finite sized cache $R$. 
* Transitions, $(s_t, a_t, r_t, s_{t+1})$, were sampled from the environment according to the exploration policy
* When the replay buffer was full the oldest samples were discarded
* At each timestep the actor and critic are updated by sampling a minibatch uniformly from the buffer. Because DDPG is an off-policy algorithm, the replay buffer can be large, allowing the algorithm to benefit from learning across a set of uncorrelated transitions.






* When learning from low dimensional feature vector observations, the different components of the observation may have different physical units (for example, positions versus velocities) and the ranges may vary across environments. This can make it difficult for the network to learn effectively and may make it difficult to find hyper-parameters which generalise across environments with different scales of state values.

* One approach to this problem is to manually scale the features so they are in similar ranges across environments and units. We address this issue by adapting a recent technique from deep learning called batch normalization (Ioffe & Szegedy, 2015). This technique normalizes each dimension across the samples in a minibatch to have unit mean and variance. In addition, it maintains a running average of the mean and variance to use for normalization during testing (in our case, during exploration or evaluation). In deep networks, it is used to minimize covariance shift during training, by ensuring that each layer receives whitened input. In the low-dimensional case, we used batch normalization on the state input and all layers of the µ network and all layers of the Q network prior to the action input (details of the networks are given in the supplementary material). With batch normalization, we were able to learn effectively across many different tasks with differing types of units, without needing to manually ensure the units were within a set range.



<img src='/assets/reinforcement_learning/ddpg/ddpg.png'>

------------------------------------------------------------------------------
* `off-policy`: 현재의 `policy`와 다른 `policy`로 수집한 데이터를 학습에 사용

    * **DQN**, **DDPG**, **TD3**, **SAC&**
    * 환경과 상호작용하지 않고도 학습 가능
    * 불일치한 `policy` 데이터로 인해 학습이 불안정
        * **importance sampling**이나 다른 안정화 기법이 요구됨


    * e.g
        * **On-policy (PPO)**:
            1. 지금 policy로 10번 시도
            2. 그 경험으로만 학습
            3. 학습 후 policy가 바뀌면, 이전 데이터는 폐기

        * **Off-policy (DQN)**:
            1. 무작위 정책이든 과거 정책이든, 경험을 다 저장
            2. replay buffer에서 샘플 뽑아 학습
            3. 수개월 전 정책의 데이터도 재사용 가능


    > `on-policy`는 현재의 `policy`에서 생성한 데이터를 학습에 사용하는 것으로 **PPO**, **REINFORCE**, 등의 알고리즘이 존재
    >  수렴이 안정적이지만, 경험 재사용이 불가능하여 샘플 효율이 낮고, 데이터 수집 비용이 크다. 


* `actor-critic`

    | 이름              | 역할            | 설명                                                   |
    | --------------- | ------------- | ---------------------------------------------------- |
    | `Actor`         | policy        | 상태 → 행동을 결정하는 deterministic 함수: 𝜇(s; θ|μ) |
    | `Critic`        | Q-function    | 상태-행동 쌍의 Q값을 예측: Q(s, a; θ|Q)              |
    | `Target Actor`  | target policy | 느리게 업데이트되는 actor 복사본                                 |
    | `Target Critic` | target Q      | 느리게 업데이트되는 critic 복사본                                |

* pseudo code
    ```
    1. Initialize: actor, critic, target networks, replay buffer
    2. For each step:
        a. 행동 선택: a = μ(s) + exploration_noise
        b. 환경으로부터 transition 수집: (s, a, r, s′)
        c. replay buffer에 저장
        d. 배치 샘플링 후 critic 업데이트
        e. actor 업데이트 (∇θ_μ J ≈ ∇_a Q(s, a) ∇θ_μ μ(s))
        f. target network soft update
    ```

    ```txt
              s_t
           │
        ┌──▼────┐
        │ Actor │ ─────────────┐
        └───────┘             │
              a_t           (s_t, a_t)
                               │
                          ┌────▼────┐
                          │ Critic  │────→ Q(s_t, a_t)
                          └─────────┘
    ```


#### 장점

| 항목                   | 내용                          |
| -------------------- | --------------------------- |
| ✔ 연속 액션 처리           | 로봇, 제어, 드론 등에 적합            |
| ✔ off-policy         | replay buffer 사용 → 샘플 효율 높음 |
| ✔ policy gradient 방식 | 안정적인 수렴 가능                  |

#### 단점

| 항목                      | 내용                                         |
| ----------------------- | ------------------------------------------ |
| ✘ exploration 어려움       | deterministic policy → 랜덤성 부족 (→ noise 필요) |
| ✘ critic overestimation | Q값이 발산할 가능성 (→ TD3에서 개선됨)                  |
| ✘ hyperparameter 민감     | 학습률, τ, noise scale 등 튜닝 필요                |

## Target Network
$$\theta_{\text{targ}} \leftarrow \rho\,\theta_{\text{targ}} + (1-\rho)\,\theta,$$ 

여기서 $\rho\in[0,1)$는 보통 0.99~0.999 수준으로 설정된다. 이 업데이트는 **Polyak averaging**(soft update) 방식으로, 타겟 네트워크가 주 네트워크를 천천히 따라오도록 한다:contentReference[oaicite:15]{index=15}.


Q-러닝 기반 알고리즘에서는 벨만 타겟 $y = r + \gamma \max_{a'}Q(s',a')$가 현재 학습 중인 Q-함수 자체에 의존하기 때문에, 그 값을 바로 사용하면 학습이 불안정해진다. 즉, 네트워크를 업데이트할 때 타겟값도 동시에 변하면 학습이 발산하거나 수렴하지 않을 수 있다. 타겟 네트워크는 이러한 문제를 해결하기 위해 일정 기간 동안 일정한(혹은 천천히 변하는) 파라미터로 타겟을 제공한다. 예를 들어, DQN에서는 일정 단계마다 네트워크를 복사해 사용하고, DDPG에서는 타겟 파라미터를 부드럽게 동기화한다. 이렇게 하면 “사고 잊음(catastrophic forgetting)” 현상을 줄여 안정적인 학습을 도모할 수 있다.


예시: 실제로 타겟 네트워크를 사용하지 않을 경우, 한 단계에서 약간의 업데이트가 다음 상태의 Q-예측을 크게 흔들어 엉뚱한 행동을 강화할 수 있다. 반면 타겟 네트워크는 “현재까지 잘 학습된 정책”을 일시적으로 고정하여, 네트워크가 충분히 많은 경험을 고려할 수 있는 시간을 벌어준다. 결과적으로 타겟값이 안정되어 손실 함수가 수렴하기 쉬워진다.


#### Bellman Optimality Equation

최적의 행동가치함수 $Q^*(s,a)$는 벨만 최적 방정식을 만족한다

여기서 $\mathbb{E}$는 환경의 상태전이 분포에 대한 기댓값이다. 이 방정식은 “현재 상태-행동의 가치 = 즉시 보상 + 할인된 미래 최대 가치”를 의미하며, Q-러닝 기반 방법의 이론적 근간이 된다.


DDPG에서의 응용: DDPG의 크리틱은 이 벨만식을 근사하도록 학습된다. 크리틱 네트워크 $Q_\phi(s,a)$는 타깃 벨만값 $y=r+\gamma Q_{\phi_{\text{targ}}}(s',\mu_{\theta_{\text{targ}}}(s'))$에 점차 일치하도록 파라미터를 조정한다. 구체적으로, 평균제곱 벨만오차(MSBE) 손실을 최소화한다
이 수식에서 RHS는 벨만 방정식에 기반한 TD 타깃이고, 이를 최소화하면 $Q_\phi$가 벨만 방정식을 만족하도록 학습된다. 즉, DDPG의 크리틱 업데이트는 Q-러닝과 동일하게 현재 가치와 일스텝 후의 예측 가치를 일치시키는 방식으로 동작한다.



## TIPs

1. Exploration: Gaussian noise 또는 Ornstein-Uhlenbeck noise 사용
    ```python
    action = actor(state) + noise
    ```

2. Target update τ는 작게 (~0.005)

3. Replay buffer는 충분히 커야 (1e6 이상)

4. Normalization: state/action scaling 중요

5. Batch size: 일반적으로 64~256

## TODO 
| 알고리즘                 | 개선점                                                         |
| -------------------- | ----------------------------------------------------------- |
| **TD3**              | DDPG + twin critics + target smoothing → overestimation 완화  |
| **SAC**              | stochastic policy + entropy regularization → exploration 향상 |
| **BCQ / AWAC / IQL** | off-policy 기반 offline RL 개선                                 |



## References



