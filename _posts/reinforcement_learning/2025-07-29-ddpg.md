---
layout: post
title: DDPG (Deep Deterministic Polcy Gradient)
category: Reinforcement Learning
tag: [rl, ddpg]
---

# [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971)



------------------------------------------------------------------------------
## 1. Intro.

* **Deep Q-learning (DQN)**ì€ `discrete` and `low-dimensional action spaces`ì—ë§Œ ì ìš©ê°€ëŠ¥

* ëŒ€ë¶€ë¶„ì˜ physical control tasksëŠ” `continuous` and `high dimensional action spaces` ìš”êµ¬

* `model-free and off-policy actor-ciritic algorithm` for `high dimensional and continuous` action spaces


* from **DQN**, 
    * previously, non-linear function approximators was difficult and unstable. 
    * DQN is able to learn value functions using such function approximators in a stable and robust way

        1. the network is trained off-policy with samples from a replay buffer to minimize correlations between samples
        
        2. the network is trained with a target Q network to give consistent targets during temporal difference backups. 
        
    * batch normalization (Ioffe & Szegedy, 2015)


------------------------------------------------------------------------------
## 2. Background

* In general, the environment may be partially observed so that the entire history of the observation, action pairs $s_t = (x_1, a_1, ..., a_{tâˆ’1}, x_t)$ may be required to describe the state. Here, we assumed the environment is fully-observed so $s_t = x_t$.

* action is defined by a policy $\pi$, $\pi : \mathcal{S} \to \mathcal{P}(\mathcal{A})$

* Markov Decision Process with
    * state space: $\mathcal{S}$
    * action space: $\mathcal{S} \in \mathbb{R}^N$
    * inital state distribution: $p(s_1)$
    * transition dynamics: $p(s_{t+1}|s_t, a_t)$
    * reward function: $r(s_t, a_t)$

* The return from a state is defined as the sum of discounted future reward, $[R_t = \sum_{i=t}^{T} \gamma^{(i-t)} r(s_i, a_i)]$ with a discounting factor $\gamma \in [0, 1]$.
    * the return depends on the actions chosen, and therefore on the policy $\pi$, and may be stochastic.

* The goal in reinforcement learning is to learn a policy which maximizes the expected return from the start distribution, $J = \mathbb{E}_{r_i, s_i \sim E, a_i \sim \pi} [R_1]$.
    * discounted state visitation distribution for a policy $\pi$ as $\rho^\pi$.

* The action-value function describes the expected return after taking an action $a_t$ in state $s_t$ and thereafter following policy $\pi$:
$$Q^\pi(s_t, a_t) = \mathbb{E}_{r_i \ge t,\, s_{i>t} \sim E,\, a_{i>t} \sim \pi} \left[ R_t \mid s_t, a_t \right]$$


* Many approaches in reinforcement learning make use of the recursive relationship known as the Bellman equation:
$$
Q^\pi(s_t, a_t) = \mathbb{E}_{r_t, s_{t+1} \sim E} \left[ r(s_t, a_t) + \gamma \mathbb{E}_{a_{t+1} \sim \pi} \left[ Q^\pi(s_{t+1}, a_{t+1}) \right] \right] 
$$

* If the target policy is deterministic we can describe it as a function $\mu : S \leftarrow \mathcal{A}$ and avoid the inner expectation:
$$
Q^\mu(s_t, a_t) = \mathbb{E}_{r_t, s_{t+1} \sim E} \left[ r(s_t, a_t) + \gamma Q^\mu(s_{t+1}, \mu(s_{t+1})) \right] \tag{3}
$$

* The expectation depends only on the environment. This means that it is possible to learn $Q^\mu$ off-policy, using transitions which are generated from a different stochastic behavior policy $\beta$.

* **Q-learning (Watkins \& Dayan, 1992)**, a commonly used off-policy algorithm, uses the greedy policy, $\mu(s) = \arg\max_a Q(s, a).$ We consider function approximators parameterized by $\theta^Q$, which we optimize by minimizing the loss:
$$
L(\theta^Q) = \mathbb{E}_{s_t \sim \rho^\beta,\, a_t \sim \beta,\, r_t \sim E} \left[ \left( Q(s_t, a_t | \theta^Q) - y_t \right)^2 \right]
$$
where
$$
y_t = r(s_t, a_t) + \gamma Q(s_{t+1}, \mu(s_{t+1}) | \theta^Q). \tag{5}
$$
While $y_t$ is also dependent on $\theta^Q$, this is typically ignored.


* The use of large, non-linear function approximators for learning value or action-value functions does not guarantee the performance, and practically learning tends to be unstable. 

* In order to scale **Q-learning**, two major changes: 
    * the use of a replay buffer
    * a separate target network for calculating yt. 
    

------------------------------------------------------------------------------
## 3. Algorithm


> It is not possible to straightforwardly apply Q-learning to continuous action spaces, because in continuous spaces finding the greedy policy requires an optimization of at at every timestep; this optimization is too slow to be practical with large, unconstrained function approximators and nontrivial action spaces. Instead, here we used an actor-critic approach based on the DPG algorithm (Silver et al., 2014). 

* DPG algorithm maintains a parameterized actor function $\mu(s|\theta^\mu)$ which specifies the current policy by deterministically mapping states to a specific action. 
* The critic $Q(s, a)$ is learned using the Bellman equation as in Q-learning. 
* The actor is updated by following the applying the chain rule to the expected return from the start distribution $J$ with respect to the actor parameters:

$$\nabla_{\theta^\mu} J \approx \mathbb{E}_{s_t \sim \rho^\beta} \left[ \nabla_{\theta^\mu} Q(s, a \mid \theta^Q) \Big|_{s = s_t,\, a = \mu(s_t \mid \theta_\mu)} \right] \\
= \mathbb{E}_{s_t \sim \rho^\beta} \left[ \nabla_a Q(s, a \mid \theta^Q) \Big|_{s = s_t,\, a = \mu(s_t)} \nabla_{\theta^\mu} \mu(s \mid \theta_\mu) \Big|_{s = s_t} \right]
$$


> As with Q learning, introducing non-linear function approximators means that convergence is no longer guaranteed. However, such approximators appear essential in order to learn and generalize on large state spaces. NFQCA (Hafner & Riedmiller, 2011), which uses the same update rules as DPG but with neural network function approximators, uses batch learning for stability, which is intractable for large networks. A minibatch version of NFQCA which does not reset the policy at each update, as would be required to scale to large networks, is equivalent to the original DPG, which we compare to here. Our contribution here is to provide modifications to DPG, inspired by
the success of DQN, which allow it to use neural network function approximators to learn in large state and action spaces online. We refer to our algorithm as Deep DPG (DDPG, Algorithm 1)

> One challenge when using neural networks for reinforcement learning is that most optimization algorithms assume that the samples are independently and identically distributed. Obviously, when the samples are generated from exploring sequentially in an environment this assumption no longer holds. Additionally, to make efficient use of hardware optimizations, it is essential to learn in minibatches, rather than online.

As in **DQN**, we used a replay buffer to address these issues. The replay buffer is:

* a finite sized cache $R$. 
* Transitions, $(s_t, a_t, r_t, s_{t+1})$, were sampled from the environment according to the exploration policy
* When the replay buffer was full the oldest samples were discarded
* At each timestep the actor and critic are updated by sampling a minibatch uniformly from the buffer. Because DDPG is an off-policy algorithm, the replay buffer can be large, allowing the algorithm to benefit from learning across a set of uncorrelated transitions.






* When learning from low dimensional feature vector observations, the different components of the observation may have different physical units (for example, positions versus velocities) and the ranges may vary across environments. This can make it difficult for the network to learn effectively and may make it difficult to find hyper-parameters which generalise across environments with different scales of state values.

* One approach to this problem is to manually scale the features so they are in similar ranges across environments and units. We address this issue by adapting a recent technique from deep learning called batch normalization (Ioffe & Szegedy, 2015). This technique normalizes each dimension across the samples in a minibatch to have unit mean and variance. In addition, it maintains a running average of the mean and variance to use for normalization during testing (in our case, during exploration or evaluation). In deep networks, it is used to minimize covariance shift during training, by ensuring that each layer receives whitened input. In the low-dimensional case, we used batch normalization on the state input and all layers of the Âµ network and all layers of the Q network prior to the action input (details of the networks are given in the supplementary material). With batch normalization, we were able to learn effectively across many different tasks with differing types of units, without needing to manually ensure the units were within a set range.



<img src='/assets/reinforcement_learning/ddpg/ddpg.png'>

------------------------------------------------------------------------------
* `off-policy`: í˜„ì¬ì˜ `policy`ì™€ ë‹¤ë¥¸ `policy`ë¡œ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš©

    * **DQN**, **DDPG**, **TD3**, **SAC&**
    * í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ì§€ ì•Šê³ ë„ í•™ìŠµ ê°€ëŠ¥
    * ë¶ˆì¼ì¹˜í•œ `policy` ë°ì´í„°ë¡œ ì¸í•´ í•™ìŠµì´ ë¶ˆì•ˆì •
        * **importance sampling**ì´ë‚˜ ë‹¤ë¥¸ ì•ˆì •í™” ê¸°ë²•ì´ ìš”êµ¬ë¨


    * e.g
        * **On-policy (PPO)**:
            1. ì§€ê¸ˆ policyë¡œ 10ë²ˆ ì‹œë„
            2. ê·¸ ê²½í—˜ìœ¼ë¡œë§Œ í•™ìŠµ
            3. í•™ìŠµ í›„ policyê°€ ë°”ë€Œë©´, ì´ì „ ë°ì´í„°ëŠ” íê¸°

        * **Off-policy (DQN)**:
            1. ë¬´ì‘ìœ„ ì •ì±…ì´ë“  ê³¼ê±° ì •ì±…ì´ë“ , ê²½í—˜ì„ ë‹¤ ì €ì¥
            2. replay bufferì—ì„œ ìƒ˜í”Œ ë½‘ì•„ í•™ìŠµ
            3. ìˆ˜ê°œì›” ì „ ì •ì±…ì˜ ë°ì´í„°ë„ ì¬ì‚¬ìš© ê°€ëŠ¥


    > `on-policy`ëŠ” í˜„ì¬ì˜ `policy`ì—ì„œ ìƒì„±í•œ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ **PPO**, **REINFORCE**, ë“±ì˜ ì•Œê³ ë¦¬ì¦˜ì´ ì¡´ì¬
    >  ìˆ˜ë ´ì´ ì•ˆì •ì ì´ì§€ë§Œ, ê²½í—˜ ì¬ì‚¬ìš©ì´ ë¶ˆê°€ëŠ¥í•˜ì—¬ ìƒ˜í”Œ íš¨ìœ¨ì´ ë‚®ê³ , ë°ì´í„° ìˆ˜ì§‘ ë¹„ìš©ì´ í¬ë‹¤. 


* `actor-critic`

    | ì´ë¦„              | ì—­í•             | ì„¤ëª…                                                   |
    | --------------- | ------------- | ---------------------------------------------------- |
    | `Actor`         | policy        | ìƒíƒœ â†’ í–‰ë™ì„ ê²°ì •í•˜ëŠ” deterministic í•¨ìˆ˜: ğœ‡(s; Î¸|Î¼) |
    | `Critic`        | Q-function    | ìƒíƒœ-í–‰ë™ ìŒì˜ Qê°’ì„ ì˜ˆì¸¡: Q(s, a; Î¸|Q)              |
    | `Target Actor`  | target policy | ëŠë¦¬ê²Œ ì—…ë°ì´íŠ¸ë˜ëŠ” actor ë³µì‚¬ë³¸                                 |
    | `Target Critic` | target Q      | ëŠë¦¬ê²Œ ì—…ë°ì´íŠ¸ë˜ëŠ” critic ë³µì‚¬ë³¸                                |

* pseudo code
    ```
    1. Initialize: actor, critic, target networks, replay buffer
    2. For each step:
        a. í–‰ë™ ì„ íƒ: a = Î¼(s) + exploration_noise
        b. í™˜ê²½ìœ¼ë¡œë¶€í„° transition ìˆ˜ì§‘: (s, a, r, sâ€²)
        c. replay bufferì— ì €ì¥
        d. ë°°ì¹˜ ìƒ˜í”Œë§ í›„ critic ì—…ë°ì´íŠ¸
        e. actor ì—…ë°ì´íŠ¸ (âˆ‡Î¸_Î¼ J â‰ˆ âˆ‡_a Q(s, a) âˆ‡Î¸_Î¼ Î¼(s))
        f. target network soft update
    ```

    ```txt
              s_t
           â”‚
        â”Œâ”€â”€â–¼â”€â”€â”€â”€â”
        â”‚ Actor â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â””â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
              a_t           (s_t, a_t)
                               â”‚
                          â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                          â”‚ Critic  â”‚â”€â”€â”€â”€â†’ Q(s_t, a_t)
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```


#### ì¥ì 

| í•­ëª©                   | ë‚´ìš©                          |
| -------------------- | --------------------------- |
| âœ” ì—°ì† ì•¡ì…˜ ì²˜ë¦¬           | ë¡œë´‡, ì œì–´, ë“œë¡  ë“±ì— ì í•©            |
| âœ” off-policy         | replay buffer ì‚¬ìš© â†’ ìƒ˜í”Œ íš¨ìœ¨ ë†’ìŒ |
| âœ” policy gradient ë°©ì‹ | ì•ˆì •ì ì¸ ìˆ˜ë ´ ê°€ëŠ¥                  |

#### ë‹¨ì 

| í•­ëª©                      | ë‚´ìš©                                         |
| ----------------------- | ------------------------------------------ |
| âœ˜ exploration ì–´ë ¤ì›€       | deterministic policy â†’ ëœë¤ì„± ë¶€ì¡± (â†’ noise í•„ìš”) |
| âœ˜ critic overestimation | Qê°’ì´ ë°œì‚°í•  ê°€ëŠ¥ì„± (â†’ TD3ì—ì„œ ê°œì„ ë¨)                  |
| âœ˜ hyperparameter ë¯¼ê°     | í•™ìŠµë¥ , Ï„, noise scale ë“± íŠœë‹ í•„ìš”                |

## Target Network
$$\theta_{\text{targ}} \leftarrow \rho\,\theta_{\text{targ}} + (1-\rho)\,\theta,$$ 

ì—¬ê¸°ì„œ $\rho\in[0,1)$ëŠ” ë³´í†µ 0.99~0.999 ìˆ˜ì¤€ìœ¼ë¡œ ì„¤ì •ëœë‹¤. ì´ ì—…ë°ì´íŠ¸ëŠ” **Polyak averaging**(soft update) ë°©ì‹ìœ¼ë¡œ, íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ê°€ ì£¼ ë„¤íŠ¸ì›Œí¬ë¥¼ ì²œì²œíˆ ë”°ë¼ì˜¤ë„ë¡ í•œë‹¤:contentReference[oaicite:15]{index=15}.


Q-ëŸ¬ë‹ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì—ì„œëŠ” ë²¨ë§Œ íƒ€ê²Ÿ $y = r + \gamma \max_{a'}Q(s',a')$ê°€ í˜„ì¬ í•™ìŠµ ì¤‘ì¸ Q-í•¨ìˆ˜ ìì²´ì— ì˜ì¡´í•˜ê¸° ë•Œë¬¸ì—, ê·¸ ê°’ì„ ë°”ë¡œ ì‚¬ìš©í•˜ë©´ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§„ë‹¤. ì¦‰, ë„¤íŠ¸ì›Œí¬ë¥¼ ì—…ë°ì´íŠ¸í•  ë•Œ íƒ€ê²Ÿê°’ë„ ë™ì‹œì— ë³€í•˜ë©´ í•™ìŠµì´ ë°œì‚°í•˜ê±°ë‚˜ ìˆ˜ë ´í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤. íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì¼ì • ê¸°ê°„ ë™ì•ˆ ì¼ì •í•œ(í˜¹ì€ ì²œì²œíˆ ë³€í•˜ëŠ”) íŒŒë¼ë¯¸í„°ë¡œ íƒ€ê²Ÿì„ ì œê³µí•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, DQNì—ì„œëŠ” ì¼ì • ë‹¨ê³„ë§ˆë‹¤ ë„¤íŠ¸ì›Œí¬ë¥¼ ë³µì‚¬í•´ ì‚¬ìš©í•˜ê³ , DDPGì—ì„œëŠ” íƒ€ê²Ÿ íŒŒë¼ë¯¸í„°ë¥¼ ë¶€ë“œëŸ½ê²Œ ë™ê¸°í™”í•œë‹¤. ì´ë ‡ê²Œ í•˜ë©´ â€œì‚¬ê³  ìŠìŒ(catastrophic forgetting)â€ í˜„ìƒì„ ì¤„ì—¬ ì•ˆì •ì ì¸ í•™ìŠµì„ ë„ëª¨í•  ìˆ˜ ìˆë‹¤.


ì˜ˆì‹œ: ì‹¤ì œë¡œ íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²½ìš°, í•œ ë‹¨ê³„ì—ì„œ ì•½ê°„ì˜ ì—…ë°ì´íŠ¸ê°€ ë‹¤ìŒ ìƒíƒœì˜ Q-ì˜ˆì¸¡ì„ í¬ê²Œ í”ë“¤ì–´ ì—‰ëš±í•œ í–‰ë™ì„ ê°•í™”í•  ìˆ˜ ìˆë‹¤. ë°˜ë©´ íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ëŠ” â€œí˜„ì¬ê¹Œì§€ ì˜ í•™ìŠµëœ ì •ì±…â€ì„ ì¼ì‹œì ìœ¼ë¡œ ê³ ì •í•˜ì—¬, ë„¤íŠ¸ì›Œí¬ê°€ ì¶©ë¶„íˆ ë§ì€ ê²½í—˜ì„ ê³ ë ¤í•  ìˆ˜ ìˆëŠ” ì‹œê°„ì„ ë²Œì–´ì¤€ë‹¤. ê²°ê³¼ì ìœ¼ë¡œ íƒ€ê²Ÿê°’ì´ ì•ˆì •ë˜ì–´ ì†ì‹¤ í•¨ìˆ˜ê°€ ìˆ˜ë ´í•˜ê¸° ì‰¬ì›Œì§„ë‹¤.


#### Bellman Optimality Equation

ìµœì ì˜ í–‰ë™ê°€ì¹˜í•¨ìˆ˜ $Q^*(s,a)$ëŠ” ë²¨ë§Œ ìµœì  ë°©ì •ì‹ì„ ë§Œì¡±í•œë‹¤

ì—¬ê¸°ì„œ $\mathbb{E}$ëŠ” í™˜ê²½ì˜ ìƒíƒœì „ì´ ë¶„í¬ì— ëŒ€í•œ ê¸°ëŒ“ê°’ì´ë‹¤. ì´ ë°©ì •ì‹ì€ â€œí˜„ì¬ ìƒíƒœ-í–‰ë™ì˜ ê°€ì¹˜ = ì¦‰ì‹œ ë³´ìƒ + í• ì¸ëœ ë¯¸ë˜ ìµœëŒ€ ê°€ì¹˜â€ë¥¼ ì˜ë¯¸í•˜ë©°, Q-ëŸ¬ë‹ ê¸°ë°˜ ë°©ë²•ì˜ ì´ë¡ ì  ê·¼ê°„ì´ ëœë‹¤.


DDPGì—ì„œì˜ ì‘ìš©: DDPGì˜ í¬ë¦¬í‹±ì€ ì´ ë²¨ë§Œì‹ì„ ê·¼ì‚¬í•˜ë„ë¡ í•™ìŠµëœë‹¤. í¬ë¦¬í‹± ë„¤íŠ¸ì›Œí¬ $Q_\phi(s,a)$ëŠ” íƒ€ê¹ƒ ë²¨ë§Œê°’ $y=r+\gamma Q_{\phi_{\text{targ}}}(s',\mu_{\theta_{\text{targ}}}(s'))$ì— ì ì°¨ ì¼ì¹˜í•˜ë„ë¡ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, í‰ê· ì œê³± ë²¨ë§Œì˜¤ì°¨(MSBE) ì†ì‹¤ì„ ìµœì†Œí™”í•œë‹¤
ì´ ìˆ˜ì‹ì—ì„œ RHSëŠ” ë²¨ë§Œ ë°©ì •ì‹ì— ê¸°ë°˜í•œ TD íƒ€ê¹ƒì´ê³ , ì´ë¥¼ ìµœì†Œí™”í•˜ë©´ $Q_\phi$ê°€ ë²¨ë§Œ ë°©ì •ì‹ì„ ë§Œì¡±í•˜ë„ë¡ í•™ìŠµëœë‹¤. ì¦‰, DDPGì˜ í¬ë¦¬í‹± ì—…ë°ì´íŠ¸ëŠ” Q-ëŸ¬ë‹ê³¼ ë™ì¼í•˜ê²Œ í˜„ì¬ ê°€ì¹˜ì™€ ì¼ìŠ¤í… í›„ì˜ ì˜ˆì¸¡ ê°€ì¹˜ë¥¼ ì¼ì¹˜ì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘í•œë‹¤.



## TIPs

1. Exploration: Gaussian noise ë˜ëŠ” Ornstein-Uhlenbeck noise ì‚¬ìš©
    ```python
    action = actor(state) + noise
    ```

2. Target update Ï„ëŠ” ì‘ê²Œ (~0.005)

3. Replay bufferëŠ” ì¶©ë¶„íˆ ì»¤ì•¼ (1e6 ì´ìƒ)

4. Normalization: state/action scaling ì¤‘ìš”

5. Batch size: ì¼ë°˜ì ìœ¼ë¡œ 64~256

## TODO 
| ì•Œê³ ë¦¬ì¦˜                 | ê°œì„ ì                                                          |
| -------------------- | ----------------------------------------------------------- |
| **TD3**              | DDPG + twin critics + target smoothing â†’ overestimation ì™„í™”  |
| **SAC**              | stochastic policy + entropy regularization â†’ exploration í–¥ìƒ |
| **BCQ / AWAC / IQL** | off-policy ê¸°ë°˜ offline RL ê°œì„                                  |



## References



