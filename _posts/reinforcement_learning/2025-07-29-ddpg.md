---
layout: post
title: DDPG (Deep Deterministic Polcy Gradient)
category: Reinforcement Learning
tag: [rl, ddpg]
---

# [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971)


* `continuous` action space

* `off-policy`: 현재의 `policy`와 다른 `policy`로 수집한 데이터를 학습에 사용

    * **DQN**, **DDPG**, **TD3**, **SAC&**
    * 환경과 상호작용하지 않고도 학습 가능
    * 불일치한 `policy` 데이터로 인해 학습이 불안정
        * **importance sampling**이나 다른 안정화 기법이 요구됨


    * e.g
        * **On-policy (PPO)**:
            1. 지금 policy로 10번 시도
            2. 그 경험으로만 학습
            3. 학습 후 policy가 바뀌면, 이전 데이터는 폐기

        * **Off-policy (DQN)**:
            1. 무작위 정책이든 과거 정책이든, 경험을 다 저장
            2. replay buffer에서 샘플 뽑아 학습
            3. 수개월 전 정책의 데이터도 재사용 가능


    > `on-policy`는 현재의 `policy`에서 생성한 데이터를 학습에 사용하는 것으로 **PPO**, **REINFORCE**, 등의 알고리즘이 존재
    >  수렴이 안정적이지만, 경험 재사용이 불가능하여 샘플 효율이 낮고, 데이터 수집 비용이 크다. 


* `actor-critic`

    | 이름              | 역할            | 설명                                                   |
    | --------------- | ------------- | ---------------------------------------------------- |
    | `Actor`         | policy        | 상태 → 행동을 결정하는 deterministic 함수: 𝜇(s; θ|μ) |
    | `Critic`        | Q-function    | 상태-행동 쌍의 Q값을 예측: Q(s, a; θ|Q)              |
    | `Target Actor`  | target policy | 느리게 업데이트되는 actor 복사본                                 |
    | `Target Critic` | target Q      | 느리게 업데이트되는 critic 복사본                                |

* pseudo code
    ```
    1. Initialize: actor, critic, target networks, replay buffer
    2. For each step:
        a. 행동 선택: a = μ(s) + exploration_noise
        b. 환경으로부터 transition 수집: (s, a, r, s′)
        c. replay buffer에 저장
        d. 배치 샘플링 후 critic 업데이트
        e. actor 업데이트 (∇θ_μ J ≈ ∇_a Q(s, a) ∇θ_μ μ(s))
        f. target network soft update
    ```

    ```txt
              s_t
           │
        ┌──▼────┐
        │ Actor │ ─────────────┐
        └───────┘             │
              a_t           (s_t, a_t)
                               │
                          ┌────▼────┐
                          │ Critic  │────→ Q(s_t, a_t)
                          └─────────┘
    ```


#### 장점

| 항목                   | 내용                          |
| -------------------- | --------------------------- |
| ✔ 연속 액션 처리           | 로봇, 제어, 드론 등에 적합            |
| ✔ off-policy         | replay buffer 사용 → 샘플 효율 높음 |
| ✔ policy gradient 방식 | 안정적인 수렴 가능                  |

#### 단점

| 항목                      | 내용                                         |
| ----------------------- | ------------------------------------------ |
| ✘ exploration 어려움       | deterministic policy → 랜덤성 부족 (→ noise 필요) |
| ✘ critic overestimation | Q값이 발산할 가능성 (→ TD3에서 개선됨)                  |
| ✘ hyperparameter 민감     | 학습률, τ, noise scale 등 튜닝 필요                |

## Target Network
$$\theta_{\text{targ}} \leftarrow \rho\,\theta_{\text{targ}} + (1-\rho)\,\theta,$$ 

여기서 $\rho\in[0,1)$는 보통 0.99~0.999 수준으로 설정된다. 이 업데이트는 **Polyak averaging**(soft update) 방식으로, 타겟 네트워크가 주 네트워크를 천천히 따라오도록 한다:contentReference[oaicite:15]{index=15}.


Q-러닝 기반 알고리즘에서는 벨만 타겟 $y = r + \gamma \max_{a'}Q(s',a')$가 현재 학습 중인 Q-함수 자체에 의존하기 때문에, 그 값을 바로 사용하면 학습이 불안정해진다. 즉, 네트워크를 업데이트할 때 타겟값도 동시에 변하면 학습이 발산하거나 수렴하지 않을 수 있다. 타겟 네트워크는 이러한 문제를 해결하기 위해 일정 기간 동안 일정한(혹은 천천히 변하는) 파라미터로 타겟을 제공한다. 예를 들어, DQN에서는 일정 단계마다 네트워크를 복사해 사용하고, DDPG에서는 타겟 파라미터를 부드럽게 동기화한다. 이렇게 하면 “사고 잊음(catastrophic forgetting)” 현상을 줄여 안정적인 학습을 도모할 수 있다.


예시: 실제로 타겟 네트워크를 사용하지 않을 경우, 한 단계에서 약간의 업데이트가 다음 상태의 Q-예측을 크게 흔들어 엉뚱한 행동을 강화할 수 있다. 반면 타겟 네트워크는 “현재까지 잘 학습된 정책”을 일시적으로 고정하여, 네트워크가 충분히 많은 경험을 고려할 수 있는 시간을 벌어준다. 결과적으로 타겟값이 안정되어 손실 함수가 수렴하기 쉬워진다.


#### Bellman Optimality Equation

최적의 행동가치함수 $Q^*(s,a)$는 벨만 최적 방정식을 만족한다

여기서 $\mathbb{E}$는 환경의 상태전이 분포에 대한 기댓값이다. 이 방정식은 “현재 상태-행동의 가치 = 즉시 보상 + 할인된 미래 최대 가치”를 의미하며, Q-러닝 기반 방법의 이론적 근간이 된다.


DDPG에서의 응용: DDPG의 크리틱은 이 벨만식을 근사하도록 학습된다. 크리틱 네트워크 $Q_\phi(s,a)$는 타깃 벨만값 $y=r+\gamma Q_{\phi_{\text{targ}}}(s',\mu_{\theta_{\text{targ}}}(s'))$에 점차 일치하도록 파라미터를 조정한다. 구체적으로, 평균제곱 벨만오차(MSBE) 손실을 최소화한다
이 수식에서 RHS는 벨만 방정식에 기반한 TD 타깃이고, 이를 최소화하면 $Q_\phi$가 벨만 방정식을 만족하도록 학습된다. 즉, DDPG의 크리틱 업데이트는 Q-러닝과 동일하게 현재 가치와 일스텝 후의 예측 가치를 일치시키는 방식으로 동작한다.



## TIPs

1. Exploration: Gaussian noise 또는 Ornstein-Uhlenbeck noise 사용
    ```python
    action = actor(state) + noise
    ```

2. Target update τ는 작게 (~0.005)

3. Replay buffer는 충분히 커야 (1e6 이상)

4. Normalization: state/action scaling 중요

5. Batch size: 일반적으로 64~256

## TODO 
| 알고리즘                 | 개선점                                                         |
| -------------------- | ----------------------------------------------------------- |
| **TD3**              | DDPG + twin critics + target smoothing → overestimation 완화  |
| **SAC**              | stochastic policy + entropy regularization → exploration 향상 |
| **BCQ / AWAC / IQL** | off-policy 기반 offline RL 개선                                 |



## References



