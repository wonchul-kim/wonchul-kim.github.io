---
layout: post
title: DDPG (Deep Deterministic Polcy Gradient)
category: Reinforcement Learning
tag: [rl, ddpg]
---



#### Deterministic Policy vs. Stochastic Policy

* **Deteterministic Policy**
    $$\mu(s)$$

    같은 상태에서 항상 같은 행동을 선택하므로 행동이 확률적으로 변하지 않는다. 

* **Stochastic policy**

    $$\pi(a|s)$$

    상태에 따라 행동이 확률로서 나타난다. 

* 결정론적 정책은 구조가 단순하고 이해하기 쉬우며, 동일한 상태에선 항상 동일한 행동을 취하므로 정밀 제어에 적합하지만, 확률적 정책은 행동이 다양하게 선택될 수 있어 탐험(exploration) 측면에서 유리하고, 상대의 전략에 따라 행동 패턴을 섞을 수 있어 경쟁 상황에서 보완적이다. 즉, 결정론적 정책은 분명한 최적 행동이 존재할 때 효과적이며, 확률적 정책은 불확실성이나 탐험이 필요한 경우에 적합하다.

* **DDPG**에서는 결정론적 정책을 사용한다. 그 이유는 DDPG가 연속적인 행동 공간에서 동작하도록 설계되었기 때문이다. 이 경우 행동 공간에 최적의 행동을 찾기 위해 $\max_a Q(s,a)$를 매번 계산하는 것은 매우 비효율적이거나 불가능하다. 대신 결정론적 정책 네트워크 $\mu_\theta(s)$를 학습하여 $\max_a Q(s,a)\approx Q(s,\mu_\theta(s))$로 근사한다. 즉, 행동 공간에서 그리디 탐색 없이도 정책 파라미터의 미분을 통해 Q함수를 최적화할 수 있으므로, 연속공간에서의 행동 최적화 문제가 상대적으로 쉽게 해결된다. 예를 들어, 행동공간의 차원이 매우 크거나 연속적일 때 결정론적 정책은 미분 가능한 Q함수를 통해 정책을 직접 개선할 수 있다. 

    * 결정론적 정책 기울기 정리에서는 $\nabla_\theta J\approx \mathbb{E}s[\nabla_a Q(s,a)|{a=\mu(s)}\nabla_\theta \mu(s)]$ 형태로 정책 업데이트가 이루어진다
