---
layout: post
title: Inductive Bias
category: Lectures
tag: inductive bias
---


'모델이 학습과정에서 본 적이 없는 분포의 데이터를 입력 받았을 때, 해당 데이터에 대한 판단을 내리기 위해 가지고 있는, 학습과정에서 습득된 편향'을 Inductive bias라고 정의하는 것 같더라구요. 
그러니까 분류모델을 예로 들자면 Inductive Bias가 없는 모델은 학습과정에서 본 적 없는 모든 입력에 대해 Unknown, 혹은 (Unknown class가 따로 없다면)균등확률분포를 뱉게 된다는 거죠. 


다행히 저런 비모수적 모델말고 모수적인 모델들은 대부분 학습과정에서 어떤 형태로든 Inductive bias를 갖게 되는 것 같습니다.

다들 별 거부감 없이 사용하는 "Transformer는 CNN보다 Inductive bias가 적다."라는 표현에는 좀 어폐가 있는 걸로 느껴집니다.
마치 Inductive bias 자체가 모델의 성능을 저해하는 요소인 것 처럼 오해할 수도 있을 것 같거든요.
Transformer는 CNN보다 '구조적으로 강제되는' Inductive bias가 적다고 표현해야 맞는거 아닌가요?
저는 Transformer가 CNN보다 (데이터만 많으면) 잘 되는 이유가,
Transformer가 Locality나 Translation Equivalence와 같이 CNN구조라면 강제적으로 가질 수 밖에 없는 Inductive bias를 더 큰 자유도를 가진 상태에서 적절하게 조절해서 가질 수 있고, 대신 기존에 구조적으로 가질 수 없었던 Inductive bias(공간적으로 아주 멀리 떨어진 픽셀들간의 상관관계 등)는 더 많이 가질 수 있게 되기 때문이라고 이해하고 있거든요.
따라서 저러한 표현을 할 때 적절하게 Inductive bias를 수식해주지 않으면, 어떤 Inductive bias는 줄어든 반면에, 다른 Inductive bias는 늘어났기 때문에 심각한 어폐가 생긴다고 생각합니다.
물론 제가 읽은 문헌(단단한 머신러닝)이 잘못됐고 제가 예시로 든 용례로 쓰시는 분들이 알고 계신 정의가 맞는 걸수도 있지만, 적어도 여러 용례가 충돌하기 때문에, 용어의 오용을 방지하기 위해 어느 정의가 맞는 것인지 확실하게 집고 넘어갈 필요가 있다고 느껴집니다. 
Inductive Bias는 도대체 뭐라고 정의해야 엄밀한 걸까요?


ps. 많은 의견을 나눠주셔서 감사합니다.
다만 제가 아직도 영 마음에 안 드는건  'Transformer는 CNN보다 Inductive Bias가 적다.' 표현인데요... 
CNN의 Locality가 Inductive Bias면 Transformer나 Vanilla MLP의 Globality(Full Connectivity)도 Inductive Bias이기 때문에, 
결과적으로 CNN과 Transformer는 서로 어떤  Inductive Bias는 상대보다 적게 가지고 있되, 
다른 Inductive Bias는 상대보다 더 많이 가지고 있을 거고, 
따라서 Transformer가 CNN보다 inductive bias의 총량(정량적으로 표현할 방법이 없는걸로 알고 있지만 다들 이야기 하는 많다, 적다하는 맥락에서)이 줄어들었냐?하는 명제에 대해서 확언할 근거가 부족하다고 생각합니다. 
데이터량에 따른 성능 변화를 관찰함으로서 실험적으로 가정해 볼 순 있지만 사실 Inductive Bias의 총량 이외에 다른 더 중요한 원인이 있을 수도 있으니까요.
아니면 정말 극단적인 예로 Transformer가 CNN보다 사실 Inductive Bias는 더 많은 데 그 편향들이 죄다 모집단의 분포를 그대로 빼다 박듯이 근사해버릴 수 있는 극도로 유용한 Inductive Bias들이고, 학습이 느려지는 원인은 따로 존재할 수도 있고요