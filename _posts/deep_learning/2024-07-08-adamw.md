---
layout: post
title: AdamW
category: Deep Learning
tag: [adamw]
---

# [Decoupled weight decay regularization](https://arxiv.org/pdf/1711.05101)


* `Adam`은 computer vision 분야에서는 특히 momentum을 포함한 `SGD`에 비해 일반화가 뒤쳐진다. 

* `SGD`에서 `L2 regularization`과 `weight decay`는 동일하게 볼 수 있으며, 약간 다르더라도 수식적으로는 동일한 효과를 만든다. 
    - 동일하다고 볼 경우, `L2 regularization`은 learning rate에 종속됨 ($\lambda' = \frac{\lambda}{\alpha}$)

* `Adam`과 같은 adaptive learning rate를 사용하는 optimizer는 `SGD`와는 다른 weight update식을 사용하며 `L2 regularization`의 효과가 덜하다. 

    - 그렇기 때문에 weight update식에 직접적으로 weight decay term을 추가하여 해결하려함.
    - 이를 `L2 regularization`과 분리된 `weight decay`라 하여, `decoupled weight decay`라고 함.


<img src='/assets/deep_learning/adamw/adamw.png'>

- $\eta$: weight update마다 learning rate를 일정비율 감소시켜주는 learning rate schedule 상수


### References:
- [Decoupled weight decay regularization](https://arxiv.org/pdf/1711.05101)