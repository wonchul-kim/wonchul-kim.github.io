---
layout: post
title: Handling `NaN` prediction
category: Deep Learning
tag: null loss
---

### 현상

- `Dataset-1`: 기존 데이터셋 
- `Dataset-2`: 기존 데이터셋에서 10 ~ 20% 정도 데이터를 추가한 데이터셋

- `Dataset-1`에 대해서 학습한 모델을 `Dataset-2`로 학습을 하되, 초기 weight를 `Dataset-1`에서 학습한 모델을 로드하여 학습
- 이후, 몇 개의 `epoch`가 지나고 `prediction`이 `NaN`이 되는 현상 발생

> `prediction`이 `NaN`이 되는 epoch에서 문제점을 파악하기 보다는, 이전의 epoch에서 업데이트가 잘 못된 방향으로 되었기 때문에 현재의 epoch에서 `prediction`이 잘 못되어 `NaN`이 나온다고 분석함 



### Stop & Train after changing parameters 

- Lower the `learning rate`

### Gradient Clipping

**Gradient Exploding**이 원인이라는 가정하에 `gradient`의 크기 자체에 제한을 둔다. 

#### **Tensorflow**

1. By value 

```python
with tf.GradientTape() as tape:
	predictions= model(inputs, training=True)
    loss = get_loss(targets, predictions)

gradients = tape.gradient(loss, model.trainable_variables)
gradients = [(tf.clip_by_value(grad, -1.0, 1.0)) for grad in gradients]
optimizer.apply_gradients(zip(gradients, model.trainable_variables))
```

2. By norm
```python
with tf.GradientTape() as tape:
	predictions= model(inputs, training=True)
    loss = get_loss(targets, predictions)

gradients = tape.gradient(loss, model.trainable_variables)
gradients = [(tf.clip_by_norm(grad, clip_norm=2.0)) for grad in gradients]
optimizer.apply_gradients(zip(gradients, model.trainable_variables))
```

#### Pytorch

1. By value 
```python
optimizer.zero_grad()
loss.backward()
nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)
optimizer.step()
```

2. By norm
```python
optimizer.zero_grad()
loss.backward()
nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)
optimizer.step()
```

### Tricks

- By `DataLoader`

추가된 데이터끼리 `mini batch`로 뭉치는 것을 방지하고 잘 분배될 수 있도록 한다?

- By roll back

현재의 epoch에서 `prediction`이 `NaN`이 나온다는 것은 이전의 epoch에서 모델 업데이트가 잘못 되었다는 것이므로, 이전의 epoch로 roll back하여 다시 학습 진행?